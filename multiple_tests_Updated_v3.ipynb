{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5765ea6-bc3a-4730-ad0f-6b1e87b3e1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(32560, 15)\n",
      "F1 Score normal: [0.67607004 0.9135514 ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.66268062 0.91267896]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(624, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(15, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1371, 5)\n",
      "F1 Score normal: [0.98689956 0.98285714]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99559471 0.99435028]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(748, 5)\n",
      "F1 Score normal: [0.87218045 0.37037037]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.88721805 0.44444444]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(285, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(568, 32)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(698, 11)\n",
      "F1 Score normal: [0.97222222 0.9375    ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.95833333 0.90625   ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(197, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1727, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(28055, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3195, 37)\n",
      "F1 Score normal: [0.98181818 0.98056156]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.96957404 0.96774194]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(540, 461)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1079, 857)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(434, 17)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(989, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(207, 61)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1472, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(689, 16)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(365, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(10000, 14)\n",
      "F1 Score normal: [0.99947562 0.99908509]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99947562 0.99908509]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(99, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(213, 11)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(305, 4)\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.75757576 0.2       ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(131, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(302, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(293, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(154, 20)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(299, 28)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(582, 11)\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.77372263 0.06060606]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(350, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(149, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19999, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(359, 91)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(31, 57)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19019, 11)\n",
      "F1 Score normal: [0.80639823 0.90904379]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.79329609 0.9046883 ]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(960, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(8123, 23)\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [1. 1.]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3822, 65)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2533, 74)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2535, 74)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(195, 24)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(7493, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(181, 13)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(25009, 11)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1054, 42)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(209, 8)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(245056, 4)\n",
      "F1 Score normal: [0.99515571 0.99810794]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99792292 0.99918926]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(46, 36)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4600, 58)\n",
      "F1 Score normal: [0.95844156 0.93495935]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.96083551 0.93951613]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 23)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 45)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(999, 21)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4434, 37)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(150, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3771, 22)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(214, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3162, 26)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(957, 10)\n",
      "F1 Score normal: [0.98412698 0.96969697]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99465241 0.99009901]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 3)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 25)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4025, 1)\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(177, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1483, 10)\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares lineales\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]   \n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       if (len(lines[x].split(\" = \")) == 1):\n",
    "                           pass\n",
    "                       # If the separator is specified    \n",
    "                       else:\n",
    "                           separator = lines[x].split(\" = \")[1]\n",
    "                           separator = separator.strip()\n",
    "                           if (\"comma\" in separator):\n",
    "                               separator = \",\"\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           target_index = lines[x].split(\" = \")[1]\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               # If there is not header specified, assume\n",
    "                               # default value\n",
    "                               if (len(lines[x].split(\" = \")) == 1):\n",
    "                                    pass\n",
    "                               else:\n",
    "                                   # If it is specified\n",
    "                                   header = lines[x].split(\" = \")[1]\n",
    "                                   # If it is 0 assign it to None\n",
    "                                   if (header.startswith(\"0\")):\n",
    "                                       header = None\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python')\n",
    "                    print(data.shape)\n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = data.columns.tolist()[int(target_index) - 1]\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [n for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "                                        # aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        aux_model = LinearRegression()\n",
    "                                        aux_model.fit(X_train, y_train)\n",
    "                                        dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared = 1\n",
    "                                        else:    \n",
    "                                            r_squared = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "                                        \n",
    "                                        dictionary_aux_r_squared[fict_target] = r_squared\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "                            \n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data_test[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "                            \n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to read the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2863a78b-a0d0-4ac0-b7c8-f3543112e7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(32560, 15)\n",
      "F1 Score normal: [0.67607004 0.9135514 ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.64616886 0.90597847]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(624, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(15, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1371, 5)\n",
      "F1 Score normal: [0.98689956 0.98285714]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [1. 1.]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(748, 5)\n",
      "F1 Score normal: [0.87218045 0.37037037]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.88888889 0.4       ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(285, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(568, 32)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(698, 11)\n",
      "F1 Score normal: [0.97222222 0.9375    ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.96551724 0.92063492]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(197, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1727, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(28055, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3195, 37)\n",
      "F1 Score normal: [0.98181818 0.98056156]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.98167006 0.98072805]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(540, 461)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1079, 857)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(434, 17)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(989, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(207, 61)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1472, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(689, 16)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(365, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(10000, 14)\n",
      "F1 Score normal: [0.99947562 0.99908509]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [1. 1.]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(99, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(213, 11)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(305, 4)\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.75757576 0.2       ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(131, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(302, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(293, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(154, 20)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(299, 28)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(582, 11)\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.79104478 0.22222222]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(350, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(149, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19999, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(359, 91)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(31, 57)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19019, 11)\n",
      "F1 Score normal: [0.80639823 0.90904379]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.8127572  0.90236052]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(960, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(8123, 23)\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [1. 1.]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3822, 65)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2533, 74)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2535, 74)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(195, 24)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(7493, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(181, 13)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(25009, 11)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip\n",
      "...\n",
      "An error occurred: <urlopen error [WinError 10060] Se produjo un error durante el intento de conexión ya que la parte conectada no respondió adecuadamente tras un periodo de tiempo, o bien se produjo un error en la conexión establecida ya que el host conectado no ha podido responder>\n",
      "(25009, 11)\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1054, 42)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(209, 8)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(245056, 4)\n",
      "F1 Score normal: [0.99515571 0.99810794]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99351251 0.99748156]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(46, 36)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4600, 58)\n",
      "F1 Score normal: [0.95844156 0.93495935]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.96732026 0.94969819]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 23)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 45)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(999, 21)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4434, 37)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(150, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3771, 22)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(214, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3162, 26)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(957, 10)\n",
      "F1 Score normal: [0.98412698 0.96969697]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.93939394 0.86666667]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 3)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 25)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4025, 1)\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(177, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1483, 10)\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares de RF\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]   \n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       if (len(lines[x].split(\" = \")) == 1):\n",
    "                           pass\n",
    "                       # If the separator is specified    \n",
    "                       else:\n",
    "                           separator = lines[x].split(\" = \")[1]\n",
    "                           separator = separator.strip()\n",
    "                           if (\"comma\" in separator):\n",
    "                               separator = \",\"\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           target_index = lines[x].split(\" = \")[1]\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               # If there is not header specified, assume\n",
    "                               # default value\n",
    "                               if (len(lines[x].split(\" = \")) == 1):\n",
    "                                    pass\n",
    "                               else:\n",
    "                                   # If it is specified\n",
    "                                   header = lines[x].split(\" = \")[1]\n",
    "                                   # If it is 0 assign it to None\n",
    "                                   if (header.startswith(\"0\")):\n",
    "                                       header = None\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python')\n",
    "                    print(data.shape)\n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = data.columns.tolist()[int(target_index) - 1]\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [n for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "                                        aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        # aux_model = LinearRegression()\n",
    "                                        aux_model.fit(X_train, y_train)\n",
    "                                        dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared = 1\n",
    "                                        else:    \n",
    "                                            r_squared = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "                                        \n",
    "                                        dictionary_aux_r_squared[fict_target] = r_squared\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "                            \n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data_test[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "                            \n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to read the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0f67f-b689-4f11-b24e-920b43d25ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se puede observar que dependiendo de si se usan modelos auxiliares lineales o de RF\n",
    "# se obtienen unos resultados u otros --> Para cada dataset habrá que ver cuál es la mejor\n",
    "# opción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b9a23-8e4e-4eb4-ad77-0d6cb21c8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebas añadiendo como nuevas columnas solo aquellas que estén\n",
    "# por encima de un cierto umbral en su correlación con la target.\n",
    "# Por lo menos estableciendo un threshold del 20%, en la mayoría\n",
    "# de los casos empeora, pero en algunos se obtiene una mejora\n",
    "# significativa --> Para cada dataset habrá que ver cuál es la mejor\n",
    "# opción, o quizá buscar otra forma de probar con distintas nuevas\n",
    "# columnas\n",
    "\n",
    "# Para todas estas pruebas además habrá que realizar varias repeticiones\n",
    "# estadísticas para obtener datos más fiables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9a73785-c57c-4440-adb7-2be09324b697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(32560, 15)\n",
      "F1 Score normal: [0.67607004 0.9135514 ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "[]\n",
      "0\n",
      "(27656, 325)\n",
      "F1 Score: [0.67607004 0.9135514 ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(624, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(15, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1371, 5)\n",
      "F1 Score normal: [0.98689956 0.98285714]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['3.6216_0', '3.6216_1', '8.6661_0', '-2.8073_0', '-2.8073_1', '-0.44699_0', '-0.44699_1']\n",
      "7\n",
      "(1145, 13)\n",
      "F1 Score: [0.03208556 0.16589862]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(748, 5)\n",
      "F1 Score normal: [0.87218045 0.37037037]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['Monetary (c.c. blood)_0']\n",
      "1\n",
      "(453, 13)\n",
      "F1 Score: [0.87301587 0.52941176]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(285, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(568, 32)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(698, 11)\n",
      "F1 Score normal: [0.97222222 0.9375    ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['1000025_1', '5_0', '5_1', '1_0', '1_1', '1.1_0', '1.1_1', '1.2_0', '1.2_1', '2_0', '2_1', '3_0', '3_1', '1.4_0', '1.5_1', '1.3_1_1', '1.3_10_1', '1.3_2_0', '1.3_3_1', '1.3_4_0', '1.3_4_1', '1.3_1']\n",
      "22\n",
      "(586, 61)\n",
      "F1 Score: [0.95833333 0.90625   ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(197, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1727, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(28055, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3195, 37)\n",
      "F1 Score normal: [0.98181818 0.98056156]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['t_t_1', 'f.15_f_0', 'f.15_f_1', 'f.15_t_0', 'f.15_t_1', 'f.16_f_0', 'f.16_f_1', 'f.16_t_0', 'f.16_t_1', 'f.17_f_0', 'f.17_f_1', 'f.17_t_0', 'f.17_t_1', 'f.18_f_0', 'f.18_f_1', 'f.18_t_0', 'f.18_t_1', 'f.19_f_0', 'f.19_f_1', 'f.19_t_0', 'f.19_t_1', 'f.20_f_0', 'f.20_f_1', 'f.20_t_0', 'f.20_t_1', 'f.21_f_0', 'f.21_f_1', 'f.21_t_0', 'f.21_t_1', 't.1_f_0', 't.1_f_1', 't.1_t_0', 't.1_t_1', 'f.22_f_0', 'f.22_f_1', 'f.22_t_0', 'f.22_t_1', 'f.23_f_0', 'f.23_f_1', 'f.23_t_0', 'f.23_t_1', 'f.24_f_0', 'f.24_f_1', 'f.24_t_0', 'f.24_t_1', 'f.25_f_0', 'f.25_f_1', 'f.25_t_0', 'f.25_t_1', 'f.26_f_0', 'f.26_f_1', 'f.26_t_0', 'f.26_t_1', 'f.27_f_0', 'f.27_f_1', 'f.27_t_0', 'f.27_t_1', 'f.28_f_0', 'f.28_f_1', 'f.28_t_0', 'f.28_t_1', 't.2_f_0', 't.2_f_1', 't.2_t_0', 't.2_t_1', 't.3_f_0', 't.3_f_1', 't.3_t_0', 't.3_t_1', 'n.1_n_0', 'n.1_n_1', 'n.1_t_0', 'n.1_t_1']\n",
      "73\n",
      "(2716, 220)\n",
      "F1 Score: [0.85       0.87258687]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(540, 461)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1079, 857)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(434, 17)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(989, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(207, 61)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1472, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(689, 16)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(365, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(10000, 14)\n",
      "F1 Score normal: [0.99947562 0.99908509]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['p3_0', 'stab_1']\n",
      "2\n",
      "(8500, 40)\n",
      "F1 Score: [0.99947562 0.99908509]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(99, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(213, 11)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(305, 4)\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['64_0']\n",
      "1\n",
      "(245, 10)\n",
      "F1 Score: [0.78125    0.36363636]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(131, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(302, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(293, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(154, 20)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(299, 28)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(582, 11)\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['3.3_0']\n",
      "1\n",
      "(480, 34)\n",
      "F1 Score: [0.80597015 0.27777778]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(350, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(149, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19999, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(359, 91)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(31, 57)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19019, 11)\n",
      "F1 Score normal: [0.80639823 0.90904379]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['0.3918_0', '0.3918_1', '0.1982_0', '27.7004_0', '27.7004_1', '22.011_0', '40.092_0', '40.092_1', '81.8828_0', '81.8828_1']\n",
      "10\n",
      "(16068, 31)\n",
      "F1 Score: [0.58853974 0.62487361]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(960, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(8123, 23)\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['x_b_0', 'x_b_1', 'x_c_0', 'x_c_1', 'x_f_1', 'x_k_0', 'x_k_1', 'x_s_0', 'x_s_1', 'x_x_0', 'x_x_1', 's_f_0', 's_f_1', 's_g_0', 's_g_1', 's_y_0', 's_y_1', 'n_b_0', 'n_b_1', 'n_c_0', 'n_c_1', 'n_e_0', 'n_e_1', 'n_g_0', 'n_g_1', 'n_n_0', 'n_n_1', 'n_p_0', 'n_p_1', 'n_r_0', 'n_r_1', 'n_u_0', 'n_u_1', 'n_w_0', 'n_w_1', 'n_y_1', 't_f_0', 't_f_1', 't_t_0', 'p.1_a_0', 'p.1_a_1', 'p.1_c_0', 'p.1_c_1', 'p.1_f_0', 'p.1_f_1', 'p.1_l_0', 'p.1_l_1', 'p.1_m_0', 'p.1_m_1', 'p.1_n_0', 'p.1_p_0', 'p.1_p_1', 'p.1_s_0', 'p.1_y_0', 'p.1_y_1', 'f_a_0', 'f_a_1', 'f_f_0', 'f_f_1', 'c_c_1', 'c_w_0', 'n.1_b_0', 'n.1_b_1', 'n.1_n_0', 'n.1_n_1', 'k_b_0', 'k_e_0', 'k_e_1', 'k_g_1', 'k_h_0', 'k_h_1', 'k_o_0', 'k_o_1', 'k_p_0', 'k_p_1', 'k_r_0', 'k_u_0', 'k_u_1', 'k_w_0', 'k_y_0', 'k_y_1', 'e_e_0', 'e_t_0', 'e.1_?_0', 'e.1_b_0', 'e.1_b_1', 'e.1_c_0', 'e.1_c_1', 'e.1_e_0', 'e.1_e_1', 'e.1_r_0', 'e.1_r_1', 's.1_f_0', 's.1_f_1', 's.1_k_0', 's.1_k_1', 's.1_s_1', 's.1_y_1', 's.2_f_0', 's.2_f_1', 's.2_k_0', 's.2_k_1', 's.2_s_1', 's.2_y_0', 's.2_y_1', 'w_b_0', 'w_b_1', 'w_c_0', 'w_c_1', 'w_e_0', 'w_e_1', 'w_g_0', 'w_g_1', 'w_n_0', 'w_n_1', 'w_o_0', 'w_o_1', 'w_p_1', 'w_w_0', 'w_y_0', 'w.1_b_0', 'w.1_b_1', 'w.1_c_0', 'w.1_c_1', 'w.1_e_0', 'w.1_e_1', 'w.1_g_0', 'w.1_g_1', 'w.1_n_0', 'w.1_n_1', 'w.1_o_0', 'w.1_o_1', 'w.1_p_0', 'w.1_p_1', 'w.1_w_0', 'w.1_w_1', 'w.1_y_1', 'p.2_p_0', 'p.2_p_1', 'w.2_n_0', 'w.2_n_1', 'w.2_o_0', 'w.2_o_1', 'w.2_w_0', 'w.2_w_1', 'w.2_y_0', 'w.2_y_1', 'o_n_0', 'o_n_1', 'o_o_0', 'o_o_1', 'o_t_0', 'o_t_1', 'p.3_e_0', 'p.3_e_1', 'p.3_f_1', 'p.3_l_0', 'p.3_l_1', 'p.3_n_0', 'p.3_n_1', 'p.3_p_0', 'k.1_b_0', 'k.1_h_0', 'k.1_h_1', 'k.1_k_0', 'k.1_k_1', 'k.1_n_0', 'k.1_n_1', 'k.1_r_0', 'k.1_r_1', 'k.1_u_0', 'k.1_w_1', 'k.1_y_0', 'k.1_y_1', 's.3_a_0', 's.3_a_1', 's.3_c_1', 's.3_n_0', 's.3_n_1', 's.3_s_0', 's.3_s_1', 's.3_v_0', 's.3_y_0', 's.3_y_1', 'u_d_0', 'u_d_1', 'u_g_0', 'u_g_1', 'u_l_0', 'u_l_1', 'u_m_0', 'u_m_1', 'u_p_0', 'u_p_1', 'u_u_0', 'u_u_1', 'u_w_0', 'u_w_1']\n",
      "198\n",
      "(6905, 352)\n",
      "F1 Score: [0.92779783 0.93975904]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3822, 65)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2533, 74)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2535, 74)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(195, 24)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(7493, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(181, 13)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(25009, 11)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1054, 42)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(209, 8)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(245056, 4)\n",
      "F1 Score normal: [0.99515571 0.99810794]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['74_0', '74_1', '85_0', '123_0', '123_1']\n",
      "5\n",
      "(43727, 10)\n",
      "F1 Score: [0.93531853 0.97662521]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(46, 36)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4600, 58)\n",
      "F1 Score normal: [0.95844156 0.93495935]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['0.1_0', '0.1_1', '0.32_0', '0.2_0', '0.4_1', '0.5_0', '0.6_1', '0.64.2_0', '0.18_0', '0.21_0', '0.21_1', '0.25_0', '0.25_1', '0.26_0', '0.26_1', '0.27_0', '0.27_1', '0.29_0', '0.29_1', '0.30_1', '0.31_0', '0.33_1', '0.34_0', '0.35_0', '0.36_0', '0.36_1', '0.39_0', '0.39_1', '0.40_1', '0.42_1', '0.43_1', '0.778_0', '0.45_0', '61_1', '278_0', '278_1']\n",
      "36\n",
      "(3578, 172)\n",
      "F1 Score: [0.88520055 0.84601113]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 23)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 45)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(999, 21)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4434, 37)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(150, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3771, 22)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(214, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3162, 26)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(957, 10)\n",
      "F1 Score normal: [0.98412698 0.96969697]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['x_b_0', 'x_b_1', 'x_o_0', 'x_o_1', 'x_x_0', 'x_x_1', 'x.1_b_0', 'x.1_b_1', 'x.1_o_0', 'x.1_o_1', 'x.1_x_0', 'x.1_x_1', 'x.2_b_0', 'x.2_b_1', 'x.2_o_0', 'x.2_o_1', 'x.2_x_0', 'x.2_x_1', 'x.3_b_0', 'x.3_b_1', 'x.3_o_0', 'x.3_o_1', 'x.3_x_0', 'x.3_x_1', 'o_b_0', 'o_b_1', 'o_o_0']\n",
      "27\n",
      "(813, 82)\n",
      "F1 Score: [0.78481013 0.        ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 3)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 25)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4025, 1)\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(177, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1483, 10)\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares lineales y threshold de 20%\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Threshold of correlation with target for new columns\n",
    "new_cols_threshold = 0.2\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]   \n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       if (len(lines[x].split(\" = \")) == 1):\n",
    "                           pass\n",
    "                       # If the separator is specified    \n",
    "                       else:\n",
    "                           separator = lines[x].split(\" = \")[1]\n",
    "                           separator = separator.strip()\n",
    "                           if (\"comma\" in separator):\n",
    "                               separator = \",\"\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           target_index = lines[x].split(\" = \")[1]\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               # If there is not header specified, assume\n",
    "                               # default value\n",
    "                               if (len(lines[x].split(\" = \")) == 1):\n",
    "                                    pass\n",
    "                               else:\n",
    "                                   # If it is specified\n",
    "                                   header = lines[x].split(\" = \")[1]\n",
    "                                   # If it is 0 assign it to None\n",
    "                                   if (header.startswith(\"0\")):\n",
    "                                       header = None\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python')\n",
    "                    print(data.shape)\n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = data.columns.tolist()[int(target_index) - 1]\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [n for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "                                        # aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        aux_model = LinearRegression()\n",
    "                                        aux_model.fit(X_train, y_train)\n",
    "                                        dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared = 1\n",
    "                                        else:    \n",
    "                                            r_squared = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "                                        \n",
    "                                        dictionary_aux_r_squared[fict_target] = r_squared\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "                            \n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "\n",
    "                            #####\n",
    "                            # Calculate correlation with the target variable\n",
    "                            correlation = result_df.corr()[target_variable].abs()\n",
    "                            # Select columns whose names end with '_0' or '_1' and have a at least certain absolute correlation with the target\n",
    "                            selected_columns = correlation[(correlation.index.str.endswith('_0') | correlation.index.str.endswith('_1')) & (correlation >= new_cols_threshold)].index\n",
    "                            selected_columns = selected_columns.tolist()\n",
    "                            print(selected_columns)\n",
    "                            print(len(selected_columns))\n",
    "                            print(result_df.shape)\n",
    "                            \n",
    "                            result_df = result_df[data.columns.tolist() + selected_columns]\n",
    "                            #####\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                                        X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                        y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                        y_real = data_test[fict_target]  \n",
    "                                \n",
    "                                        mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                                        rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                        weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                        # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "\n",
    "                                        rmse = [1e5 if (x > 1e5) else x for x in rmse]\n",
    "                                \n",
    "                                        # Add column to list of new columns\n",
    "                                        new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new2 = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                for u in dictionary_case.keys():\n",
    "                                    if ((u + '_' + str(case)) in selected_columns):    \n",
    "                                        names_cols_dataframe_new2.append(u + \"_\" + str(case))\n",
    "                            dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            # Adapt order of columns to the order in which the model was\n",
    "                            # trained\n",
    "                            data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to read the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fb27a59-f581-4aca-82db-90779eef8a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(32560, 15)\n",
      "F1 Score normal: [0.67607004 0.9135514 ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['39_0', ' 13_1', ' Bachelors_ 12th_1', ' Bachelors_ Assoc-voc_0', ' Bachelors_ Preschool_0', ' Not-in-family_ Other-relative_1']\n",
      "6\n",
      "(27656, 325)\n",
      "F1 Score: [0.6651439 0.9031832]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(624, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(15, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1371, 5)\n",
      "F1 Score normal: [0.98689956 0.98285714]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['3.6216_0', '3.6216_1', '8.6661_0', '8.6661_1', '-2.8073_0', '-2.8073_1', '-0.44699_0', '-0.44699_1']\n",
      "8\n",
      "(1145, 13)\n",
      "F1 Score: [0.11458333 0.19811321]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(748, 5)\n",
      "F1 Score normal: [0.87218045 0.37037037]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['Monetary (c.c. blood)_0']\n",
      "1\n",
      "(453, 13)\n",
      "F1 Score: [0.87692308 0.46666667]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(285, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(568, 32)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(698, 11)\n",
      "F1 Score normal: [0.97222222 0.9375    ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['1000025_1', '5_0', '5_1', '1_0', '1_1', '1.1_0', '1.1_1', '1.2_0', '1.2_1', '2_0', '2_1', '3_0', '3_1', '1.4_0', '1.5_1', '1.3_1_1', '1.3_10_1', '1.3_2_0', '1.3_2_1', '1.3_3_1', '1.3_4_0', '1.3_5_0', '1.3_5_1', '1.3_7_0', '1.3_8_0', '1.3_8_1', '1.3_9_0', '1.3_?_1', '1.3_1']\n",
      "29\n",
      "(586, 61)\n",
      "F1 Score: [0.90225564 0.82666667]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(197, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1727, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(28055, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3195, 37)\n",
      "F1 Score normal: [0.98181818 0.98056156]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['f.24_f_0', 'f.24_f_1']\n",
      "2\n",
      "(2716, 220)\n",
      "F1 Score: [0.87804878 0.89151874]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(540, 461)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1079, 857)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(434, 17)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(989, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(207, 61)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1472, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(689, 16)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(365, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(10000, 14)\n",
      "F1 Score normal: [0.99947562 0.99908509]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['tau1_0', 'tau1_1', 'tau2_0', 'tau2_1', 'tau3_0', 'tau3_1', 'tau4_0', 'tau4_1', 'p1_0', 'p1_1', 'p2_0', 'p2_1', 'p3_0', 'p3_1', 'p4_0', 'p4_1', 'g1_0', 'g2_0', 'g3_0', 'g3_1', 'g4_0', 'g4_1', 'stab_0', 'stab_1']\n",
      "24\n",
      "(8500, 40)\n",
      "F1 Score: [0.99947562 0.99908509]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(99, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(213, 11)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(305, 4)\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['64_0']\n",
      "1\n",
      "(245, 10)\n",
      "F1 Score: [0.7761194  0.21052632]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(131, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(302, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(293, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(154, 20)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(299, 28)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(582, 11)\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['65_0', '6.8_1', '3.3_0']\n",
      "3\n",
      "(480, 34)\n",
      "F1 Score: [0.8        0.06666667]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(350, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(149, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19999, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(359, 91)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(31, 57)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19019, 11)\n",
      "F1 Score normal: [0.80639823 0.90904379]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['16.0021_0', '0.1982_0', '27.7004_0', '27.7004_1', '22.011_0', '22.011_1', '-8.2027_0', '-8.2027_1', '40.092_0', '40.092_1', '81.8828_0', '81.8828_1']\n",
      "12\n",
      "(16068, 31)\n",
      "F1 Score: [0.51201281 0.05088266]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(960, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(8123, 23)\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['x_b_0', 'x_f_1', 'x_k_0', 'x_s_0', 'x_x_0', 's_f_0', 's_f_1', 's_g_0', 's_g_1', 's_y_0', 's_y_1', 'n_c_0', 'n_c_1', 'n_e_0', 'n_e_1', 'n_g_1', 'n_n_0', 'n_n_1', 'n_p_0', 'n_w_1', 'n_y_1', 't_f_1', 't_t_0', 'p.1_a_0', 'p.1_a_1', 'p.1_c_0', 'p.1_c_1', 'p.1_f_0', 'p.1_l_1', 'p.1_m_0', 'p.1_m_1', 'p.1_n_0', 'p.1_p_1', 'p.1_s_0', 'p.1_y_0', 'p.1_y_1', 'f_a_1', 'c_c_1', 'n.1_b_0', 'n.1_b_1', 'k_e_0', 'k_g_1', 'k_h_0', 'k_p_0', 'k_p_1', 'k_r_0', 'k_w_0', 'k_y_0', 'k_y_1', 'e_e_0', 'e_t_0', 'e.1_?_0', 'e.1_b_0', 'e.1_b_1', 'e.1_c_0', 'e.1_c_1', 'e.1_e_0', 'e.1_e_1', 'e.1_r_0', 'e.1_r_1', 's.1_f_0', 's.1_f_1', 's.1_k_1', 's.1_y_1', 's.2_s_1', 'w_b_0', 'w_b_1', 'w_c_0', 'w_e_0', 'w_e_1', 'w_g_0', 'w_g_1', 'w_n_0', 'w_o_1', 'w_p_1', 'w_w_0', 'w_y_0', 'w.1_b_0', 'w.1_b_1', 'w.1_n_0', 'w.1_o_0', 'w.1_o_1', 'w.1_p_1', 'w.1_w_1', 'w.1_y_1', 'p.2_p_0', 'w.2_o_0', 'w.2_o_1', 'w.2_w_0', 'w.2_w_1', 'w.2_y_1', 'o_n_0', 'o_o_0', 'o_o_1', 'o_t_0', 'p.3_e_0', 'p.3_e_1', 'p.3_l_1', 'p.3_n_1', 'p.3_p_0', 'k.1_b_0', 'k.1_k_0', 'k.1_n_0', 'k.1_n_1', 'k.1_y_1', 's.3_a_0', 's.3_a_1', 's.3_c_1', 's.3_n_1', 's.3_s_0', 's.3_s_1', 's.3_y_0', 's.3_y_1', 'u_g_1', 'u_l_0', 'u_l_1', 'u_m_0', 'u_m_1', 'u_p_0', 'u_p_1', 'u_u_1']\n",
      "121\n",
      "(6905, 352)\n",
      "F1 Score: [0.54899416 0.22346369]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3822, 65)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2533, 74)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2535, 74)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(195, 24)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(7493, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(181, 13)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(25009, 11)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1054, 42)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(209, 8)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(245056, 4)\n",
      "F1 Score normal: [0.99515571 0.99810794]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['74_0', '74_1', '85_0', '85_1', '123_0', '123_1']\n",
      "6\n",
      "(43727, 10)\n",
      "F1 Score: [0.35596708 0.84997603]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(46, 36)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4600, 58)\n",
      "F1 Score normal: [0.95844156 0.93495935]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['0.64.1_0', '0.64.1_1', '0.1_0', '0.1_1', '0.32_0', '0.32_1', '0.2_0', '0.3_0', '0.4_1', '0.5_0', '0.5_1', '0.6_1', '0.7_0', '0.64.2_0', '0.12_1', '0.18_0', '0.20_0', '0.26_0', '0.28_0', '0.29_1', '0.33_1', '0.34_0', '0.35_0', '0.36_0', '0.39_0', '0.39_1', '0.778_0']\n",
      "27\n",
      "(3578, 172)\n",
      "F1 Score: [0.82568807 0.61025641]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 23)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 45)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(999, 21)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4434, 37)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(150, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3771, 22)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(214, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3162, 26)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(957, 10)\n",
      "F1 Score normal: [0.98412698 0.96969697]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "[]\n",
      "0\n",
      "(813, 82)\n",
      "F1 Score: [0.98412698 0.96969697]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 3)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 25)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4025, 1)\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(177, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1483, 10)\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares de RF y threshold de 20%\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Threshold of correlation with target for new columns\n",
    "new_cols_threshold = 0.2\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]   \n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       if (len(lines[x].split(\" = \")) == 1):\n",
    "                           pass\n",
    "                       # If the separator is specified    \n",
    "                       else:\n",
    "                           separator = lines[x].split(\" = \")[1]\n",
    "                           separator = separator.strip()\n",
    "                           if (\"comma\" in separator):\n",
    "                               separator = \",\"\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           target_index = lines[x].split(\" = \")[1]\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               # If there is not header specified, assume\n",
    "                               # default value\n",
    "                               if (len(lines[x].split(\" = \")) == 1):\n",
    "                                    pass\n",
    "                               else:\n",
    "                                   # If it is specified\n",
    "                                   header = lines[x].split(\" = \")[1]\n",
    "                                   # If it is 0 assign it to None\n",
    "                                   if (header.startswith(\"0\")):\n",
    "                                       header = None\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python')\n",
    "                    print(data.shape)\n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = data.columns.tolist()[int(target_index) - 1]\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [n for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "                                        aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        # aux_model = LinearRegression()\n",
    "                                        aux_model.fit(X_train, y_train)\n",
    "                                        dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared = 1\n",
    "                                        else:    \n",
    "                                            r_squared = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "                                        \n",
    "                                        dictionary_aux_r_squared[fict_target] = r_squared\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "                            \n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "\n",
    "                            #####\n",
    "                            # Calculate correlation with the target variable\n",
    "                            correlation = result_df.corr()[target_variable].abs()\n",
    "                            # Select columns whose names end with '_0' or '_1' and have a at least certain absolute correlation with the target\n",
    "                            selected_columns = correlation[(correlation.index.str.endswith('_0') | correlation.index.str.endswith('_1')) & (correlation >= new_cols_threshold)].index\n",
    "                            selected_columns = selected_columns.tolist()\n",
    "                            print(selected_columns)\n",
    "                            print(len(selected_columns))\n",
    "                            print(result_df.shape)\n",
    "                            \n",
    "                            result_df = result_df[data.columns.tolist() + selected_columns]\n",
    "                            #####\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                                        X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                        y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                        y_real = data_test[fict_target]  \n",
    "                                \n",
    "                                        mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                                        rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                        weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                        # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "\n",
    "                                        rmse = [1e5 if (x > 1e5) else x for x in rmse]\n",
    "                                \n",
    "                                        # Add column to list of new columns\n",
    "                                        new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new2 = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                for u in dictionary_case.keys():\n",
    "                                    if ((u + '_' + str(case)) in selected_columns):    \n",
    "                                        names_cols_dataframe_new2.append(u + \"_\" + str(case))\n",
    "                            dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            # Adapt order of columns to the order in which the model was\n",
    "                            # trained\n",
    "                            data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to read the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4effe9-6fc5-4658-bbf3-6415694126ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4ead5-ecc7-4fd8-8b05-ed510835bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebas realizando un feature importance de un modelo generado solo con las\n",
    "# variables nuevas, y extrayendo el 10% de las variables nuevas con mayor \n",
    "# feature importance para añadir al dataset. Esta estrategia por lo general\n",
    "# parece poco efectiva, a diferencia de la selección por correlación con la\n",
    "# target, donde a veces (en el caso de los modelos lineales) sí se apreciaba\n",
    "# una ventaja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6472f66a-7b11-4d0a-9d07-4256d1418a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(32560, 15)\n",
      "F1 Score normal: [0.67607004 0.9135514 ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.52893519 0.89865538]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(624, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(15, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1371, 5)\n",
      "F1 Score normal: [0.98689956 0.98285714]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.98689956 0.98285714]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(748, 5)\n",
      "F1 Score normal: [0.87218045 0.37037037]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.87218045 0.37037037]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(285, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(568, 32)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(698, 11)\n",
      "F1 Score normal: [0.97222222 0.9375    ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.96551724 0.92063492]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(197, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1727, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(28055, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3195, 37)\n",
      "F1 Score normal: [0.98181818 0.98056156]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.94780793 0.94780793]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(540, 461)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1079, 857)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(434, 17)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(989, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(207, 61)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1472, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(689, 16)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(365, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(10000, 14)\n",
      "F1 Score normal: [0.99947562 0.99908509]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99947562 0.99908509]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(99, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(213, 11)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(305, 4)\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.75757576 0.2       ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(131, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(302, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(293, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(154, 20)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(299, 28)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(582, 11)\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.79710145 0.125     ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(350, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(149, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19999, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(359, 91)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(31, 57)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19019, 11)\n",
      "F1 Score normal: [0.80639823 0.90904379]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.66424682 0.78662053]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(960, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(8123, 23)\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.55581948 0.00531915]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3822, 65)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2533, 74)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2535, 74)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(195, 24)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(7493, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(181, 13)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(25009, 11)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1054, 42)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(209, 8)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(245056, 4)\n",
      "F1 Score normal: [0.99515571 0.99810794]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99515571 0.99810794]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(46, 36)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4600, 58)\n",
      "F1 Score normal: [0.95844156 0.93495935]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.89242054 0.8018018 ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 23)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 45)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(999, 21)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4434, 37)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(150, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3771, 22)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(214, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3162, 26)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(957, 10)\n",
      "F1 Score normal: [0.98412698 0.96969697]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.78481013 0.        ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 3)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 25)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4025, 1)\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(177, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1483, 10)\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares lineales\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Threshold of correlation with target for new columns\n",
    "new_cols_threshold = 0.2\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]   \n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       if (len(lines[x].split(\" = \")) == 1):\n",
    "                           pass\n",
    "                       # If the separator is specified    \n",
    "                       else:\n",
    "                           separator = lines[x].split(\" = \")[1]\n",
    "                           separator = separator.strip()\n",
    "                           if (\"comma\" in separator):\n",
    "                               separator = \",\"\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           target_index = lines[x].split(\" = \")[1]\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               # If there is not header specified, assume\n",
    "                               # default value\n",
    "                               if (len(lines[x].split(\" = \")) == 1):\n",
    "                                    pass\n",
    "                               else:\n",
    "                                   # If it is specified\n",
    "                                   header = lines[x].split(\" = \")[1]\n",
    "                                   # If it is 0 assign it to None\n",
    "                                   if (header.startswith(\"0\")):\n",
    "                                       header = None\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python')\n",
    "                    print(data.shape)\n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = data.columns.tolist()[int(target_index) - 1]\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [n for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "                                        # aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        aux_model = LinearRegression()\n",
    "                                        aux_model.fit(X_train, y_train)\n",
    "                                        dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared = 1\n",
    "                                        else:    \n",
    "                                            r_squared = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "                                        \n",
    "                                        dictionary_aux_r_squared[fict_target] = r_squared\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "\n",
    "                            #####################\n",
    "                            ####################\n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            aux_df = pd.concat([dataframe_new, data[[target_variable]]], axis=1, ignore_index=True)\n",
    "                            aux_df.columns = cols_b + [target_variable]\n",
    "                            \n",
    "                            features = aux_df.drop(target_variable, axis=1)\n",
    "                            target = aux_df[target_variable]\n",
    "                            rf_model_aux = RandomForestClassifier(n_estimators=200, random_state=42, min_samples_split=10, min_samples_leaf=5)\n",
    "                            rf_model_aux.fit(features, target)\n",
    "                            \n",
    "                            feature_importances = rf_model_aux.feature_importances_\n",
    "                            \n",
    "                            # Create a list of (feature_name, importance) tuples\n",
    "                            feature_importance_tuples = zip(features.columns, feature_importances)\n",
    "                                                        \n",
    "                            # Sort the tuples based on importance\n",
    "                            sorted_feature_importance_tuples = sorted(feature_importance_tuples, key=lambda x: x[1], reverse=True)\n",
    "                            \n",
    "                            # Extract sorted feature names\n",
    "                            sorted_feature_names = [feature_name for feature_name, _ in sorted_feature_importance_tuples]\n",
    "\n",
    "                            # Select some features\n",
    "                            selected_columns = sorted_feature_names[0: int(len(sorted_feature_names)/10)]\n",
    "                            \n",
    "                            ####################\n",
    "                            ####################\n",
    "\n",
    "                            dataframe_new = dataframe_new.loc[:, selected_columns]\n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "\n",
    "                            #####\n",
    "                            # Calculate correlation with the target variable\n",
    "                            # correlation = result_df.corr()[target_variable].abs()\n",
    "                            # Select columns whose names end with '_0' or '_1' and have a at least certain absolute correlation with the target\n",
    "                            # selected_columns = correlation[(correlation.index.str.endswith('_0') | correlation.index.str.endswith('_1')) & (correlation >= new_cols_threshold)].index\n",
    "                            # selected_columns = selected_columns.tolist()\n",
    "                            # print(selected_columns)\n",
    "                            # print(len(selected_columns))\n",
    "                            # print(result_df.shape)\n",
    "                            \n",
    "                            # result_df = result_df[data.columns.tolist() + selected_columns]\n",
    "                            #####\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                                        X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                        y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                        y_real = data_test[fict_target]  \n",
    "                                \n",
    "                                        mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                                        rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                        weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                        # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "\n",
    "                                        rmse = [1e5 if (x > 1e5) else x for x in rmse]\n",
    "                                \n",
    "                                        # Add column to list of new columns\n",
    "                                        new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new2 = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                for u in dictionary_case.keys():\n",
    "                                    if ((u + '_' + str(case)) in selected_columns):    \n",
    "                                        names_cols_dataframe_new2.append(u + \"_\" + str(case))\n",
    "                            dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            # Adapt order of columns to the order in which the model was\n",
    "                            # trained\n",
    "                            data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to read the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c99e2664-2d11-4b88-a3e5-1b6d46c42791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(32560, 15)\n",
      "F1 Score normal: [0.67607004 0.9135514 ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.58933195 0.89870992]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(624, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(15, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1371, 5)\n",
      "F1 Score normal: [0.98689956 0.98285714]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.98689956 0.98285714]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(748, 5)\n",
      "F1 Score normal: [0.87218045 0.37037037]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.87218045 0.37037037]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(285, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(568, 32)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(698, 11)\n",
      "F1 Score normal: [0.97222222 0.9375    ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.89393939 0.81578947]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(197, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1727, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(28055, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3195, 37)\n",
      "F1 Score normal: [0.98181818 0.98056156]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.96537678 0.96359743]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(540, 461)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1079, 857)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(434, 17)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(989, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(207, 61)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1472, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(689, 16)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(365, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(10000, 14)\n",
      "F1 Score normal: [0.99947562 0.99908509]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99947617 0.99908341]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(99, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(213, 11)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(305, 4)\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.75757576 0.2       ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(131, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(302, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(293, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(154, 20)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(299, 28)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(582, 11)\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.80851064 0.06896552]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(350, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(149, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19999, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(359, 91)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(31, 57)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19019, 11)\n",
      "F1 Score normal: [0.80639823 0.90904379]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.56926258 0.54873646]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(960, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(8123, 23)\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.62805663 0.3446712 ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3822, 65)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2533, 74)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2535, 74)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(195, 24)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(7493, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(181, 13)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(25009, 11)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1054, 42)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(209, 8)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(245056, 4)\n",
      "F1 Score normal: [0.99515571 0.99810794]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99515571 0.99810794]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(46, 36)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4600, 58)\n",
      "F1 Score normal: [0.95844156 0.93495935]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.85175879 0.74678112]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 23)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 45)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(999, 21)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4434, 37)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(150, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3771, 22)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(214, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3162, 26)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(957, 10)\n",
      "F1 Score normal: [0.98412698 0.96969697]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.97894737 0.95918367]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 3)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 25)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4025, 1)\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(177, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1483, 10)\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares de RF\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Threshold of correlation with target for new columns\n",
    "new_cols_threshold = 0.2\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]   \n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       if (len(lines[x].split(\" = \")) == 1):\n",
    "                           pass\n",
    "                       # If the separator is specified    \n",
    "                       else:\n",
    "                           separator = lines[x].split(\" = \")[1]\n",
    "                           separator = separator.strip()\n",
    "                           if (\"comma\" in separator):\n",
    "                               separator = \",\"\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           target_index = lines[x].split(\" = \")[1]\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               # If there is not header specified, assume\n",
    "                               # default value\n",
    "                               if (len(lines[x].split(\" = \")) == 1):\n",
    "                                    pass\n",
    "                               else:\n",
    "                                   # If it is specified\n",
    "                                   header = lines[x].split(\" = \")[1]\n",
    "                                   # If it is 0 assign it to None\n",
    "                                   if (header.startswith(\"0\")):\n",
    "                                       header = None\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python')\n",
    "                    print(data.shape)\n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = data.columns.tolist()[int(target_index) - 1]\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [n for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "                                        aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        # aux_model = LinearRegression()\n",
    "                                        aux_model.fit(X_train, y_train)\n",
    "                                        dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared = 1\n",
    "                                        else:    \n",
    "                                            r_squared = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "                                        \n",
    "                                        dictionary_aux_r_squared[fict_target] = r_squared\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "\n",
    "                            #####################\n",
    "                            ####################\n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            aux_df = pd.concat([dataframe_new, data[[target_variable]]], axis=1, ignore_index=True)\n",
    "                            aux_df.columns = cols_b + [target_variable]\n",
    "                            \n",
    "                            features = aux_df.drop(target_variable, axis=1)\n",
    "                            target = aux_df[target_variable]\n",
    "                            rf_model_aux = RandomForestClassifier(n_estimators=200, random_state=42, min_samples_split=10, min_samples_leaf=5)\n",
    "                            rf_model_aux.fit(features, target)\n",
    "                            \n",
    "                            feature_importances = rf_model_aux.feature_importances_\n",
    "                            \n",
    "                            # Create a list of (feature_name, importance) tuples\n",
    "                            feature_importance_tuples = zip(features.columns, feature_importances)\n",
    "                                                        \n",
    "                            # Sort the tuples based on importance\n",
    "                            sorted_feature_importance_tuples = sorted(feature_importance_tuples, key=lambda x: x[1], reverse=True)\n",
    "                            \n",
    "                            # Extract sorted feature names\n",
    "                            sorted_feature_names = [feature_name for feature_name, _ in sorted_feature_importance_tuples]\n",
    "\n",
    "                            # Select some features\n",
    "                            selected_columns = sorted_feature_names[0: int(len(sorted_feature_names)/10)]\n",
    "                            ####################\n",
    "                            ####################\n",
    "\n",
    "                            dataframe_new = dataframe_new.loc[:, selected_columns]\n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "\n",
    "                            #####\n",
    "                            # Calculate correlation with the target variable\n",
    "                            # correlation = result_df.corr()[target_variable].abs()\n",
    "                            # Select columns whose names end with '_0' or '_1' and have a at least certain absolute correlation with the target\n",
    "                            # selected_columns = correlation[(correlation.index.str.endswith('_0') | correlation.index.str.endswith('_1')) & (correlation >= new_cols_threshold)].index\n",
    "                            # selected_columns = selected_columns.tolist()\n",
    "                            # print(selected_columns)\n",
    "                            # print(len(selected_columns))\n",
    "                            # print(result_df.shape)\n",
    "                            \n",
    "                            # result_df = result_df[data.columns.tolist() + selected_columns]\n",
    "                            #####\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                                        X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                        y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                        y_real = data_test[fict_target]  \n",
    "                                \n",
    "                                        mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                                        rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                        weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                        # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "\n",
    "                                        rmse = [1e5 if (x > 1e5) else x for x in rmse]\n",
    "                                \n",
    "                                        # Add column to list of new columns\n",
    "                                        new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new2 = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                for u in dictionary_case.keys():\n",
    "                                    if ((u + '_' + str(case)) in selected_columns):    \n",
    "                                        names_cols_dataframe_new2.append(u + \"_\" + str(case))\n",
    "                            dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            # Adapt order of columns to the order in which the model was\n",
    "                            # trained\n",
    "                            data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to read the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3becf-d295-41ce-89b2-a3a5b1e463e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b57e3f-a162-48fe-a962-6e17f1efbdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de nuevas columnas en base a la importancia de las variables\n",
    "# del modelo normal. En concreto, se toman como base el 10% de las variables\n",
    "# con más importancia en el modelo normal y luego se seleccionan para cada\n",
    "# una de ellas la variable nueva terminada en \"_0\" y la terminada en \"_1\".\n",
    "\n",
    "# Al menos usando modelos auxiliares lineales, se observa algunos casos donde\n",
    "# mejora, como en el breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d4467a-85b9-49af-8dc8-71d983cbf786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.69234427 0.91552422]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.64977192 0.91128515]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.98591549 0.98429319]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.98591549 0.98429319]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.78688525 0.31578947]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.78688525 0.31578947]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.94890511 0.90140845]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.95652174 0.91428571]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.98231827 0.97995546]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.98624754 0.9844098 ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [1. 1.]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.75757576 0.2       ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.75968992 0.24390244]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.81151832 0.90430622]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.78669276 0.87982359]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.02520252 0.1838734 ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.99607843 0.99846833]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99607843 0.99846833]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.94626474 0.91816367]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.89815951 0.81514477]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.95145631 0.87804878]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.98989899 0.97777778]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares lineales\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header_option = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]  \n",
    "                   data_url = data_url.strip()\n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       try:\n",
    "                           if (len(lines[x].split(\" = \")) == 1):\n",
    "                               pass\n",
    "                           # If the separator is specified    \n",
    "                           else:\n",
    "                               separator = lines[x].split(\" = \")[1]\n",
    "                               separator = separator.strip()\n",
    "                               if (\"comma\" in separator):\n",
    "                                   separator = \",\"\n",
    "                       except:\n",
    "                           pass\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           try:\n",
    "                               target_index = lines[x].split(\" = \")[1]\n",
    "                               target_index = int(target_index.strip())\n",
    "                           except:\n",
    "                               pass\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               try:\n",
    "                                   # If there is not header specified, assume\n",
    "                                   # default value\n",
    "                                   if (len(lines[x].split(\" = \")) == 1):\n",
    "                                        pass\n",
    "                                   else:\n",
    "                                       # If it is specified\n",
    "                                       header_option = lines[x].split(\" = \")[1]\n",
    "                                       header_option = int(header_option.strip())\n",
    "                                       # If it is 0 assign it to None\n",
    "                                       if (str(header_option).startswith(\"0\")):\n",
    "                                           header_option = None\n",
    "                               except:\n",
    "                                   pass\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python', header=header_option)\n",
    "                    aux_names_columns = [str(aux_n_c) for aux_n_c in data.columns]\n",
    "                    data.columns = aux_names_columns\n",
    "                    \n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = str(data.columns.tolist()[int(target_index) - 1])\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [str(n) for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "\n",
    "                            #######################################\n",
    "                            # Calculations for later selection of new columns\n",
    "                            feature_importances = rf_normal.feature_importances_\n",
    "                            # Create a list of (feature_name, importance) tuples\n",
    "                            feature_importance_tuples = zip(X_train.columns, feature_importances)\n",
    "                            # Sort the tuples based on importance\n",
    "                            sorted_feature_importance_tuples = sorted(feature_importance_tuples, key=lambda x: x[1], reverse=True)\n",
    "                            # Extract sorted feature names\n",
    "                            sorted_feature_names = [feature_name for feature_name, _ in sorted_feature_importance_tuples]\n",
    "                            # Select some features\n",
    "                            base_selected_columns = sorted_feature_names[0: int(len(sorted_feature_names)/10)]\n",
    "                            #######################################\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "                                        # aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        aux_model = LinearRegression()\n",
    "                                        aux_model.fit(X_train, y_train)\n",
    "                                        dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared = 1\n",
    "                                        else:    \n",
    "                                            r_squared = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "                                        \n",
    "                                        dictionary_aux_r_squared[fict_target] = r_squared\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "\n",
    "                            #####################\n",
    "                            ####################\n",
    "                            \n",
    "                            # Select some features\n",
    "                            selected_columns = [] \n",
    "                            for aux_base_cols in base_selected_columns:\n",
    "                                selected_columns.append(aux_base_cols + \"_0\")\n",
    "                                selected_columns.append(aux_base_cols + \"_1\")\n",
    "                            \n",
    "                            ####################\n",
    "                            ####################\n",
    "\n",
    "                            dataframe_new = dataframe_new.loc[:, selected_columns]\n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "\n",
    "                            #####\n",
    "                            # Calculate correlation with the target variable\n",
    "                            # correlation = result_df.corr()[target_variable].abs()\n",
    "                            # Select columns whose names end with '_0' or '_1' and have a at least certain absolute correlation with the target\n",
    "                            # selected_columns = correlation[(correlation.index.str.endswith('_0') | correlation.index.str.endswith('_1')) & (correlation >= new_cols_threshold)].index\n",
    "                            # selected_columns = selected_columns.tolist()\n",
    "                            # print(selected_columns)\n",
    "                            # print(len(selected_columns))\n",
    "                            # print(result_df.shape)\n",
    "                            \n",
    "                            # result_df = result_df[data.columns.tolist() + selected_columns]\n",
    "                            #####\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                                        X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                        y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                        y_real = data_test[fict_target]  \n",
    "                                \n",
    "                                        mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                                        rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                        weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                        # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "\n",
    "                                        rmse = [1e5 if (x > 1e5) else x for x in rmse]\n",
    "                                \n",
    "                                        # Add column to list of new columns\n",
    "                                        new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new2 = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                for u in dictionary_case.keys():\n",
    "                                    if ((u + '_' + str(case)) in selected_columns):    \n",
    "                                        names_cols_dataframe_new2.append(u + \"_\" + str(case))\n",
    "                            dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            # Adapt order of columns to the order in which the model was\n",
    "                            # trained\n",
    "                            data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to process the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3377461e-bcd2-4878-b2ce-5d288a649836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.69234427 0.91552422]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.67910751 0.89159929]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.98591549 0.98429319]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.98591549 0.98429319]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.78688525 0.31578947]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.78688525 0.31578947]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.94890511 0.90140845]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.95652174 0.91428571]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.98231827 0.97995546]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.98245614 0.97977528]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [1. 1.]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.75757576 0.2       ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.76335878 0.20512821]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.81151832 0.90430622]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.68487085 0.7116813 ]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99831366 0.99840256]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.99607843 0.99846833]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99607843 0.99846833]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.94626474 0.91816367]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.89901478 0.81858407]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.95145631 0.87804878]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.97029703 0.93023256]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares RF\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header_option = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]  \n",
    "                   data_url = data_url.strip()\n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       try:\n",
    "                           if (len(lines[x].split(\" = \")) == 1):\n",
    "                               pass\n",
    "                           # If the separator is specified    \n",
    "                           else:\n",
    "                               separator = lines[x].split(\" = \")[1]\n",
    "                               separator = separator.strip()\n",
    "                               if (\"comma\" in separator):\n",
    "                                   separator = \",\"\n",
    "                       except:\n",
    "                           pass\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           try:\n",
    "                               target_index = lines[x].split(\" = \")[1]\n",
    "                               target_index = int(target_index.strip())\n",
    "                           except:\n",
    "                               pass\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               try:\n",
    "                                   # If there is not header specified, assume\n",
    "                                   # default value\n",
    "                                   if (len(lines[x].split(\" = \")) == 1):\n",
    "                                        pass\n",
    "                                   else:\n",
    "                                       # If it is specified\n",
    "                                       header_option = lines[x].split(\" = \")[1]\n",
    "                                       header_option = int(header_option.strip())\n",
    "                                       # If it is 0 assign it to None\n",
    "                                       if (str(header_option).startswith(\"0\")):\n",
    "                                           header_option = None\n",
    "                               except:\n",
    "                                   pass\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python', header=header_option)\n",
    "                    aux_names_columns = [str(aux_n_c) for aux_n_c in data.columns]\n",
    "                    data.columns = aux_names_columns\n",
    "                    \n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = str(data.columns.tolist()[int(target_index) - 1])\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [str(n) for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "\n",
    "                            #######################################\n",
    "                            # Calculations for later selection of new columns\n",
    "                            feature_importances = rf_normal.feature_importances_\n",
    "                            # Create a list of (feature_name, importance) tuples\n",
    "                            feature_importance_tuples = zip(X_train.columns, feature_importances)\n",
    "                            # Sort the tuples based on importance\n",
    "                            sorted_feature_importance_tuples = sorted(feature_importance_tuples, key=lambda x: x[1], reverse=True)\n",
    "                            # Extract sorted feature names\n",
    "                            sorted_feature_names = [feature_name for feature_name, _ in sorted_feature_importance_tuples]\n",
    "                            # Select some features\n",
    "                            base_selected_columns = sorted_feature_names[0: int(len(sorted_feature_names)/10)]\n",
    "                            #######################################\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "                                        aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        # aux_model = LinearRegression()\n",
    "                                        aux_model.fit(X_train, y_train)\n",
    "                                        dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared = 1\n",
    "                                        else:    \n",
    "                                            r_squared = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "                                        \n",
    "                                        dictionary_aux_r_squared[fict_target] = r_squared\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "\n",
    "                            #####################\n",
    "                            ####################\n",
    "                            \n",
    "                            # Select some features\n",
    "                            selected_columns = [] \n",
    "                            for aux_base_cols in base_selected_columns:\n",
    "                                selected_columns.append(aux_base_cols + \"_0\")\n",
    "                                selected_columns.append(aux_base_cols + \"_1\")\n",
    "                            \n",
    "                            ####################\n",
    "                            ####################\n",
    "\n",
    "                            dataframe_new = dataframe_new.loc[:, selected_columns]\n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "\n",
    "                            #####\n",
    "                            # Calculate correlation with the target variable\n",
    "                            # correlation = result_df.corr()[target_variable].abs()\n",
    "                            # Select columns whose names end with '_0' or '_1' and have a at least certain absolute correlation with the target\n",
    "                            # selected_columns = correlation[(correlation.index.str.endswith('_0') | correlation.index.str.endswith('_1')) & (correlation >= new_cols_threshold)].index\n",
    "                            # selected_columns = selected_columns.tolist()\n",
    "                            # print(selected_columns)\n",
    "                            # print(len(selected_columns))\n",
    "                            # print(result_df.shape)\n",
    "                            \n",
    "                            # result_df = result_df[data.columns.tolist() + selected_columns]\n",
    "                            #####\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                                        X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                        y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                        y_real = data_test[fict_target]  \n",
    "                                \n",
    "                                        mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                                        rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                        weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                        # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "\n",
    "                                        rmse = [1e5 if (x > 1e5) else x for x in rmse]\n",
    "                                \n",
    "                                        # Add column to list of new columns\n",
    "                                        new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new2 = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                for u in dictionary_case.keys():\n",
    "                                    if ((u + '_' + str(case)) in selected_columns):    \n",
    "                                        names_cols_dataframe_new2.append(u + \"_\" + str(case))\n",
    "                            dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            # Adapt order of columns to the order in which the model was\n",
    "                            # trained\n",
    "                            data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to process the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c90061-f96a-4580-8672-502fcf0611e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original but choosing for each auxiliary model\n",
    "# the best performance (linear vs RF). It seems\n",
    "# that this giving in general poor results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d9beaec-ed8b-4945-91e4-99184bb6aaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.69234427 0.91552422]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.60213904 0.90572732]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.98591549 0.98429319]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.59482759 0.45348837]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.78688525 0.31578947]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.78688525 0.31578947]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.94890511 0.90140845]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.91729323 0.85333333]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.98231827 0.97995546]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.84513274 0.86166008]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [1. 1.]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.71875    0.18181818]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.79432624 0.        ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.81151832 0.90430622]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.5330911  0.01099505]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [1. 1.]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.99607843 0.99846833]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.41528993 0.85381064]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.94626474 0.91816367]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.80672269 0.62645012]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.95145631 0.87804878]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.81327801 0.04255319]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares RF\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header_option = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]  \n",
    "                   data_url = data_url.strip()\n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       try:\n",
    "                           if (len(lines[x].split(\" = \")) == 1):\n",
    "                               pass\n",
    "                           # If the separator is specified    \n",
    "                           else:\n",
    "                               separator = lines[x].split(\" = \")[1]\n",
    "                               separator = separator.strip()\n",
    "                               if (\"comma\" in separator):\n",
    "                                   separator = \",\"\n",
    "                       except:\n",
    "                           pass\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           try:\n",
    "                               target_index = lines[x].split(\" = \")[1]\n",
    "                               target_index = int(target_index.strip())\n",
    "                           except:\n",
    "                               pass\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               try:\n",
    "                                   # If there is not header specified, assume\n",
    "                                   # default value\n",
    "                                   if (len(lines[x].split(\" = \")) == 1):\n",
    "                                        pass\n",
    "                                   else:\n",
    "                                       # If it is specified\n",
    "                                       header_option = lines[x].split(\" = \")[1]\n",
    "                                       header_option = int(header_option.strip())\n",
    "                                       # If it is 0 assign it to None\n",
    "                                       if (str(header_option).startswith(\"0\")):\n",
    "                                           header_option = None\n",
    "                               except:\n",
    "                                   pass\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python', header=header_option)\n",
    "                    aux_names_columns = [str(aux_n_c) for aux_n_c in data.columns]\n",
    "                    data.columns = aux_names_columns\n",
    "                    \n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = str(data.columns.tolist()[int(target_index) - 1])\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [str(n) for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "\n",
    "                            #######################################\n",
    "                            # Calculations for later selection of new columns\n",
    "                            feature_importances = rf_normal.feature_importances_\n",
    "                            # Create a list of (feature_name, importance) tuples\n",
    "                            feature_importance_tuples = zip(X_train.columns, feature_importances)\n",
    "                            # Sort the tuples based on importance\n",
    "                            sorted_feature_importance_tuples = sorted(feature_importance_tuples, key=lambda x: x[1], reverse=True)\n",
    "                            # Extract sorted feature names\n",
    "                            sorted_feature_names = [feature_name for feature_name, _ in sorted_feature_importance_tuples]\n",
    "                            # Select some features\n",
    "                            base_selected_columns = sorted_feature_names[0: int(len(sorted_feature_names)/10)]\n",
    "                            #######################################\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "\n",
    "                                        # First try RF\n",
    "                                        aux_model_RF = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        aux_model_RF.fit(X_train, y_train)\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model_RF.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared_RF = 1\n",
    "                                        else:    \n",
    "                                            r_squared_RF = 1 - (rss / tss)    \n",
    "\n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "\n",
    "  \n",
    "                                        # Now try linear regression\n",
    "                                        aux_model_linear = LinearRegression()\n",
    "                                        aux_model_linear.fit(X_train, y_train)\n",
    "                                        \n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model_linear.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared_linear  = 1\n",
    "                                        else:    \n",
    "                                            r_squared_linear = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "\n",
    "                                        if (r_squared_RF > r_squared_linear):\n",
    "                                            dictionary_aux[fict_target] = aux_model_RF\n",
    "                                            dictionary_aux_r_squared[fict_target] = r_squared_RF\n",
    "                                        else:\n",
    "                                            dictionary_aux[fict_target] = aux_model_linear\n",
    "                                            dictionary_aux_r_squared[fict_target] = r_squared_linear\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "\n",
    "                            #####################\n",
    "                            ####################\n",
    "                            \n",
    "                            # Select some features\n",
    "                            selected_columns = [] \n",
    "                            for aux_base_cols in base_selected_columns:\n",
    "                                selected_columns.append(aux_base_cols + \"_0\")\n",
    "                                selected_columns.append(aux_base_cols + \"_1\")\n",
    "                            \n",
    "                            ####################\n",
    "                            ####################\n",
    "\n",
    "                            # dataframe_new = dataframe_new.loc[:, selected_columns]\n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "\n",
    "                            #####\n",
    "                            # Calculate correlation with the target variable\n",
    "                            # correlation = result_df.corr()[target_variable].abs()\n",
    "                            # Select columns whose names end with '_0' or '_1' and have a at least certain absolute correlation with the target\n",
    "                            # selected_columns = correlation[(correlation.index.str.endswith('_0') | correlation.index.str.endswith('_1')) & (correlation >= new_cols_threshold)].index\n",
    "                            # selected_columns = selected_columns.tolist()\n",
    "                            # print(selected_columns)\n",
    "                            # print(len(selected_columns))\n",
    "                            # print(result_df.shape)\n",
    "                            \n",
    "                            # result_df = result_df[data.columns.tolist() + selected_columns]\n",
    "                            #####\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    # if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    if True:\n",
    "                                    \n",
    "                                        X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                        y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                        y_real = data_test[fict_target]  \n",
    "                                \n",
    "                                        mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                                        rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                        weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                        # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "\n",
    "                                        rmse = [1e5 if (x > 1e5) else x for x in rmse]\n",
    "                                \n",
    "                                        # Add column to list of new columns\n",
    "                                        new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new2 = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                for u in dictionary_case.keys():\n",
    "                                    # if ((u + '_' + str(case)) in selected_columns):    \n",
    "                                    names_cols_dataframe_new2.append(u + \"_\" + str(case))\n",
    "                            dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            # Adapt order of columns to the order in which the model was\n",
    "                            # trained\n",
    "                            data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to process the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c9bbcf-53c8-4b15-8d2b-1f21be5a58a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing multiple hyperparameters (type of auxiliary model, selection of new variables).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "983e81a4-3ed4-4f32-8f32-748796883e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.7824 +- 0.0037\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.7876 +- 0.0063\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.776 +- 0.0056\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.7908 +- 0.0066\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.7935 +- 0.0061\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.7915 +- 0.0055\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.7888 +- 0.0067\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.774 +- 0.0082\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.7763 +- 0.0077\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.7749 +- 0.0054\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.7753 +- 0.0067\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.7954 +- 0.0067\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.7936 +- 0.0056\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.7755 +- 0.0057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fjlobo\\Desktop\\prueba_missings_IAReproII\\virtual_env\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.779 +- 0.0073\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.795363</td>\n",
       "      <td>0.006742</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.793569</td>\n",
       "      <td>0.005620</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.793462</td>\n",
       "      <td>0.006128</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.791547</td>\n",
       "      <td>0.005530</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.790843</td>\n",
       "      <td>0.006596</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.788783</td>\n",
       "      <td>0.006679</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.787560</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.778967</td>\n",
       "      <td>0.007253</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.776307</td>\n",
       "      <td>0.007660</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.776012</td>\n",
       "      <td>0.005648</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.775488</td>\n",
       "      <td>0.005658</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.775251</td>\n",
       "      <td>0.006743</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.774945</td>\n",
       "      <td>0.005448</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.773987</td>\n",
       "      <td>0.008218</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.795363   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.793569   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.793462   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.791547   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.790843   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.788783   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.787560   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.778967   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.776307   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.776012   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.775488   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.775251   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.774945   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.773987   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "10        0.006742              0.782361                0.00365  \n",
       "11        0.005620              0.782361                0.00365  \n",
       "3         0.006128              0.782361                0.00365  \n",
       "4         0.005530              0.782361                0.00365  \n",
       "2         0.006596              0.782361                0.00365  \n",
       "5         0.006679              0.782361                0.00365  \n",
       "0         0.006325              0.782361                0.00365  \n",
       "13        0.007253              0.782361                0.00365  \n",
       "7         0.007660              0.782361                0.00365  \n",
       "1         0.005648              0.782361                0.00365  \n",
       "12        0.005658              0.782361                0.00365  \n",
       "9         0.006743              0.782361                0.00365  \n",
       "8         0.005448              0.782361                0.00365  \n",
       "6         0.008218              0.782361                0.00365  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.795\n",
      "Optimum case new: type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.992 +- 0.0077\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.9965 +- 0.0034\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.9985 +- 0.0024\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.991 +- 0.0071\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9975 +- 0.0035\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9975 +- 0.0035\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9975 +- 0.0035\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.991 +- 0.0071\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.996 +- 0.004\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.998 +- 0.0036\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.998 +- 0.0036\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9965 +- 0.0034\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9965 +- 0.0047\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9985 +- 0.0024\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9985 +- 0.0024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.998486</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.998486</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.998486</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.997974</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.997974</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.997501</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.997501</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.997501</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.996507</td>\n",
       "      <td>0.004711</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.996500</td>\n",
       "      <td>0.003361</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.996500</td>\n",
       "      <td>0.003361</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.995977</td>\n",
       "      <td>0.003986</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.990955</td>\n",
       "      <td>0.007147</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.990955</td>\n",
       "      <td>0.007147</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.998486   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.998486   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.998486   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.997974   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.997974   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.997501   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.997501   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.997501   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.996507   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.996500   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.996500   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.995977   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.990955   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.990955   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "1         0.002438              0.991955               0.007678  \n",
       "12        0.002438              0.991955               0.007678  \n",
       "13        0.002438              0.991955               0.007678  \n",
       "8         0.003563              0.991955               0.007678  \n",
       "9         0.003563              0.991955               0.007678  \n",
       "3         0.003521              0.991955               0.007678  \n",
       "4         0.003521              0.991955               0.007678  \n",
       "5         0.003521              0.991955               0.007678  \n",
       "11        0.004711              0.991955               0.007678  \n",
       "0         0.003361              0.991955               0.007678  \n",
       "10        0.003361              0.991955               0.007678  \n",
       "7         0.003986              0.991955               0.007678  \n",
       "2         0.007147              0.991955               0.007678  \n",
       "6         0.007147              0.991955               0.007678  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.998\n",
      "Optimum case new: type_aux_mode=randomforest; selection_method_case=None\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.6788 +- 0.0315\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.6025 +- 0.0623\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.6284 +- 0.0435\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.6152 +- 0.0336\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.6277 +- 0.0521\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.6117 +- 0.061\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.6117 +- 0.061\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.6152 +- 0.0336\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.6246 +- 0.0358\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.6099 +- 0.0447\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.6099 +- 0.0447\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.6086 +- 0.0627\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.6062 +- 0.0559\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.6131 +- 0.0407\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.6412 +- 0.0654\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.641212</td>\n",
       "      <td>0.065373</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.628433</td>\n",
       "      <td>0.043542</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.627715</td>\n",
       "      <td>0.052091</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.624614</td>\n",
       "      <td>0.035812</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.615211</td>\n",
       "      <td>0.033604</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.615211</td>\n",
       "      <td>0.033604</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.613056</td>\n",
       "      <td>0.040687</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.611704</td>\n",
       "      <td>0.060978</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.611704</td>\n",
       "      <td>0.060978</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.609923</td>\n",
       "      <td>0.044670</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.609923</td>\n",
       "      <td>0.044670</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.608602</td>\n",
       "      <td>0.062709</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.606201</td>\n",
       "      <td>0.055917</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.602538</td>\n",
       "      <td>0.062269</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.641212   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.628433   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.627715   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.624614   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.615211   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.615211   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.613056   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.611704   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.611704   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.609923   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.609923   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.608602   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.606201   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.602538   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "13        0.065373              0.678828               0.031536  \n",
       "1         0.043542              0.678828               0.031536  \n",
       "3         0.052091              0.678828               0.031536  \n",
       "7         0.035812              0.678828               0.031536  \n",
       "2         0.033604              0.678828               0.031536  \n",
       "6         0.033604              0.678828               0.031536  \n",
       "12        0.040687              0.678828               0.031536  \n",
       "4         0.060978              0.678828               0.031536  \n",
       "5         0.060978              0.678828               0.031536  \n",
       "8         0.044670              0.678828               0.031536  \n",
       "9         0.044670              0.678828               0.031536  \n",
       "10        0.062709              0.678828               0.031536  \n",
       "11        0.055917              0.678828               0.031536  \n",
       "0         0.062269              0.678828               0.031536  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement found\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.963 +- 0.0171\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.9674 +- 0.0129\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.9725 +- 0.0102\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9596 +- 0.0136\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9619 +- 0.0145\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9621 +- 0.0137\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9631 +- 0.0137\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9529 +- 0.0162\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.963 +- 0.0126\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9675 +- 0.0128\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9684 +- 0.0124\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9619 +- 0.0153\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9642 +- 0.0116\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9704 +- 0.0111\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9725 +- 0.0115\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>0.010152</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.972491</td>\n",
       "      <td>0.011458</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.970377</td>\n",
       "      <td>0.011089</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.968351</td>\n",
       "      <td>0.012409</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.967469</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.967364</td>\n",
       "      <td>0.012898</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.964216</td>\n",
       "      <td>0.011639</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.963116</td>\n",
       "      <td>0.013744</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.963019</td>\n",
       "      <td>0.012593</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.962087</td>\n",
       "      <td>0.013695</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.961936</td>\n",
       "      <td>0.014545</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>0.015282</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.959587</td>\n",
       "      <td>0.013619</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.952887</td>\n",
       "      <td>0.016181</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.972500   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.972491   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.970377   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.968351   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.967469   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.967364   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.964216   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.963116   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.963019   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.962087   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.961936   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.961859   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.959587   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.952887   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "1         0.010152              0.962994               0.017077  \n",
       "13        0.011458              0.962994               0.017077  \n",
       "12        0.011089              0.962994               0.017077  \n",
       "9         0.012409              0.962994               0.017077  \n",
       "8         0.012784              0.962994               0.017077  \n",
       "0         0.012898              0.962994               0.017077  \n",
       "11        0.011639              0.962994               0.017077  \n",
       "5         0.013744              0.962994               0.017077  \n",
       "7         0.012593              0.962994               0.017077  \n",
       "4         0.013695              0.962994               0.017077  \n",
       "3         0.014545              0.962994               0.017077  \n",
       "10        0.015282              0.962994               0.017077  \n",
       "2         0.013619              0.962994               0.017077  \n",
       "6         0.016181              0.962994               0.017077  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.973\n",
      "Optimum case new: type_aux_mode=randomforest; selection_method_case=None\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.9818 +- 0.0085\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.9765 +- 0.0102\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.9784 +- 0.0096\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9805 +- 0.0083\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9786 +- 0.0109\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9761 +- 0.0101\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9757 +- 0.0091\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9792 +- 0.0094\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9761 +- 0.0102\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9786 +- 0.01\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9784 +- 0.0079\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9788 +- 0.0092\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.979 +- 0.0081\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9828 +- 0.0093\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.979 +- 0.0081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.982818</td>\n",
       "      <td>0.009266</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.980500</td>\n",
       "      <td>0.008253</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.979246</td>\n",
       "      <td>0.009410</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.979037</td>\n",
       "      <td>0.008137</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.979037</td>\n",
       "      <td>0.008137</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.978824</td>\n",
       "      <td>0.009212</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.978630</td>\n",
       "      <td>0.009979</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.978625</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.978429</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.978418</td>\n",
       "      <td>0.007891</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.976514</td>\n",
       "      <td>0.010182</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.976107</td>\n",
       "      <td>0.010227</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.976100</td>\n",
       "      <td>0.010103</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.975687</td>\n",
       "      <td>0.009080</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.982818   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.980500   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.979246   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.979037   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.979037   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.978824   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.978630   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.978625   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.978429   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.978418   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.976514   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.976107   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.976100   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.975687   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "12        0.009266              0.981764                0.00846  \n",
       "2         0.008253              0.981764                0.00846  \n",
       "6         0.009410              0.981764                0.00846  \n",
       "11        0.008137              0.981764                0.00846  \n",
       "13        0.008137              0.981764                0.00846  \n",
       "10        0.009212              0.981764                0.00846  \n",
       "8         0.009979              0.981764                0.00846  \n",
       "3         0.010858              0.981764                0.00846  \n",
       "1         0.009567              0.981764                0.00846  \n",
       "9         0.007891              0.981764                0.00846  \n",
       "0         0.010182              0.981764                0.00846  \n",
       "7         0.010227              0.981764                0.00846  \n",
       "4         0.010103              0.981764                0.00846  \n",
       "5         0.009080              0.981764                0.00846  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.983\n",
      "Optimum case new: type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\echocardiogram\\config.ini could not be processed.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.9997 +- 0.0004\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.9996 +- 0.0005\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.9996 +- 0.0005\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9998 +- 0.0003\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9997 +- 0.0004\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9996 +- 0.0005\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9996 +- 0.0006\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9997 +- 0.0005\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9997 +- 0.0004\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9996 +- 0.0004\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9996 +- 0.0004\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9996 +- 0.0005\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9998 +- 0.0003\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9996 +- 0.0005\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9996 +- 0.0005\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.999783</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.999783</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.999710</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.999638</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.999638</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.999566</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.999566</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.999566</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.999565</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.999783   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.999783   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.999711   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.999711   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.999710   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.999639   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.999639   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.999639   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.999638   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.999638   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.999566   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.999566   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.999566   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.999565   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "2         0.000349              0.999711               0.000374  \n",
       "11        0.000349              0.999711               0.000374  \n",
       "6         0.000505              0.999711               0.000374  \n",
       "3         0.000374              0.999711               0.000374  \n",
       "7         0.000375              0.999711               0.000374  \n",
       "0         0.000510              0.999711               0.000374  \n",
       "4         0.000510              0.999711               0.000374  \n",
       "10        0.000510              0.999711               0.000374  \n",
       "8         0.000381              0.999711               0.000374  \n",
       "9         0.000381              0.999711               0.000374  \n",
       "1         0.000505              0.999711               0.000374  \n",
       "12        0.000505              0.999711               0.000374  \n",
       "13        0.000505              0.999711               0.000374  \n",
       "5         0.000611              0.999711               0.000374  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 1.0\n",
      "Optimum case new: type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.6405 +- 0.1033\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.5397 +- 0.0759\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.5187 +- 0.0786\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.5394 +- 0.0713\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.5394 +- 0.0713\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.5836 +- 0.0672\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.5307 +- 0.0479\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.5394 +- 0.0713\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.5394 +- 0.0713\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.542 +- 0.0688\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.5413 +- 0.0746\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.5591 +- 0.0605\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.5821 +- 0.0529\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.503 +- 0.0807\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.5748 +- 0.1039\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.583612</td>\n",
       "      <td>0.067176</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.582095</td>\n",
       "      <td>0.052870</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.574805</td>\n",
       "      <td>0.103937</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>0.060452</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.541997</td>\n",
       "      <td>0.068772</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.541265</td>\n",
       "      <td>0.074614</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.539732</td>\n",
       "      <td>0.075890</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.539376</td>\n",
       "      <td>0.071329</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.539376</td>\n",
       "      <td>0.071329</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.539376</td>\n",
       "      <td>0.071329</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.539376</td>\n",
       "      <td>0.071329</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.530662</td>\n",
       "      <td>0.047914</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.518716</td>\n",
       "      <td>0.078636</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.502955</td>\n",
       "      <td>0.080691</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.583612   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.582095   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.574805   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.559097   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.541997   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.541265   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.539732   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.539376   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.539376   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.539376   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.539376   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.530662   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.518716   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.502955   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "4         0.067176              0.640499               0.103315  \n",
       "11        0.052870              0.640499               0.103315  \n",
       "13        0.103937              0.640499               0.103315  \n",
       "10        0.060452              0.640499               0.103315  \n",
       "8         0.068772              0.640499               0.103315  \n",
       "9         0.074614              0.640499               0.103315  \n",
       "0         0.075890              0.640499               0.103315  \n",
       "2         0.071329              0.640499               0.103315  \n",
       "3         0.071329              0.640499               0.103315  \n",
       "6         0.071329              0.640499               0.103315  \n",
       "7         0.071329              0.640499               0.103315  \n",
       "5         0.047914              0.640499               0.103315  \n",
       "1         0.078636              0.640499               0.103315  \n",
       "12        0.080691              0.640499               0.103315  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement found\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\image-segmentation\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.6293 +- 0.0523\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.5153 +- 0.0422\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.5772 +- 0.0577\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.5588 +- 0.0482\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.5639 +- 0.0424\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.5523 +- 0.0622\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.5607 +- 0.0342\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.5795 +- 0.0468\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.6014 +- 0.0636\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.5946 +- 0.0382\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.6067 +- 0.0579\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.5418 +- 0.0231\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.5481 +- 0.0642\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.5575 +- 0.0453\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.5917 +- 0.0483\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.606677</td>\n",
       "      <td>0.057884</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.601390</td>\n",
       "      <td>0.063596</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.594648</td>\n",
       "      <td>0.038173</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.591710</td>\n",
       "      <td>0.048290</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.579550</td>\n",
       "      <td>0.046795</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.577245</td>\n",
       "      <td>0.057708</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.563932</td>\n",
       "      <td>0.042367</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.560705</td>\n",
       "      <td>0.034196</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.558754</td>\n",
       "      <td>0.048250</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.557468</td>\n",
       "      <td>0.045289</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.552256</td>\n",
       "      <td>0.062219</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.548142</td>\n",
       "      <td>0.064215</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.541801</td>\n",
       "      <td>0.023140</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.515277</td>\n",
       "      <td>0.042198</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.606677   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.601390   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.594648   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.591710   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.579550   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.577245   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.563932   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.560705   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.558754   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.557468   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.552256   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.548142   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.541801   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.515277   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "9         0.057884              0.629257               0.052268  \n",
       "7         0.063596              0.629257               0.052268  \n",
       "8         0.038173              0.629257               0.052268  \n",
       "13        0.048290              0.629257               0.052268  \n",
       "6         0.046795              0.629257               0.052268  \n",
       "1         0.057708              0.629257               0.052268  \n",
       "3         0.042367              0.629257               0.052268  \n",
       "5         0.034196              0.629257               0.052268  \n",
       "2         0.048250              0.629257               0.052268  \n",
       "12        0.045289              0.629257               0.052268  \n",
       "4         0.062219              0.629257               0.052268  \n",
       "11        0.064215              0.629257               0.052268  \n",
       "10        0.023140              0.629257               0.052268  \n",
       "0         0.042198              0.629257               0.052268  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement found\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.8564 +- 0.007\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.8581 +- 0.0047\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.8363 +- 0.0045\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.8533 +- 0.006\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.859 +- 0.0052\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.8586 +- 0.0049\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.8595 +- 0.0061\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.8061 +- 0.0106\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.8276 +- 0.0053\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.8409 +- 0.0049\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.8396 +- 0.0067\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.8591 +- 0.0044\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.858 +- 0.0057\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.829 +- 0.0045\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.829 +- 0.0045\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>0.006124</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.859072</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.858987</td>\n",
       "      <td>0.005207</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.858576</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.858103</td>\n",
       "      <td>0.004660</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.857954</td>\n",
       "      <td>0.005680</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.853276</td>\n",
       "      <td>0.006046</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.840889</td>\n",
       "      <td>0.004936</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.839561</td>\n",
       "      <td>0.006717</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.836325</td>\n",
       "      <td>0.004466</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.828987</td>\n",
       "      <td>0.004490</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.828987</td>\n",
       "      <td>0.004490</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.827597</td>\n",
       "      <td>0.005294</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.806103</td>\n",
       "      <td>0.010594</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.859456   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.859072   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.858987   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.858576   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.858103   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.857954   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.853276   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.840889   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.839561   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.836325   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.828987   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.828987   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.827597   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.806103   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "5         0.006124              0.856379               0.007044  \n",
       "10        0.004448              0.856379               0.007044  \n",
       "3         0.005207              0.856379               0.007044  \n",
       "4         0.004886              0.856379               0.007044  \n",
       "0         0.004660              0.856379               0.007044  \n",
       "11        0.005680              0.856379               0.007044  \n",
       "2         0.006046              0.856379               0.007044  \n",
       "8         0.004936              0.856379               0.007044  \n",
       "9         0.006717              0.856379               0.007044  \n",
       "1         0.004466              0.856379               0.007044  \n",
       "12        0.004490              0.856379               0.007044  \n",
       "13        0.004490              0.856379               0.007044  \n",
       "7         0.005294              0.856379               0.007044  \n",
       "6         0.010594              0.856379               0.007044  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.859\n",
      "Optimum case new: type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 1.0 +- 0.0\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "1.0 +- 0.0\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "1.0 +- 0.0\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "1.0 +- 0.0\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "1.0 +- 0.0\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "1.0 +- 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "0    type_aux_mode=linear; selection_method_case=None            1.0   \n",
       "1   type_aux_mode=randomforest; selection_method_c...            1.0   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...            1.0   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...            1.0   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...            1.0   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...            1.0   \n",
       "6   type_aux_mode=randomforest; selection_method_c...            1.0   \n",
       "7   type_aux_mode=randomforest; selection_method_c...            1.0   \n",
       "8   type_aux_mode=randomforest; selection_method_c...            1.0   \n",
       "9   type_aux_mode=randomforest; selection_method_c...            1.0   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...            1.0   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...            1.0   \n",
       "12  type_aux_mode_case=randomforest; selection_met...            1.0   \n",
       "13  type_aux_mode_case=randomforest; selection_met...            1.0   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "0              0.0                   1.0                    0.0  \n",
       "1              0.0                   1.0                    0.0  \n",
       "2              0.0                   1.0                    0.0  \n",
       "3              0.0                   1.0                    0.0  \n",
       "4              0.0                   1.0                    0.0  \n",
       "5              0.0                   1.0                    0.0  \n",
       "6              0.0                   1.0                    0.0  \n",
       "7              0.0                   1.0                    0.0  \n",
       "8              0.0                   1.0                    0.0  \n",
       "9              0.0                   1.0                    0.0  \n",
       "10             0.0                   1.0                    0.0  \n",
       "11             0.0                   1.0                    0.0  \n",
       "12             0.0                   1.0                    0.0  \n",
       "13             0.0                   1.0                    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement found\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\polish-bankruptcy\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\seismic-bumps\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.9965 +- 0.0007\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.9975 +- 0.0006\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.994 +- 0.0013\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.997 +- 0.0007\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.997 +- 0.0007\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9966 +- 0.0008\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9972 +- 0.0008\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.997 +- 0.0007\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.997 +- 0.0007\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9924 +- 0.001\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9927 +- 0.0014\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9975 +- 0.0006\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9973 +- 0.0005\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.994 +- 0.0013\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.994 +- 0.0013\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.997542</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.997542</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.997258</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.997210</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.997050</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.997050</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.997050</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.997050</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.996608</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.993955</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.993955</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.993955</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.992687</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.992357</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "0    type_aux_mode=linear; selection_method_case=None       0.997542   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.997542   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.997258   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.997210   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.997050   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.997050   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.997050   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.997050   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.996608   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.993955   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.993955   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.993955   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.992687   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.992357   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "0         0.000600               0.99653               0.000692  \n",
       "10        0.000600               0.99653               0.000692  \n",
       "11        0.000525               0.99653               0.000692  \n",
       "5         0.000753               0.99653               0.000692  \n",
       "2         0.000742               0.99653               0.000692  \n",
       "3         0.000742               0.99653               0.000692  \n",
       "6         0.000742               0.99653               0.000692  \n",
       "7         0.000742               0.99653               0.000692  \n",
       "4         0.000792               0.99653               0.000692  \n",
       "1         0.001285               0.99653               0.000692  \n",
       "12        0.001285               0.99653               0.000692  \n",
       "13        0.001285               0.99653               0.000692  \n",
       "9         0.001362               0.99653               0.000692  \n",
       "8         0.000956               0.99653               0.000692  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.998\n",
      "Optimum case new: type_aux_mode=linear; selection_method_case=None\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.9367 +- 0.0106\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.9331 +- 0.0109\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.9407 +- 0.0108\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9316 +- 0.0116\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9325 +- 0.0096\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9351 +- 0.0104\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9326 +- 0.0105\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9292 +- 0.0146\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9372 +- 0.0091\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9401 +- 0.0099\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9391 +- 0.0116\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9341 +- 0.0122\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9285 +- 0.0128\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9382 +- 0.01\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9357 +- 0.0115\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.940708</td>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.940117</td>\n",
       "      <td>0.009925</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.939094</td>\n",
       "      <td>0.011571</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.938246</td>\n",
       "      <td>0.010035</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.937165</td>\n",
       "      <td>0.009067</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.935706</td>\n",
       "      <td>0.011547</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.935061</td>\n",
       "      <td>0.010414</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.934132</td>\n",
       "      <td>0.012193</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.933097</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.932568</td>\n",
       "      <td>0.010459</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.932527</td>\n",
       "      <td>0.009622</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.931593</td>\n",
       "      <td>0.011565</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.929203</td>\n",
       "      <td>0.014608</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.928505</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.940708   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.940117   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.939094   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.938246   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.937165   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.935706   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.935061   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.934132   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.933097   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.932568   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.932527   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.931593   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.929203   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.928505   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "1         0.010840              0.936735               0.010633  \n",
       "8         0.009925              0.936735               0.010633  \n",
       "9         0.011571              0.936735               0.010633  \n",
       "12        0.010035              0.936735               0.010633  \n",
       "7         0.009067              0.936735               0.010633  \n",
       "13        0.011547              0.936735               0.010633  \n",
       "4         0.010414              0.936735               0.010633  \n",
       "10        0.012193              0.936735               0.010633  \n",
       "0         0.010941              0.936735               0.010633  \n",
       "5         0.010459              0.936735               0.010633  \n",
       "3         0.009622              0.936735               0.010633  \n",
       "2         0.011565              0.936735               0.010633  \n",
       "6         0.014608              0.936735               0.010633  \n",
       "11        0.012820              0.936735               0.010633  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.941\n",
      "Optimum case new: type_aux_mode=randomforest; selection_method_case=None\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\statlog-shuttle\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\thoracic-surgery\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\thyroid-disease-allbp\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\thyroid-disease-allhyper\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\thyroid-disease-allrep\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\thyroid-disease-dis\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\thyroid-disease-sick\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.9544 +- 0.0151\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.9753 +- 0.0089\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.7949 +- 0.0563\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9753 +- 0.0089\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9753 +- 0.0089\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9753 +- 0.0089\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9753 +- 0.0089\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.8742 +- 0.0328\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.8493 +- 0.0279\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.8364 +- 0.0404\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.8019 +- 0.044\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9753 +- 0.0089\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9753 +- 0.0089\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.8869 +- 0.0508\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.8869 +- 0.0508\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.975289</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.975289</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.975289</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.975289</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.975289</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.975289</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.975289</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.886910</td>\n",
       "      <td>0.050814</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.886910</td>\n",
       "      <td>0.050814</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.874202</td>\n",
       "      <td>0.032825</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.849307</td>\n",
       "      <td>0.027895</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.836411</td>\n",
       "      <td>0.040393</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.801887</td>\n",
       "      <td>0.044024</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.794865</td>\n",
       "      <td>0.056349</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "0    type_aux_mode=linear; selection_method_case=None       0.975289   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.975289   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.975289   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.975289   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.975289   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.975289   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.975289   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.886910   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.886910   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.874202   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.849307   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.836411   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.801887   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.794865   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "0         0.008928              0.954425               0.015065  \n",
       "2         0.008928              0.954425               0.015065  \n",
       "3         0.008928              0.954425               0.015065  \n",
       "4         0.008928              0.954425               0.015065  \n",
       "5         0.008928              0.954425               0.015065  \n",
       "10        0.008928              0.954425               0.015065  \n",
       "11        0.008928              0.954425               0.015065  \n",
       "12        0.050814              0.954425               0.015065  \n",
       "13        0.050814              0.954425               0.015065  \n",
       "6         0.032825              0.954425               0.015065  \n",
       "7         0.027895              0.954425               0.015065  \n",
       "8         0.040393              0.954425               0.015065  \n",
       "9         0.044024              0.954425               0.015065  \n",
       "1         0.056349              0.954425               0.015065  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.975\n",
      "Optimum case new: type_aux_mode=linear; selection_method_case=None\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import statistics\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "def aux_func(data, target_variable, only_normal=False, type_aux_mod=\"linear\", selection_method=None, \n",
    "             frac_feature_imp_normal=0.1, new_cols_corr_thr=0.2, seed=123):\n",
    "    \n",
    "    #################################\n",
    "    # PREPROCESSING\n",
    "    data = data.drop_duplicates()\n",
    "                            \n",
    "    # Handling missing values (drop rows with missing values for simplicity)\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    # Encoding categorical variables using one-hot encoding (OHE).\n",
    "    # First convert the target into categorical. This facilitates later\n",
    "    # calculations\n",
    "    data[target_variable] = data[target_variable].astype(\"category\")\n",
    "    # Now apply OHE to categorical columns (including the target)\n",
    "    data = pd.get_dummies(data)\n",
    "\n",
    "    # After OHE there will be two columns created for the target (since it was binary).\n",
    "    # Choose one of those columns as target and discard the other one. It is not important\n",
    "    # which of the two is selected as target, since the F1-score performance is measured\n",
    "    # using 'macro' average option\n",
    "    aux_names_target = [str(n) for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "    target_variable = aux_names_target[0]\n",
    "    data = data.drop(columns=[aux_names_target[1]])\n",
    "                            \n",
    "    # Normalizing numerical variables. Since previously OHE\n",
    "    # was performed, all variables with more than 2 different\n",
    "    # values will be numerical\n",
    "    numerical_columns = []\n",
    "    non_numerical_columns = []\n",
    "    for aux_num in data.columns:\n",
    "        unique_values_count = data[aux_num].nunique()\n",
    "        if (unique_values_count > 2):\n",
    "            numerical_columns.append(aux_num)\n",
    "        else:\n",
    "            non_numerical_columns.append(aux_num)\n",
    "    # Normalization\n",
    "    if (len(numerical_columns) != 0):\n",
    "        scaler = StandardScaler()\n",
    "        aux_data = scaler.fit_transform(data[numerical_columns])\n",
    "        aux_data = pd.DataFrame(aux_data, columns=numerical_columns)\n",
    "        aux_data = aux_data.reset_index(drop=True)\n",
    "        data_non_numerical = data[non_numerical_columns].reset_index(drop=True)\n",
    "        data = pd.concat([aux_data, data_non_numerical], axis=1)\n",
    "                                                                                \n",
    "\n",
    "    # Shuffle the DataFrame to randomize the rows\n",
    "    data = data.sample(frac=1, random_state=seed)  \n",
    "                            \n",
    "    # Save some registers for testing performance:\n",
    "    data_test = data.sample(frac=0.15, random_state=42)\n",
    "    data = data.drop(data_test.index)\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_counts = data[target_variable].value_counts()\n",
    "    total_samples = class_counts.sum()\n",
    "    class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "\n",
    "    # Define custom scorer\n",
    "    average_metric_type = 'macro'\n",
    "    custom_scorer = make_scorer(f1_score, average=average_metric_type)\n",
    "\n",
    "    #################################\n",
    "    # NORMAL MODEL\n",
    "\n",
    "    # If the function only returns the normal performance or the selection\n",
    "    # method of new variables is feature_imp_normal\n",
    "    if ((only_normal) | (selection_method is not None)):\n",
    "\n",
    "        X_normal = data.drop(columns=[target_variable])\n",
    "        y_normal = data[target_variable] \n",
    "\n",
    "        # Define RandomForestClassifier\n",
    "        rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        \n",
    "        # Define the parameter grid for grid search\n",
    "        min_samples_split_grid_search = [5, 10, 15]\n",
    "        min_samples_leaf_grid_search = [5, 10, 15]\n",
    "        param_grid = {\n",
    "            'min_samples_split': min_samples_split_grid_search,\n",
    "            'min_samples_leaf' : min_samples_leaf_grid_search\n",
    "        }\n",
    "        \n",
    "        # Perform grid search        \n",
    "        grid_search = GridSearchCV(estimator=rf_normal, param_grid=param_grid,\n",
    "                                   scoring=custom_scorer, cv=5, n_jobs=-1)\n",
    "        # The normal model takes imbalance into account to generate \n",
    "        # maximum normal performance\n",
    "        grid_search.fit(X_normal, y_normal, sample_weight=y_normal.map(class_weights))\n",
    "        \n",
    "        # Get the best normal model\n",
    "        rf_normal = grid_search.best_estimator_\n",
    "\n",
    "        # If the call to the function was to just calculate the\n",
    "        # normal performance\n",
    "        if (only_normal):\n",
    "            features_test_normal = data_test.drop(target_variable, axis=1)\n",
    "            target_test_normal = data_test[target_variable]\n",
    "            predictions_normal = rf_normal.predict(features_test_normal)\n",
    "            normal_score = f1_score(target_test_normal, predictions_normal, average=average_metric_type)\n",
    "                           \n",
    "            return (normal_score)\n",
    "    \n",
    "    \n",
    "    #######################################\n",
    "        \n",
    "                                                \n",
    "    #################################\n",
    "    # GENERATION OF AUXILIARY MODELS\n",
    "        \n",
    "    # List of dictionaries\n",
    "    list_of_dictionaries = []\n",
    "                            \n",
    "    # For each value of the target\n",
    "    for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                \n",
    "        # Generate auxiliary dataset\n",
    "        dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "        # Discard target in auxiliary dataset\n",
    "        dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "        # Generate dictionary of ficticious targets and the models that predict them:\n",
    "        dictionary_aux = {}\n",
    "                                \n",
    "        for fict_target in dataset_aux.columns.tolist():\n",
    "        \n",
    "            # Train auxiliary model and save it\n",
    "            X = dataset_aux.drop(columns=[fict_target])\n",
    "            y = dataset_aux[fict_target]                                 \n",
    "\n",
    "            if (type_aux_mod == \"linear\"):\n",
    "                aux_model = LinearRegression(n_jobs=-1)\n",
    "            else:\n",
    "                if (type_aux_mod == \"randomforest\"):\n",
    "                    aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5, n_jobs=-1)\n",
    "                else:\n",
    "                    print(\"Error: current allowed type for auxiliary models is 'linear' or 'randomforest'\")\n",
    "                    sys.exit()\n",
    "            \n",
    "            aux_model.fit(X, y)\n",
    "                                        \n",
    "            dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                    \n",
    "        list_of_dictionaries.append(dictionary_aux)    \n",
    "            \n",
    "\n",
    "    #################################\n",
    "    # GENERATION OF NEW COLUMNS BASED ON\n",
    "    # AUXILIARY MODELS\n",
    "        \n",
    "    list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "    list_of_rows_dataframe_new = []\n",
    "                          \n",
    "    new_columns = []\n",
    "\n",
    "    # For each value of the target\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "                              \n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "                                    \n",
    "        for fict_target in dictionary_case:\n",
    "                                    \n",
    "            X = data.drop(columns=[target_variable, fict_target])\n",
    "            y_predicted = dictionary_case[fict_target].predict(X)\n",
    "            y_real = data[fict_target]  \n",
    "                            \n",
    "            mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "            rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                          \n",
    "            # Add column to list of new columns\n",
    "            new_columns.append(rmse)\n",
    "\n",
    "        \n",
    "    dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "    names_cols_dataframe_new = []\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "        for u in dictionary_case.keys():\n",
    "            names_cols_dataframe_new.append(u + \"_\" + str(case))\n",
    "    dataframe_new.columns = names_cols_dataframe_new\n",
    "\n",
    "    cols_a = data.columns.to_list()\n",
    "    cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "    data = data.reset_index(drop=True)\n",
    "    dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                           \n",
    "    # Concatenate horizontally\n",
    "    result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "    result_df.columns = cols_b + cols_a    \n",
    "                            \n",
    "    #################################\n",
    "    # SELECTION OF NEW COLUMNS CREATED TO CREATE\n",
    "    # AUGMENTED DF.\n",
    "\n",
    "    if (selection_method == \"feature_imp_normal\"):\n",
    "        # Calculations for later selection of new columns\n",
    "        feature_importances = rf_normal.feature_importances_\n",
    "        # Create a list of (feature_name, importance) tuples\n",
    "        feature_importance_tuples = zip(X_normal.columns, feature_importances)\n",
    "        # Sort the tuples based on importance\n",
    "        sorted_feature_importance_tuples = sorted(feature_importance_tuples, key=lambda x: x[1], reverse=True)\n",
    "        # Extract sorted feature names\n",
    "        sorted_feature_names = [feature_name for feature_name, _ in sorted_feature_importance_tuples]\n",
    "        # Select some features\n",
    "        base_selected_columns = sorted_feature_names[0: int(len(sorted_feature_names) * frac_feature_imp_normal)]\n",
    "        \n",
    "        selected_columns = [] \n",
    "        for aux_base_cols in base_selected_columns:\n",
    "            selected_columns.append(aux_base_cols + \"_0\")\n",
    "            selected_columns.append(aux_base_cols + \"_1\")\n",
    "        \n",
    "    else:          \n",
    "        if (selection_method == \"correlation_target\"):\n",
    "            # Obtain new variables that have at least certain absolute correlation with the target\n",
    "            correlation = result_df.corr()[target_variable].abs()\n",
    "            # Select columns that have a at least certain absolute correlation with the target\n",
    "            selected_columns = correlation[correlation >= new_cols_corr_thr].index\n",
    "            selected_columns = [w for w in selected_columns if (w in dataframe_new.columns)]\n",
    "            \n",
    "        else:\n",
    "            if (selection_method is None):\n",
    "                # Select all created new columns\n",
    "                selected_columns = dataframe_new.columns.tolist()            \n",
    "                \n",
    "            else:\n",
    "                print(\"Error. The parameter selection_method must be 'feature_imp_normal', 'correlation_target' or None\")\n",
    "                sys.exit()\n",
    "                    \n",
    "\n",
    "    # The augmented dataframe will contain the original columns plus the \n",
    "    # selected new columns\n",
    "    result_df = result_df.loc[:, data.columns.tolist() + selected_columns]\n",
    "\n",
    "                            \n",
    "\n",
    "    #################################\n",
    "    # TRAINING OF MODEL BASED ON AUGMENTED DF.\n",
    "\n",
    "    features = result_df.drop(target_variable, axis=1)\n",
    "    target = result_df[target_variable]\n",
    "    \n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        \n",
    "    # Define the parameter grid for grid search\n",
    "    min_samples_split_grid_search = [5, 10, 15]\n",
    "    min_samples_leaf_grid_search = [5, 10, 15]\n",
    "    param_grid = {\n",
    "        'min_samples_split': min_samples_split_grid_search,\n",
    "        'min_samples_leaf' : min_samples_leaf_grid_search\n",
    "    }\n",
    "        \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                              scoring=custom_scorer, cv=5, n_jobs=-1)\n",
    "    # It is not advised to apply imbalance correction to the enhanced dataframe\n",
    "    # grid_search.fit(features, target, sample_weight=target.map(class_weights))\n",
    "    # Instead, normal fit is advisable to generate better performance\n",
    "    grid_search.fit(features, target)\n",
    "    \n",
    "    # Get the best model and corresponding score\n",
    "    rf_model = grid_search.best_estimator_\n",
    "                            \n",
    "\n",
    "    #################################\n",
    "    # PROCESSING OF TEST DATASET SO THAT THE TRAINED MODEL\n",
    "    # CAN BE APPLIED TO IT, AND IN THIS WAY OBTAIN \n",
    "    # PERFORMANCE METRICS\n",
    "    \n",
    "    # For each value of the target\n",
    "    new_columns = []\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "        \n",
    "        for fict_target in dictionary_case:\n",
    "            if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                y_real = data_test[fict_target]  \n",
    "                               \n",
    "                mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                \n",
    "                # Add column to list of new columns\n",
    "                new_columns.append(rmse)\n",
    "                            \n",
    "    dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "    names_cols_dataframe_new2 = []\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "        for fict_target in dictionary_case:\n",
    "            if ((fict_target + '_' + str(case)) in selected_columns):    \n",
    "                names_cols_dataframe_new2.append(fict_target + \"_\" + str(case))\n",
    "    dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "    cols_a = data_test.columns.to_list()\n",
    "    cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "    data_test = data_test.reset_index(drop=True)\n",
    "    dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                                                    \n",
    "    # Concatenate horizontally\n",
    "    data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "    data_test_processed.columns = cols_b + cols_a\n",
    "    # Adapt order of columns to the order in which the model was\n",
    "    # trained\n",
    "    data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                         \n",
    "    # Now apply trained model on test dataset to gauge performance\n",
    "    features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "    target_test = data_test_processed[target_variable]\n",
    "    predictions = rf_model.predict(features_test)\n",
    "                           \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(target_test, predictions, average=average_metric_type)\n",
    "    return (f1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "\n",
    "# TEST ON DATAFRAMES FROM UCI, DOWNLOAD ADAPTED FROM Perales-González, Carlos, (2020). UCI download-process, v1.3, GitHub repository, https://github.com/cperales/uci-download-process\n",
    "\n",
    "# Number of statistical repetitions\n",
    "num_stat_rep = 10\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        \n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header_option = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]  \n",
    "                   data_url = data_url.strip()\n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       try:\n",
    "                           if (len(lines[x].split(\" = \")) == 1):\n",
    "                               pass\n",
    "                           # If the separator is specified    \n",
    "                           else:\n",
    "                               separator = lines[x].split(\" = \")[1]\n",
    "                               separator = separator.strip()\n",
    "                               if (\"comma\" in separator):\n",
    "                                   separator = \",\"\n",
    "                       except:\n",
    "                           pass\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           try:\n",
    "                               target_index = lines[x].split(\" = \")[1]\n",
    "                               target_index = int(target_index.strip())\n",
    "                           except:\n",
    "                               pass\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               try:\n",
    "                                   # If there is not header specified, assume\n",
    "                                   # default value\n",
    "                                   if (len(lines[x].split(\" = \")) == 1):\n",
    "                                        pass\n",
    "                                   else:\n",
    "                                       # If it is specified\n",
    "                                       header_option = lines[x].split(\" = \")[1]\n",
    "                                       header_option = int(header_option.strip())\n",
    "                                       # If it is 0 assign it to None\n",
    "                                       if (str(header_option).startswith(\"0\")):\n",
    "                                           header_option = None\n",
    "                               except:\n",
    "                                   pass\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            try:\n",
    "                data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python', header=header_option)\n",
    "                aux_names_columns = [str(aux_n_c) for aux_n_c in data.columns]\n",
    "                data.columns = aux_names_columns\n",
    "                    \n",
    "                # If there will be enough dimensionality after one hot encoding\n",
    "                data_check = pd.get_dummies(data)\n",
    "                if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                    # Name of the target of the dataset (target_index - 1 since \n",
    "                    # in python first position is 0)\n",
    "                    target_variable = str(data.columns.tolist()[int(target_index) - 1])\n",
    "        \n",
    "                    # If it is a binary classification\n",
    "                    unique_values_count = data[target_variable].nunique()\n",
    "                    if (unique_values_count == 2):\n",
    "                        \n",
    "                        normal_performance = []\n",
    "                        for c in range(0, num_stat_rep):\n",
    "                            normal_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                           only_normal=True, seed=c))\n",
    "\n",
    "                        normal_performance_stdev = statistics.stdev(normal_performance)\n",
    "                        normal_performance = statistics.mean(normal_performance)\n",
    "                        print(\"Normal performance: \" + str(round(normal_performance, 4)) + \" +- \" + str(round(normal_performance_stdev, 4)))\n",
    "                        \n",
    "                        cases_new = [\"type_aux_mode=linear; selection_method_case=None\"]\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=None\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\")\n",
    "                        cases_new.append(\"type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\")\n",
    "                        cases_new.append(\"type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\")\n",
    "                        cases_new.append(\"type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\")\n",
    "                        cases_new.append(\"type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\")\n",
    "\n",
    "                        cases_new_performances = []\n",
    "                        cases_new_performances_stdev = []\n",
    "                        \n",
    "                        \n",
    "                        for case in cases_new:\n",
    "                            \n",
    "                            if (\"linear\" in case):\n",
    "                                model_aux = \"linear\"\n",
    "                            else:\n",
    "                                model_aux = \"randomforest\"\n",
    "\n",
    "                            if (\"None\" in case):\n",
    "                                new_performance = []\n",
    "                                for c in range(0, num_stat_rep):                            \n",
    "                                    new_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                                   type_aux_mod=model_aux, selection_method=None,\n",
    "                                                                    seed=c))\n",
    "                                new_performance_stdev = statistics.stdev(new_performance)    \n",
    "                                cases_new_performances_stdev.append(new_performance_stdev)\n",
    "                                new_performance = statistics.mean(new_performance)        \n",
    "                                cases_new_performances.append(new_performance)\n",
    "                                \n",
    "                            else:     \n",
    "                                if (\"feature_imp_normal\" in case):\n",
    "                                    frac_feature_imp_normal_value = float(case.split(\"frac_feature_imp_normal=\")[1])\n",
    "                                    new_performance = []\n",
    "                                    for c in range(0, num_stat_rep):         \n",
    "                                        new_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                                      type_aux_mod=model_aux, selection_method=\"feature_imp_normal\", \n",
    "                                                                      frac_feature_imp_normal=frac_feature_imp_normal_value,\n",
    "                                                                      seed=c))\n",
    "                                    new_performance_stdev = statistics.stdev(new_performance)\n",
    "                                    cases_new_performances_stdev.append(new_performance_stdev)    \n",
    "                                    new_performance = statistics.mean(new_performance)        \n",
    "                                    cases_new_performances.append(new_performance)\n",
    "                                else:\n",
    "                                    new_cols_corr_thr_value = float(case.split(\"new_cols_corr_thr=\")[1])\n",
    "                                    new_performance = []\n",
    "                                    for c in range(0, num_stat_rep):         \n",
    "                                        new_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                                     type_aux_mod=model_aux, selection_method=\"correlation_target\", \n",
    "                                                                     new_cols_corr_thr=new_cols_corr_thr_value, \n",
    "                                                                     seed=c))\n",
    "                                        \n",
    "                                    new_performance_stdev = statistics.stdev(new_performance)\n",
    "                                    cases_new_performances_stdev.append(new_performance_stdev)    \n",
    "                                    new_performance = statistics.mean(new_performance)        \n",
    "                                    cases_new_performances.append(new_performance)\n",
    "                                    \n",
    "                            print(case)\n",
    "                            print(str(round(new_performance, 4)) + \" +- \" + str(round(new_performance_stdev, 4)))\n",
    "                            \n",
    "\n",
    "                        # Save result\n",
    "                        result = pd.DataFrame({\"Case\": cases_new, \"Mean F1-score\": cases_new_performances, \"Stdev F1-score\": cases_new_performances_stdev})\n",
    "                        result[\"Normal Mean F1-score\"] = normal_performance\n",
    "                        result[\"Normal stdev F1-score\"] = normal_performance_stdev\n",
    "                        result = result.sort_values(by=\"Mean F1-score\", ascending=False)\n",
    "                        data_name = data_url.split(\"/\")\n",
    "                        data_name = data_name[len(data_name) - 1]\n",
    "                        display(result)\n",
    "                        result.to_csv(\"metrics_\" + data_name + \".csv\", index=False)\n",
    "                        \n",
    "                        # Get maximum performance of the new cases\n",
    "                        if (max(cases_new_performances) > normal_performance):\n",
    "                            \n",
    "                            print(\"New performance: \" + str(round(max(cases_new_performances), 3)))\n",
    "                            # Print details\n",
    "                            max_index = cases_new_performances.index(max(cases_new_performances))    \n",
    "                            print(\"Optimum case new: \" + str(cases_new[max_index]))\n",
    "                        else:\n",
    "                            print(\"No improvement found\")                        \n",
    "                                                        \n",
    "            \n",
    "            except:   \n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:  \n",
    "        print(f\"The file '{i}' does not exist.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b0501e8a-32a9-4964-bedf-84c624934af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.7824 +- 0.0037\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.7866 +- 0.0035\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.7883 +- 0.0052\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.7912 +- 0.0038\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.7935 +- 0.0037\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.7922 +- 0.0039\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.7883 +- 0.0042\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.7818 +- 0.0056\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.7884 +- 0.0042\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.7893 +- 0.0064\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.7888 +- 0.0062\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.7912 +- 0.0039\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.7862 +- 0.0038\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.7861 +- 0.0043\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.7869 +- 0.0055\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.793526</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.792178</td>\n",
       "      <td>0.003880</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.791216</td>\n",
       "      <td>0.003782</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.791197</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.789346</td>\n",
       "      <td>0.006401</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.788834</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.788439</td>\n",
       "      <td>0.004240</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.788326</td>\n",
       "      <td>0.005174</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.788308</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.786908</td>\n",
       "      <td>0.005545</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.786626</td>\n",
       "      <td>0.003545</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.786230</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.786127</td>\n",
       "      <td>0.004258</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.781792</td>\n",
       "      <td>0.005633</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.793526   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.792178   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.791216   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.791197   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.789346   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.788834   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.788439   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.788326   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.788308   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.786908   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.786626   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.786230   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.786127   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.781792   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "3         0.003749              0.782361                0.00365  \n",
       "4         0.003880              0.782361                0.00365  \n",
       "2         0.003782              0.782361                0.00365  \n",
       "10        0.003904              0.782361                0.00365  \n",
       "8         0.006401              0.782361                0.00365  \n",
       "9         0.006221              0.782361                0.00365  \n",
       "7         0.004240              0.782361                0.00365  \n",
       "1         0.005174              0.782361                0.00365  \n",
       "5         0.004182              0.782361                0.00365  \n",
       "13        0.005545              0.782361                0.00365  \n",
       "0         0.003545              0.782361                0.00365  \n",
       "11        0.003832              0.782361                0.00365  \n",
       "12        0.004258              0.782361                0.00365  \n",
       "6         0.005633              0.782361                0.00365  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.794\n",
      "Optimum case new: type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.992 +- 0.0077\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.9955 +- 0.0049\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.9985 +- 0.0024\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.992 +- 0.0077\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.997 +- 0.0042\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.997 +- 0.0035\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.997 +- 0.0035\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.992 +- 0.0077\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9945 +- 0.0065\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.998 +- 0.0036\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.998 +- 0.0036\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9955 +- 0.0049\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.996 +- 0.0051\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9985 +- 0.0024\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9985 +- 0.0024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.998486</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.998486</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.998486</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.997974</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.997974</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.997002</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.997002</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.997002</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.996007</td>\n",
       "      <td>0.005134</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.995506</td>\n",
       "      <td>0.004942</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.995506</td>\n",
       "      <td>0.004942</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.994477</td>\n",
       "      <td>0.006453</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "      <td>0.991955</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.998486   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.998486   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.998486   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.997974   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.997974   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.997002   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.997002   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.997002   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.996007   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.995506   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.995506   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.994477   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.991955   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.991955   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "1         0.002438              0.991955               0.007678  \n",
       "12        0.002438              0.991955               0.007678  \n",
       "13        0.002438              0.991955               0.007678  \n",
       "8         0.003563              0.991955               0.007678  \n",
       "9         0.003563              0.991955               0.007678  \n",
       "3         0.004204              0.991955               0.007678  \n",
       "4         0.003482              0.991955               0.007678  \n",
       "5         0.003482              0.991955               0.007678  \n",
       "11        0.005134              0.991955               0.007678  \n",
       "0         0.004942              0.991955               0.007678  \n",
       "10        0.004942              0.991955               0.007678  \n",
       "7         0.006453              0.991955               0.007678  \n",
       "2         0.007678              0.991955               0.007678  \n",
       "6         0.007678              0.991955               0.007678  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.998\n",
      "Optimum case new: type_aux_mode=randomforest; selection_method_case=None\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.6788 +- 0.0315\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.6637 +- 0.038\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.6577 +- 0.0368\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.6788 +- 0.0315\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.6502 +- 0.0235\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.656 +- 0.0333\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.656 +- 0.0333\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.6788 +- 0.0315\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.658 +- 0.031\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.6492 +- 0.0477\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.6492 +- 0.0477\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.6509 +- 0.0317\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.6708 +- 0.0297\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.6662 +- 0.0395\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.6694 +- 0.0346\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.670778</td>\n",
       "      <td>0.029694</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.669411</td>\n",
       "      <td>0.034604</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.666182</td>\n",
       "      <td>0.039478</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.663735</td>\n",
       "      <td>0.037954</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.658020</td>\n",
       "      <td>0.030990</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.657706</td>\n",
       "      <td>0.036821</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.655998</td>\n",
       "      <td>0.033284</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.655998</td>\n",
       "      <td>0.033284</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.650876</td>\n",
       "      <td>0.031680</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.650184</td>\n",
       "      <td>0.023503</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.649218</td>\n",
       "      <td>0.047734</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.649218</td>\n",
       "      <td>0.047734</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>0.031536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.678828   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.678828   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.670778   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.669411   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.666182   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.663735   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.658020   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.657706   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.655998   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.655998   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.650876   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.650184   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.649218   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.649218   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "2         0.031536              0.678828               0.031536  \n",
       "6         0.031536              0.678828               0.031536  \n",
       "11        0.029694              0.678828               0.031536  \n",
       "13        0.034604              0.678828               0.031536  \n",
       "12        0.039478              0.678828               0.031536  \n",
       "0         0.037954              0.678828               0.031536  \n",
       "7         0.030990              0.678828               0.031536  \n",
       "1         0.036821              0.678828               0.031536  \n",
       "4         0.033284              0.678828               0.031536  \n",
       "5         0.033284              0.678828               0.031536  \n",
       "10        0.031680              0.678828               0.031536  \n",
       "3         0.023503              0.678828               0.031536  \n",
       "8         0.047734              0.678828               0.031536  \n",
       "9         0.047734              0.678828               0.031536  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement found\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.963 +- 0.0171\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.9601 +- 0.0097\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.9652 +- 0.0134\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9577 +- 0.0175\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9538 +- 0.0137\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.958 +- 0.0125\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.959 +- 0.0156\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9533 +- 0.0144\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.959 +- 0.0117\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9634 +- 0.0126\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9642 +- 0.0105\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.956 +- 0.014\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9589 +- 0.0163\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9674 +- 0.0117\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9673 +- 0.0107\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.967443</td>\n",
       "      <td>0.011724</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.967313</td>\n",
       "      <td>0.010685</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.965214</td>\n",
       "      <td>0.013390</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.964230</td>\n",
       "      <td>0.010468</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.963414</td>\n",
       "      <td>0.012624</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.960057</td>\n",
       "      <td>0.009726</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.959013</td>\n",
       "      <td>0.015640</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.958974</td>\n",
       "      <td>0.011717</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.958922</td>\n",
       "      <td>0.016295</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.958016</td>\n",
       "      <td>0.012479</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.957734</td>\n",
       "      <td>0.017522</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.956042</td>\n",
       "      <td>0.014041</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.953809</td>\n",
       "      <td>0.013711</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.953312</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.962994</td>\n",
       "      <td>0.017077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.967443   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.967313   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.965214   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.964230   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.963414   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.960057   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.959013   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.958974   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.958922   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.958016   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.957734   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.956042   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.953809   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.953312   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "12        0.011724              0.962994               0.017077  \n",
       "13        0.010685              0.962994               0.017077  \n",
       "1         0.013390              0.962994               0.017077  \n",
       "9         0.010468              0.962994               0.017077  \n",
       "8         0.012624              0.962994               0.017077  \n",
       "0         0.009726              0.962994               0.017077  \n",
       "5         0.015640              0.962994               0.017077  \n",
       "7         0.011717              0.962994               0.017077  \n",
       "11        0.016295              0.962994               0.017077  \n",
       "4         0.012479              0.962994               0.017077  \n",
       "2         0.017522              0.962994               0.017077  \n",
       "10        0.014041              0.962994               0.017077  \n",
       "3         0.013711              0.962994               0.017077  \n",
       "6         0.014400              0.962994               0.017077  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.967\n",
      "Optimum case new: type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.9818 +- 0.0085\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.9734 +- 0.0095\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.9776 +- 0.0076\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9797 +- 0.0076\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9795 +- 0.0097\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9763 +- 0.0088\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9753 +- 0.0091\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9797 +- 0.0086\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9753 +- 0.0079\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9795 +- 0.0096\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9784 +- 0.0096\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9801 +- 0.0081\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9799 +- 0.0088\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9828 +- 0.0096\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9799 +- 0.0088\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.982826</td>\n",
       "      <td>0.009577</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.980083</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.979884</td>\n",
       "      <td>0.008809</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.979884</td>\n",
       "      <td>0.008809</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.979670</td>\n",
       "      <td>0.007625</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.979663</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.979479</td>\n",
       "      <td>0.009619</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.979453</td>\n",
       "      <td>0.009651</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.978423</td>\n",
       "      <td>0.009629</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.977594</td>\n",
       "      <td>0.007597</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.976311</td>\n",
       "      <td>0.008827</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.975273</td>\n",
       "      <td>0.009108</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.975267</td>\n",
       "      <td>0.007857</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.973378</td>\n",
       "      <td>0.009496</td>\n",
       "      <td>0.981764</td>\n",
       "      <td>0.00846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.982826   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.980083   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.979884   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.979884   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.979670   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.979663   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.979479   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.979453   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.978423   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.977594   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.976311   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.975273   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.975267   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.973378   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "12        0.009577              0.981764                0.00846  \n",
       "10        0.008085              0.981764                0.00846  \n",
       "11        0.008809              0.981764                0.00846  \n",
       "13        0.008809              0.981764                0.00846  \n",
       "2         0.007625              0.981764                0.00846  \n",
       "6         0.008600              0.981764                0.00846  \n",
       "8         0.009619              0.981764                0.00846  \n",
       "3         0.009651              0.981764                0.00846  \n",
       "9         0.009629              0.981764                0.00846  \n",
       "1         0.007597              0.981764                0.00846  \n",
       "4         0.008827              0.981764                0.00846  \n",
       "5         0.009108              0.981764                0.00846  \n",
       "7         0.007857              0.981764                0.00846  \n",
       "0         0.009496              0.981764                0.00846  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.983\n",
      "Optimum case new: type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\echocardiogram\\config.ini could not be processed.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.9997 +- 0.0004\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.9996 +- 0.0005\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.9996 +- 0.0005\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9997 +- 0.0005\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9996 +- 0.0005\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9996 +- 0.0005\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9996 +- 0.0006\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9996 +- 0.0005\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9997 +- 0.0004\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9996 +- 0.0004\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9996 +- 0.0004\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9996 +- 0.0005\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9997 +- 0.0005\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9996 +- 0.0005\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9996 +- 0.0005\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.999712</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.999712</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.999710</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.999638</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.999638</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.999565</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.999565</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.999565</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.999565</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.999712   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.999712   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.999710   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.999639   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.999639   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.999639   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.999639   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.999639   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.999638   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.999638   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.999565   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.999565   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.999565   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.999565   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "2         0.000504              0.999711               0.000374  \n",
       "11        0.000504              0.999711               0.000374  \n",
       "7         0.000375              0.999711               0.000374  \n",
       "6         0.000510              0.999711               0.000374  \n",
       "0         0.000510              0.999711               0.000374  \n",
       "3         0.000510              0.999711               0.000374  \n",
       "4         0.000510              0.999711               0.000374  \n",
       "10        0.000510              0.999711               0.000374  \n",
       "8         0.000381              0.999711               0.000374  \n",
       "9         0.000381              0.999711               0.000374  \n",
       "1         0.000508              0.999711               0.000374  \n",
       "12        0.000508              0.999711               0.000374  \n",
       "13        0.000508              0.999711               0.000374  \n",
       "5         0.000611              0.999711               0.000374  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 1.0\n",
      "Optimum case new: type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.6405 +- 0.1033\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.5946 +- 0.0921\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.564 +- 0.0827\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.6405 +- 0.1033\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.6405 +- 0.1033\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.6222 +- 0.0764\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.6062 +- 0.1071\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.6405 +- 0.1033\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.6405 +- 0.1033\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.6007 +- 0.0832\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.6098 +- 0.0901\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.6044 +- 0.0948\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.612 +- 0.0901\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.5641 +- 0.0774\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.5821 +- 0.0908\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.622187</td>\n",
       "      <td>0.076410</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.611968</td>\n",
       "      <td>0.090081</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.609807</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.606248</td>\n",
       "      <td>0.107144</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.604426</td>\n",
       "      <td>0.094838</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.600713</td>\n",
       "      <td>0.083187</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.594556</td>\n",
       "      <td>0.092115</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.582130</td>\n",
       "      <td>0.090811</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.564124</td>\n",
       "      <td>0.077367</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.563985</td>\n",
       "      <td>0.082668</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.103315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.640499   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.640499   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.640499   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.640499   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.622187   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.611968   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.609807   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.606248   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.604426   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.600713   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.594556   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.582130   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.564124   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.563985   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "2         0.103315              0.640499               0.103315  \n",
       "3         0.103315              0.640499               0.103315  \n",
       "6         0.103315              0.640499               0.103315  \n",
       "7         0.103315              0.640499               0.103315  \n",
       "4         0.076410              0.640499               0.103315  \n",
       "11        0.090081              0.640499               0.103315  \n",
       "9         0.090067              0.640499               0.103315  \n",
       "5         0.107144              0.640499               0.103315  \n",
       "10        0.094838              0.640499               0.103315  \n",
       "8         0.083187              0.640499               0.103315  \n",
       "0         0.092115              0.640499               0.103315  \n",
       "13        0.090811              0.640499               0.103315  \n",
       "12        0.077367              0.640499               0.103315  \n",
       "1         0.082668              0.640499               0.103315  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement found\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\image-segmentation\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.6293 +- 0.0523\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.6162 +- 0.0525\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.6357 +- 0.0636\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.6374 +- 0.0458\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.6307 +- 0.0387\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.6074 +- 0.0391\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.6152 +- 0.0522\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.633 +- 0.0544\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.6108 +- 0.0482\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.647 +- 0.0666\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.6429 +- 0.0609\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.6231 +- 0.0434\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.6369 +- 0.0332\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.6273 +- 0.0648\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.6429 +- 0.0484\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.646962</td>\n",
       "      <td>0.066621</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.642871</td>\n",
       "      <td>0.060931</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.642863</td>\n",
       "      <td>0.048428</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.637387</td>\n",
       "      <td>0.045766</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.636920</td>\n",
       "      <td>0.033235</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.635747</td>\n",
       "      <td>0.063573</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.633027</td>\n",
       "      <td>0.054444</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.630739</td>\n",
       "      <td>0.038734</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.627336</td>\n",
       "      <td>0.064794</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.623136</td>\n",
       "      <td>0.043410</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.616245</td>\n",
       "      <td>0.052530</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.615235</td>\n",
       "      <td>0.052221</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.610841</td>\n",
       "      <td>0.048228</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.607438</td>\n",
       "      <td>0.039111</td>\n",
       "      <td>0.629257</td>\n",
       "      <td>0.052268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.646962   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.642871   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.642863   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.637387   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.636920   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.635747   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.633027   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.630739   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.627336   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.623136   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.616245   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.615235   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.610841   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.607438   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "8         0.066621              0.629257               0.052268  \n",
       "9         0.060931              0.629257               0.052268  \n",
       "13        0.048428              0.629257               0.052268  \n",
       "2         0.045766              0.629257               0.052268  \n",
       "11        0.033235              0.629257               0.052268  \n",
       "1         0.063573              0.629257               0.052268  \n",
       "6         0.054444              0.629257               0.052268  \n",
       "3         0.038734              0.629257               0.052268  \n",
       "12        0.064794              0.629257               0.052268  \n",
       "10        0.043410              0.629257               0.052268  \n",
       "0         0.052530              0.629257               0.052268  \n",
       "5         0.052221              0.629257               0.052268  \n",
       "7         0.048228              0.629257               0.052268  \n",
       "4         0.039111              0.629257               0.052268  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.647\n",
      "Optimum case new: type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.8564 +- 0.007\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.8582 +- 0.0048\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.8278 +- 0.0055\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.8544 +- 0.0053\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.8594 +- 0.0043\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.8582 +- 0.0047\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.8579 +- 0.0043\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.8025 +- 0.0083\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.8205 +- 0.0035\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.8313 +- 0.0054\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.8316 +- 0.0049\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.8577 +- 0.0062\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.8586 +- 0.0064\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.8195 +- 0.0049\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.8195 +- 0.0049\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.859373</td>\n",
       "      <td>0.004271</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.858553</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.858237</td>\n",
       "      <td>0.004771</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.858218</td>\n",
       "      <td>0.004685</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.857860</td>\n",
       "      <td>0.004280</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.857667</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.854400</td>\n",
       "      <td>0.005265</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.831642</td>\n",
       "      <td>0.004885</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.831295</td>\n",
       "      <td>0.005443</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.827835</td>\n",
       "      <td>0.005529</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.820454</td>\n",
       "      <td>0.003478</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.819457</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.819457</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.802476</td>\n",
       "      <td>0.008291</td>\n",
       "      <td>0.856379</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.859373   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.858553   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.858237   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.858218   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.857860   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.857667   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.854400   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.831642   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.831295   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.827835   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.820454   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.819457   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.819457   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.802476   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "3         0.004271              0.856379               0.007044  \n",
       "11        0.006389              0.856379               0.007044  \n",
       "0         0.004771              0.856379               0.007044  \n",
       "4         0.004685              0.856379               0.007044  \n",
       "5         0.004280              0.856379               0.007044  \n",
       "10        0.006181              0.856379               0.007044  \n",
       "2         0.005265              0.856379               0.007044  \n",
       "9         0.004885              0.856379               0.007044  \n",
       "8         0.005443              0.856379               0.007044  \n",
       "1         0.005529              0.856379               0.007044  \n",
       "7         0.003478              0.856379               0.007044  \n",
       "12        0.004949              0.856379               0.007044  \n",
       "13        0.004949              0.856379               0.007044  \n",
       "6         0.008291              0.856379               0.007044  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.859\n",
      "Optimum case new: type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 1.0 +- 0.0\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "1.0 +- 0.0\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "1.0 +- 0.0\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "1.0 +- 0.0\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "1.0 +- 0.0\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "1.0 +- 0.0\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "1.0 +- 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "0    type_aux_mode=linear; selection_method_case=None            1.0   \n",
       "1   type_aux_mode=randomforest; selection_method_c...            1.0   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...            1.0   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...            1.0   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...            1.0   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...            1.0   \n",
       "6   type_aux_mode=randomforest; selection_method_c...            1.0   \n",
       "7   type_aux_mode=randomforest; selection_method_c...            1.0   \n",
       "8   type_aux_mode=randomforest; selection_method_c...            1.0   \n",
       "9   type_aux_mode=randomforest; selection_method_c...            1.0   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...            1.0   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...            1.0   \n",
       "12  type_aux_mode_case=randomforest; selection_met...            1.0   \n",
       "13  type_aux_mode_case=randomforest; selection_met...            1.0   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "0              0.0                   1.0                    0.0  \n",
       "1              0.0                   1.0                    0.0  \n",
       "2              0.0                   1.0                    0.0  \n",
       "3              0.0                   1.0                    0.0  \n",
       "4              0.0                   1.0                    0.0  \n",
       "5              0.0                   1.0                    0.0  \n",
       "6              0.0                   1.0                    0.0  \n",
       "7              0.0                   1.0                    0.0  \n",
       "8              0.0                   1.0                    0.0  \n",
       "9              0.0                   1.0                    0.0  \n",
       "10             0.0                   1.0                    0.0  \n",
       "11             0.0                   1.0                    0.0  \n",
       "12             0.0                   1.0                    0.0  \n",
       "13             0.0                   1.0                    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement found\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\polish-bankruptcy\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\seismic-bumps\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.9965 +- 0.0007\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.9974 +- 0.0006\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.9943 +- 0.001\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9965 +- 0.0007\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9965 +- 0.0007\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9961 +- 0.0008\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9967 +- 0.0007\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9965 +- 0.0007\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9965 +- 0.0007\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9921 +- 0.0012\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9928 +- 0.0013\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9974 +- 0.0006\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.997 +- 0.0004\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9943 +- 0.001\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9943 +- 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.997383</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.997383</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.996988</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.996735</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.996530</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.996530</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.996530</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.996530</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.996105</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.994280</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.994280</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.994280</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.992776</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.992118</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.99653</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "0    type_aux_mode=linear; selection_method_case=None       0.997383   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.997383   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.996988   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.996735   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.996530   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.996530   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.996530   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.996530   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.996105   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.994280   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.994280   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.994280   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.992776   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.992118   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "0         0.000561               0.99653               0.000692  \n",
       "10        0.000561               0.99653               0.000692  \n",
       "11        0.000428               0.99653               0.000692  \n",
       "5         0.000685               0.99653               0.000692  \n",
       "2         0.000692               0.99653               0.000692  \n",
       "3         0.000692               0.99653               0.000692  \n",
       "6         0.000692               0.99653               0.000692  \n",
       "7         0.000692               0.99653               0.000692  \n",
       "4         0.000792               0.99653               0.000692  \n",
       "1         0.001002               0.99653               0.000692  \n",
       "12        0.001002               0.99653               0.000692  \n",
       "13        0.001002               0.99653               0.000692  \n",
       "9         0.001265               0.99653               0.000692  \n",
       "8         0.001209               0.99653               0.000692  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.997\n",
      "Optimum case new: type_aux_mode=linear; selection_method_case=None\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.9367 +- 0.0106\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.9332 +- 0.0097\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.9415 +- 0.0096\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9333 +- 0.0084\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9342 +- 0.0083\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9363 +- 0.0083\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.934 +- 0.008\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9336 +- 0.013\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9414 +- 0.0099\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9429 +- 0.0097\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9418 +- 0.0092\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9356 +- 0.0106\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9314 +- 0.0098\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9401 +- 0.0094\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9359 +- 0.0101\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.942947</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.941818</td>\n",
       "      <td>0.009228</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.941532</td>\n",
       "      <td>0.009578</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.941421</td>\n",
       "      <td>0.009894</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.940130</td>\n",
       "      <td>0.009407</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.936303</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.935945</td>\n",
       "      <td>0.010094</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.935644</td>\n",
       "      <td>0.010571</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.934215</td>\n",
       "      <td>0.008308</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.933953</td>\n",
       "      <td>0.008031</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.933598</td>\n",
       "      <td>0.013012</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.933257</td>\n",
       "      <td>0.008356</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.933158</td>\n",
       "      <td>0.009728</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.931352</td>\n",
       "      <td>0.009788</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>0.010633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.942947   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.941818   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.941532   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.941421   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.940130   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.936303   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.935945   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.935644   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.934215   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.933953   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.933598   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.933257   \n",
       "0    type_aux_mode=linear; selection_method_case=None       0.933158   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.931352   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "8         0.009657              0.936735               0.010633  \n",
       "9         0.009228              0.936735               0.010633  \n",
       "1         0.009578              0.936735               0.010633  \n",
       "7         0.009894              0.936735               0.010633  \n",
       "12        0.009407              0.936735               0.010633  \n",
       "4         0.008347              0.936735               0.010633  \n",
       "13        0.010094              0.936735               0.010633  \n",
       "10        0.010571              0.936735               0.010633  \n",
       "3         0.008308              0.936735               0.010633  \n",
       "5         0.008031              0.936735               0.010633  \n",
       "6         0.013012              0.936735               0.010633  \n",
       "2         0.008356              0.936735               0.010633  \n",
       "0         0.009728              0.936735               0.010633  \n",
       "11        0.009788              0.936735               0.010633  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.943\n",
      "Optimum case new: type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\statlog-shuttle\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\thoracic-surgery\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\thyroid-disease-allbp\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\thyroid-disease-allhyper\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\thyroid-disease-allrep\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\thyroid-disease-dis\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Dataset C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\thyroid-disease-sick\\config.ini could not be processed.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.9544 +- 0.0151\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.9753 +- 0.0089\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.9122 +- 0.0207\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9753 +- 0.0089\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9753 +- 0.0089\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.9753 +- 0.0089\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9753 +- 0.0089\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.953 +- 0.0132\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9357 +- 0.0162\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.931 +- 0.0151\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9052 +- 0.0215\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9753 +- 0.0089\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9753 +- 0.0089\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\n",
      "0.9544 +- 0.0151\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\n",
      "0.9544 +- 0.0151\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Mean F1-score</th>\n",
       "      <th>Stdev F1-score</th>\n",
       "      <th>Normal Mean F1-score</th>\n",
       "      <th>Normal stdev F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=None</td>\n",
       "      <td>0.975289</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.975289</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.975289</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.975289</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_aux_mode=linear; selection_method_case=fe...</td>\n",
       "      <td>0.975289</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.975289</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type_aux_mode_case=linear; selection_method_ca...</td>\n",
       "      <td>0.975289</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>type_aux_mode_case=randomforest; selection_met...</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.952975</td>\n",
       "      <td>0.013196</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.935731</td>\n",
       "      <td>0.016234</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.931029</td>\n",
       "      <td>0.015099</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.912196</td>\n",
       "      <td>0.020707</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>type_aux_mode=randomforest; selection_method_c...</td>\n",
       "      <td>0.905217</td>\n",
       "      <td>0.021472</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.015065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Case  Mean F1-score  \\\n",
       "0    type_aux_mode=linear; selection_method_case=None       0.975289   \n",
       "2   type_aux_mode=linear; selection_method_case=fe...       0.975289   \n",
       "3   type_aux_mode=linear; selection_method_case=fe...       0.975289   \n",
       "4   type_aux_mode=linear; selection_method_case=fe...       0.975289   \n",
       "5   type_aux_mode=linear; selection_method_case=fe...       0.975289   \n",
       "10  type_aux_mode_case=linear; selection_method_ca...       0.975289   \n",
       "11  type_aux_mode_case=linear; selection_method_ca...       0.975289   \n",
       "12  type_aux_mode_case=randomforest; selection_met...       0.954425   \n",
       "13  type_aux_mode_case=randomforest; selection_met...       0.954425   \n",
       "6   type_aux_mode=randomforest; selection_method_c...       0.952975   \n",
       "7   type_aux_mode=randomforest; selection_method_c...       0.935731   \n",
       "8   type_aux_mode=randomforest; selection_method_c...       0.931029   \n",
       "1   type_aux_mode=randomforest; selection_method_c...       0.912196   \n",
       "9   type_aux_mode=randomforest; selection_method_c...       0.905217   \n",
       "\n",
       "    Stdev F1-score  Normal Mean F1-score  Normal stdev F1-score  \n",
       "0         0.008928              0.954425               0.015065  \n",
       "2         0.008928              0.954425               0.015065  \n",
       "3         0.008928              0.954425               0.015065  \n",
       "4         0.008928              0.954425               0.015065  \n",
       "5         0.008928              0.954425               0.015065  \n",
       "10        0.008928              0.954425               0.015065  \n",
       "11        0.008928              0.954425               0.015065  \n",
       "12        0.015065              0.954425               0.015065  \n",
       "13        0.015065              0.954425               0.015065  \n",
       "6         0.013196              0.954425               0.015065  \n",
       "7         0.016234              0.954425               0.015065  \n",
       "8         0.015099              0.954425               0.015065  \n",
       "1         0.020707              0.954425               0.015065  \n",
       "9         0.021472              0.954425               0.015065  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New performance: 0.975\n",
      "Optimum case new: type_aux_mode=linear; selection_method_case=None\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import statistics\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "def aux_func(data, target_variable, only_normal=False, type_aux_mod=\"linear\", selection_method=None, \n",
    "             frac_feature_imp_normal=0.1, new_cols_corr_thr=0.2, seed=123):\n",
    "    \n",
    "    #################################\n",
    "    # PREPROCESSING\n",
    "    data = data.drop_duplicates()\n",
    "                            \n",
    "    # Handling missing values (drop rows with missing values for simplicity)\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    # Encoding categorical variables using one-hot encoding (OHE).\n",
    "    # First convert the target into categorical. This facilitates later\n",
    "    # calculations\n",
    "    data[target_variable] = data[target_variable].astype(\"category\")\n",
    "    # Now apply OHE to categorical columns (including the target)\n",
    "    data = pd.get_dummies(data)\n",
    "\n",
    "    # After OHE there will be two columns created for the target (since it was binary).\n",
    "    # Choose one of those columns as target and discard the other one. It is not important\n",
    "    # which of the two is selected as target, since the F1-score performance is measured\n",
    "    # using 'macro' average option\n",
    "    aux_names_target = [str(n) for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "    target_variable = aux_names_target[0]\n",
    "    data = data.drop(columns=[aux_names_target[1]])\n",
    "                            \n",
    "    # Normalizing numerical variables. Since previously OHE\n",
    "    # was performed, all variables with more than 2 different\n",
    "    # values will be numerical\n",
    "    numerical_columns = []\n",
    "    non_numerical_columns = []\n",
    "    for aux_num in data.columns:\n",
    "        unique_values_count = data[aux_num].nunique()\n",
    "        if (unique_values_count > 2):\n",
    "            numerical_columns.append(aux_num)\n",
    "        else:\n",
    "            non_numerical_columns.append(aux_num)\n",
    "    # Normalization\n",
    "    if (len(numerical_columns) != 0):\n",
    "        scaler = StandardScaler()\n",
    "        aux_data = scaler.fit_transform(data[numerical_columns])\n",
    "        aux_data = pd.DataFrame(aux_data, columns=numerical_columns)\n",
    "        aux_data = aux_data.reset_index(drop=True)\n",
    "        data_non_numerical = data[non_numerical_columns].reset_index(drop=True)\n",
    "        data = pd.concat([aux_data, data_non_numerical], axis=1)\n",
    "                                                                                \n",
    "\n",
    "    # Shuffle the DataFrame to randomize the rows\n",
    "    data = data.sample(frac=1, random_state=seed)  \n",
    "                            \n",
    "    # Save some registers for testing performance:\n",
    "    data_test = data.sample(frac=0.15, random_state=42)\n",
    "    data = data.drop(data_test.index)\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_counts = data[target_variable].value_counts()\n",
    "    total_samples = class_counts.sum()\n",
    "    class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "\n",
    "    # Define custom scorer\n",
    "    average_metric_type = 'macro'\n",
    "    custom_scorer = make_scorer(f1_score, average=average_metric_type)\n",
    "\n",
    "    #################################\n",
    "    # NORMAL MODEL\n",
    "\n",
    "    # If the function only returns the normal performance or the selection\n",
    "    # method of new variables is feature_imp_normal\n",
    "    if ((only_normal) | (selection_method is not None)):\n",
    "\n",
    "        X_normal = data.drop(columns=[target_variable])\n",
    "        y_normal = data[target_variable] \n",
    "\n",
    "        # Define RandomForestClassifier\n",
    "        rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        \n",
    "        # Define the parameter grid for grid search\n",
    "        min_samples_split_grid_search = [5, 10, 15]\n",
    "        min_samples_leaf_grid_search = [5, 10, 15]\n",
    "        param_grid = {\n",
    "            'min_samples_split': min_samples_split_grid_search,\n",
    "            'min_samples_leaf' : min_samples_leaf_grid_search\n",
    "        }\n",
    "        \n",
    "        # Perform grid search        \n",
    "        grid_search = GridSearchCV(estimator=rf_normal, param_grid=param_grid,\n",
    "                                   scoring=custom_scorer, cv=5, n_jobs=-1)\n",
    "        # The normal model takes imbalance into account to generate \n",
    "        # maximum normal performance\n",
    "        grid_search.fit(X_normal, y_normal, sample_weight=y_normal.map(class_weights))\n",
    "        \n",
    "        # Get the best normal model\n",
    "        rf_normal = grid_search.best_estimator_\n",
    "\n",
    "        # If the call to the function was to just calculate the\n",
    "        # normal performance\n",
    "        if (only_normal):\n",
    "            features_test_normal = data_test.drop(target_variable, axis=1)\n",
    "            target_test_normal = data_test[target_variable]\n",
    "            predictions_normal = rf_normal.predict(features_test_normal)\n",
    "            normal_score = f1_score(target_test_normal, predictions_normal, average=average_metric_type)\n",
    "                           \n",
    "            return (normal_score)\n",
    "    \n",
    "    \n",
    "    #######################################\n",
    "        \n",
    "                                                \n",
    "    #################################\n",
    "    # GENERATION OF AUXILIARY MODELS\n",
    "        \n",
    "    # List of dictionaries\n",
    "    list_of_dictionaries = []\n",
    "                            \n",
    "    # For each value of the target\n",
    "    for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                \n",
    "        # Generate auxiliary dataset\n",
    "        dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "        # Discard target in auxiliary dataset\n",
    "        dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "        # Generate dictionary of ficticious targets and the models that predict them:\n",
    "        dictionary_aux = {}\n",
    "                                \n",
    "        for fict_target in dataset_aux.columns.tolist():\n",
    "        \n",
    "            # Train auxiliary model and save it\n",
    "            X = dataset_aux.drop(columns=[fict_target])\n",
    "            y = dataset_aux[fict_target]                                 \n",
    "\n",
    "            if (type_aux_mod == \"linear\"):\n",
    "                aux_model = LinearRegression(n_jobs=-1)\n",
    "            else:\n",
    "                if (type_aux_mod == \"randomforest\"):\n",
    "                    aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5, n_jobs=-1)\n",
    "                else:\n",
    "                    print(\"Error: current allowed type for auxiliary models is 'linear' or 'randomforest'\")\n",
    "                    sys.exit()\n",
    "            \n",
    "            aux_model.fit(X, y)\n",
    "                                        \n",
    "            dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                    \n",
    "        list_of_dictionaries.append(dictionary_aux)    \n",
    "            \n",
    "\n",
    "    #################################\n",
    "    # GENERATION OF NEW COLUMNS BASED ON\n",
    "    # AUXILIARY MODELS\n",
    "        \n",
    "    list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "    list_of_rows_dataframe_new = []\n",
    "                          \n",
    "    new_columns = []\n",
    "\n",
    "    # For each value of the target\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "                              \n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "                                    \n",
    "        for fict_target in dictionary_case:\n",
    "                                    \n",
    "            X = data.drop(columns=[target_variable, fict_target])\n",
    "            y_predicted = dictionary_case[fict_target].predict(X)\n",
    "            y_real = data[fict_target]  \n",
    "                            \n",
    "            mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "            rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                          \n",
    "            # Add column to list of new columns\n",
    "            new_columns.append(rmse)\n",
    "\n",
    "        \n",
    "    dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "    names_cols_dataframe_new = []\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "        for u in dictionary_case.keys():\n",
    "            names_cols_dataframe_new.append(u + \"_\" + str(case))\n",
    "    dataframe_new.columns = names_cols_dataframe_new\n",
    "\n",
    "    cols_a = data.columns.to_list()\n",
    "    cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "    data = data.reset_index(drop=True)\n",
    "    dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                           \n",
    "    # Concatenate horizontally\n",
    "    result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "    result_df.columns = cols_b + cols_a    \n",
    "                            \n",
    "    #################################\n",
    "    # SELECTION OF NEW COLUMNS CREATED TO CREATE\n",
    "    # AUGMENTED DF.\n",
    "\n",
    "    if (selection_method == \"feature_imp_normal\"):\n",
    "        # Calculations for later selection of new columns\n",
    "        feature_importances = rf_normal.feature_importances_\n",
    "        # Create a list of (feature_name, importance) tuples\n",
    "        feature_importance_tuples = zip(X_normal.columns, feature_importances)\n",
    "        # Sort the tuples based on importance\n",
    "        sorted_feature_importance_tuples = sorted(feature_importance_tuples, key=lambda x: x[1], reverse=True)\n",
    "        # Extract sorted feature names\n",
    "        sorted_feature_names = [feature_name for feature_name, _ in sorted_feature_importance_tuples]\n",
    "        # Select some features\n",
    "        base_selected_columns = sorted_feature_names[0: int(len(sorted_feature_names) * frac_feature_imp_normal)]\n",
    "        \n",
    "        selected_columns = [] \n",
    "        for aux_base_cols in base_selected_columns:\n",
    "            selected_columns.append(aux_base_cols + \"_0\")\n",
    "            selected_columns.append(aux_base_cols + \"_1\")\n",
    "        \n",
    "    else:          \n",
    "        if (selection_method == \"correlation_target\"):\n",
    "            # Obtain new variables that have at least certain absolute correlation with the target\n",
    "            correlation = result_df.corr()[target_variable].abs()\n",
    "            # Select columns that have a at least certain absolute correlation with the target\n",
    "            selected_columns = correlation[correlation >= new_cols_corr_thr].index\n",
    "            selected_columns = [w for w in selected_columns if (w in dataframe_new.columns)]\n",
    "            \n",
    "        else:\n",
    "            if (selection_method is None):\n",
    "                # Select all created new columns\n",
    "                selected_columns = dataframe_new.columns.tolist()            \n",
    "                \n",
    "            else:\n",
    "                print(\"Error. The parameter selection_method must be 'feature_imp_normal', 'correlation_target' or None\")\n",
    "                sys.exit()\n",
    "                    \n",
    "\n",
    "    # The augmented dataframe will contain the original columns plus the \n",
    "    # selected new columns\n",
    "    result_df = result_df.loc[:, data.columns.tolist() + selected_columns]\n",
    "\n",
    "                            \n",
    "\n",
    "    #################################\n",
    "    # TRAINING OF MODEL BASED ON AUGMENTED DF.\n",
    "\n",
    "    features = result_df.drop(target_variable, axis=1)\n",
    "    target = result_df[target_variable]\n",
    "    \n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        \n",
    "    # Define the parameter grid for grid search\n",
    "    min_samples_split_grid_search = [5, 10, 15]\n",
    "    min_samples_leaf_grid_search = [5, 10, 15]\n",
    "    param_grid = {\n",
    "        'min_samples_split': min_samples_split_grid_search,\n",
    "        'min_samples_leaf' : min_samples_leaf_grid_search\n",
    "    }\n",
    "        \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                              scoring=custom_scorer, cv=5, n_jobs=-1)\n",
    "    \n",
    "    grid_search.fit(features, target, sample_weight=target.map(class_weights))\n",
    "  \n",
    "    # grid_search.fit(features, target)\n",
    "    \n",
    "    # Get the best model and corresponding score\n",
    "    rf_model = grid_search.best_estimator_\n",
    "                            \n",
    "\n",
    "    #################################\n",
    "    # PROCESSING OF TEST DATASET SO THAT THE TRAINED MODEL\n",
    "    # CAN BE APPLIED TO IT, AND IN THIS WAY OBTAIN \n",
    "    # PERFORMANCE METRICS\n",
    "    \n",
    "    # For each value of the target\n",
    "    new_columns = []\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "        \n",
    "        for fict_target in dictionary_case:\n",
    "            if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                y_real = data_test[fict_target]  \n",
    "                               \n",
    "                mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                \n",
    "                # Add column to list of new columns\n",
    "                new_columns.append(rmse)\n",
    "                            \n",
    "    dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "    names_cols_dataframe_new2 = []\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "        for fict_target in dictionary_case:\n",
    "            if ((fict_target + '_' + str(case)) in selected_columns):    \n",
    "                names_cols_dataframe_new2.append(fict_target + \"_\" + str(case))\n",
    "    dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "    cols_a = data_test.columns.to_list()\n",
    "    cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "    data_test = data_test.reset_index(drop=True)\n",
    "    dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                                                    \n",
    "    # Concatenate horizontally\n",
    "    data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "    data_test_processed.columns = cols_b + cols_a\n",
    "    # Adapt order of columns to the order in which the model was\n",
    "    # trained\n",
    "    data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                         \n",
    "    # Now apply trained model on test dataset to gauge performance\n",
    "    features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "    target_test = data_test_processed[target_variable]\n",
    "    predictions = rf_model.predict(features_test)\n",
    "                           \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(target_test, predictions, average=average_metric_type)\n",
    "    return (f1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "\n",
    "# TEST ON DATAFRAMES FROM UCI, DOWNLOAD ADAPTED FROM Perales-González, Carlos, (2020). UCI download-process, v1.3, GitHub repository, https://github.com/cperales/uci-download-process\n",
    "\n",
    "# Number of statistical repetitions\n",
    "num_stat_rep = 10\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        \n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header_option = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]  \n",
    "                   data_url = data_url.strip()\n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       try:\n",
    "                           if (len(lines[x].split(\" = \")) == 1):\n",
    "                               pass\n",
    "                           # If the separator is specified    \n",
    "                           else:\n",
    "                               separator = lines[x].split(\" = \")[1]\n",
    "                               separator = separator.strip()\n",
    "                               if (\"comma\" in separator):\n",
    "                                   separator = \",\"\n",
    "                       except:\n",
    "                           pass\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           try:\n",
    "                               target_index = lines[x].split(\" = \")[1]\n",
    "                               target_index = int(target_index.strip())\n",
    "                           except:\n",
    "                               pass\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               try:\n",
    "                                   # If there is not header specified, assume\n",
    "                                   # default value\n",
    "                                   if (len(lines[x].split(\" = \")) == 1):\n",
    "                                        pass\n",
    "                                   else:\n",
    "                                       # If it is specified\n",
    "                                       header_option = lines[x].split(\" = \")[1]\n",
    "                                       header_option = int(header_option.strip())\n",
    "                                       # If it is 0 assign it to None\n",
    "                                       if (str(header_option).startswith(\"0\")):\n",
    "                                           header_option = None\n",
    "                               except:\n",
    "                                   pass\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            try:\n",
    "                data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python', header=header_option)\n",
    "                aux_names_columns = [str(aux_n_c) for aux_n_c in data.columns]\n",
    "                data.columns = aux_names_columns\n",
    "                    \n",
    "                # If there will be enough dimensionality after one hot encoding\n",
    "                data_check = pd.get_dummies(data)\n",
    "                if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                    # Name of the target of the dataset (target_index - 1 since \n",
    "                    # in python first position is 0)\n",
    "                    target_variable = str(data.columns.tolist()[int(target_index) - 1])\n",
    "        \n",
    "                    # If it is a binary classification\n",
    "                    unique_values_count = data[target_variable].nunique()\n",
    "                    if (unique_values_count == 2):\n",
    "                        \n",
    "                        normal_performance = []\n",
    "                        for c in range(0, num_stat_rep):\n",
    "                            normal_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                           only_normal=True, seed=c))\n",
    "\n",
    "                        normal_performance_stdev = statistics.stdev(normal_performance)\n",
    "                        normal_performance = statistics.mean(normal_performance)\n",
    "                        print(\"Normal performance: \" + str(round(normal_performance, 4)) + \" +- \" + str(round(normal_performance_stdev, 4)))\n",
    "                        \n",
    "                        cases_new = [\"type_aux_mode=linear; selection_method_case=None\"]\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=None\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\")\n",
    "                        cases_new.append(\"type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\")\n",
    "                        cases_new.append(\"type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\")\n",
    "                        cases_new.append(\"type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\")\n",
    "                        cases_new.append(\"type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\")\n",
    "\n",
    "                        cases_new_performances = []\n",
    "                        cases_new_performances_stdev = []\n",
    "                        \n",
    "                        \n",
    "                        for case in cases_new:\n",
    "                            \n",
    "                            if (\"linear\" in case):\n",
    "                                model_aux = \"linear\"\n",
    "                            else:\n",
    "                                model_aux = \"randomforest\"\n",
    "\n",
    "                            if (\"None\" in case):\n",
    "                                new_performance = []\n",
    "                                for c in range(0, num_stat_rep):                            \n",
    "                                    new_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                                   type_aux_mod=model_aux, selection_method=None,\n",
    "                                                                    seed=c))\n",
    "                                new_performance_stdev = statistics.stdev(new_performance)    \n",
    "                                cases_new_performances_stdev.append(new_performance_stdev)\n",
    "                                new_performance = statistics.mean(new_performance)        \n",
    "                                cases_new_performances.append(new_performance)\n",
    "                                \n",
    "                            else:     \n",
    "                                if (\"feature_imp_normal\" in case):\n",
    "                                    frac_feature_imp_normal_value = float(case.split(\"frac_feature_imp_normal=\")[1])\n",
    "                                    new_performance = []\n",
    "                                    for c in range(0, num_stat_rep):         \n",
    "                                        new_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                                      type_aux_mod=model_aux, selection_method=\"feature_imp_normal\", \n",
    "                                                                      frac_feature_imp_normal=frac_feature_imp_normal_value,\n",
    "                                                                      seed=c))\n",
    "                                    new_performance_stdev = statistics.stdev(new_performance)\n",
    "                                    cases_new_performances_stdev.append(new_performance_stdev)    \n",
    "                                    new_performance = statistics.mean(new_performance)        \n",
    "                                    cases_new_performances.append(new_performance)\n",
    "                                else:\n",
    "                                    new_cols_corr_thr_value = float(case.split(\"new_cols_corr_thr=\")[1])\n",
    "                                    new_performance = []\n",
    "                                    for c in range(0, num_stat_rep):         \n",
    "                                        new_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                                     type_aux_mod=model_aux, selection_method=\"correlation_target\", \n",
    "                                                                     new_cols_corr_thr=new_cols_corr_thr_value, \n",
    "                                                                     seed=c))\n",
    "                                        \n",
    "                                    new_performance_stdev = statistics.stdev(new_performance)\n",
    "                                    cases_new_performances_stdev.append(new_performance_stdev)    \n",
    "                                    new_performance = statistics.mean(new_performance)        \n",
    "                                    cases_new_performances.append(new_performance)\n",
    "                                    \n",
    "                            print(case)\n",
    "                            print(str(round(new_performance, 4)) + \" +- \" + str(round(new_performance_stdev, 4)))\n",
    "                            \n",
    "\n",
    "                        # Save result\n",
    "                        result = pd.DataFrame({\"Case\": cases_new, \"Mean F1-score\": cases_new_performances, \"Stdev F1-score\": cases_new_performances_stdev})\n",
    "                        result[\"Normal Mean F1-score\"] = normal_performance\n",
    "                        result[\"Normal stdev F1-score\"] = normal_performance_stdev\n",
    "                        result = result.sort_values(by=\"Mean F1-score\", ascending=False)\n",
    "                        data_name = data_url.split(\"/\")\n",
    "                        data_name = data_name[len(data_name) - 1]\n",
    "                        display(result)\n",
    "                        result.to_csv(\"metrics_\" + data_name + \".csv\", index=False)\n",
    "                        \n",
    "                        # Get maximum performance of the new cases\n",
    "                        if (max(cases_new_performances) > normal_performance):\n",
    "                            \n",
    "                            print(\"New performance: \" + str(round(max(cases_new_performances), 3)))\n",
    "                            # Print details\n",
    "                            max_index = cases_new_performances.index(max(cases_new_performances))    \n",
    "                            print(\"Optimum case new: \" + str(cases_new[max_index]))\n",
    "                        else:\n",
    "                            print(\"No improvement found\")                        \n",
    "                                                        \n",
    "            \n",
    "            except:   \n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:  \n",
    "        print(f\"The file '{i}' does not exist.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e9a0aa1-f1fb-4b65-b81a-8600037ef753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;apply_class_weights&#x27;,\n",
       "                                                  ColumnWeightTransformer(class_weights={0: 1,\n",
       "                                                                                         1: 3},\n",
       "                                                                          columns_to_exclude=[&#x27;feature3&#x27;,\n",
       "                                                                                              &#x27;feature4&#x27;,\n",
       "                                                                                              &#x27;feature5&#x27;,\n",
       "                                                                                              &#x27;feature6&#x27;]),\n",
       "                                                  [&#x27;feature1&#x27;, &#x27;feature2&#x27;]),\n",
       "                                                 (&#x27;other_columns&#x27;,\n",
       "                                                  &#x27;passthrough&#x27;,\n",
       "                                                  [&#x27;feature3&#x27;, &#x27;feature4&#x27;,\n",
       "                                                   &#x27;feature5&#x27;, &#x27;feature6&#x27;])])),\n",
       "                (&#x27;classifier&#x27;, RandomForestClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;apply_class_weights&#x27;,\n",
       "                                                  ColumnWeightTransformer(class_weights={0: 1,\n",
       "                                                                                         1: 3},\n",
       "                                                                          columns_to_exclude=[&#x27;feature3&#x27;,\n",
       "                                                                                              &#x27;feature4&#x27;,\n",
       "                                                                                              &#x27;feature5&#x27;,\n",
       "                                                                                              &#x27;feature6&#x27;]),\n",
       "                                                  [&#x27;feature1&#x27;, &#x27;feature2&#x27;]),\n",
       "                                                 (&#x27;other_columns&#x27;,\n",
       "                                                  &#x27;passthrough&#x27;,\n",
       "                                                  [&#x27;feature3&#x27;, &#x27;feature4&#x27;,\n",
       "                                                   &#x27;feature5&#x27;, &#x27;feature6&#x27;])])),\n",
       "                (&#x27;classifier&#x27;, RandomForestClassifier())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;preprocessor: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for preprocessor: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(transformers=[(&#x27;apply_class_weights&#x27;,\n",
       "                                 ColumnWeightTransformer(class_weights={0: 1,\n",
       "                                                                        1: 3},\n",
       "                                                         columns_to_exclude=[&#x27;feature3&#x27;,\n",
       "                                                                             &#x27;feature4&#x27;,\n",
       "                                                                             &#x27;feature5&#x27;,\n",
       "                                                                             &#x27;feature6&#x27;]),\n",
       "                                 [&#x27;feature1&#x27;, &#x27;feature2&#x27;]),\n",
       "                                (&#x27;other_columns&#x27;, &#x27;passthrough&#x27;,\n",
       "                                 [&#x27;feature3&#x27;, &#x27;feature4&#x27;, &#x27;feature5&#x27;,\n",
       "                                  &#x27;feature6&#x27;])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">apply_class_weights</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;feature1&#x27;, &#x27;feature2&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">ColumnWeightTransformer</label><div class=\"sk-toggleable__content fitted\"><pre>ColumnWeightTransformer(class_weights={0: 1, 1: 3},\n",
       "                        columns_to_exclude=[&#x27;feature3&#x27;, &#x27;feature4&#x27;, &#x27;feature5&#x27;,\n",
       "                                            &#x27;feature6&#x27;])</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">other_columns</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;feature3&#x27;, &#x27;feature4&#x27;, &#x27;feature5&#x27;, &#x27;feature6&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier()</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('apply_class_weights',\n",
       "                                                  ColumnWeightTransformer(class_weights={0: 1,\n",
       "                                                                                         1: 3},\n",
       "                                                                          columns_to_exclude=['feature3',\n",
       "                                                                                              'feature4',\n",
       "                                                                                              'feature5',\n",
       "                                                                                              'feature6']),\n",
       "                                                  ['feature1', 'feature2']),\n",
       "                                                 ('other_columns',\n",
       "                                                  'passthrough',\n",
       "                                                  ['feature3', 'feature4',\n",
       "                                                   'feature5', 'feature6'])])),\n",
       "                ('classifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'feature1': [1, 2, 3, 4],\n",
    "    'feature2': [0.5, 1.5, 2.5, 3.5],\n",
    "    'feature3': [10, 20, 30, 40],\n",
    "    'feature4': [5, 15, 25, 35],\n",
    "    'feature5': [2, 1, 2, 3],\n",
    "    'feature6': [1, 4, 1, 6],\n",
    "    'target': [0, 1, 0, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define class weights for target variable\n",
    "class_weights = {0: 1, 1: 3}  # Example class weights (adjust as needed)\n",
    "\n",
    "# Define columns to exclude from class weights\n",
    "columns_to_exclude = ['feature3', 'feature4', 'feature5', 'feature6']\n",
    "\n",
    "# Custom transformer to apply target class weights to specific columns\n",
    "class ColumnWeightTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, class_weights, columns_to_exclude):\n",
    "        self.class_weights = class_weights\n",
    "        self.columns_to_exclude = columns_to_exclude\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Apply class weights to columns not in the specified list\n",
    "        X_copy = X.copy()\n",
    "        for col in X.columns:\n",
    "            if col not in self.columns_to_exclude:\n",
    "                X_copy[col] *= self.class_weights.get(col, 1)  # Multiply by class weight or 1 if not specified\n",
    "        return X_copy\n",
    "\n",
    "# Define a preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('apply_class_weights', ColumnWeightTransformer(class_weights, columns_to_exclude), ['feature1', 'feature2']),  # Example numerical columns\n",
    "        ('other_columns', 'passthrough', ['feature3', 'feature4', 'feature5', 'feature6'])  # Treat these columns normally\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define RandomForestClassifier pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())  # Apply class weights here\n",
    "])\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns='target')\n",
    "y = df['target']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train RandomForestClassifier pipeline\n",
    "rf_pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "727e13ee-3099-4046-8a67-6bc37d76c342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature__1</th>\n",
       "      <th>feature__2</th>\n",
       "      <th>feature__3</th>\n",
       "      <th>feature__4</th>\n",
       "      <th>feature__5</th>\n",
       "      <th>feature__6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.548814</td>\n",
       "      <td>0.715189</td>\n",
       "      <td>0.602763</td>\n",
       "      <td>0.544883</td>\n",
       "      <td>0.423655</td>\n",
       "      <td>0.645894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.437587</td>\n",
       "      <td>0.891773</td>\n",
       "      <td>0.963663</td>\n",
       "      <td>0.383442</td>\n",
       "      <td>0.791725</td>\n",
       "      <td>0.528895</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.568045</td>\n",
       "      <td>0.925597</td>\n",
       "      <td>0.071036</td>\n",
       "      <td>0.087129</td>\n",
       "      <td>0.020218</td>\n",
       "      <td>0.832620</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.778157</td>\n",
       "      <td>0.870012</td>\n",
       "      <td>0.978618</td>\n",
       "      <td>0.799159</td>\n",
       "      <td>0.461479</td>\n",
       "      <td>0.780529</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.118274</td>\n",
       "      <td>0.639921</td>\n",
       "      <td>0.143353</td>\n",
       "      <td>0.944669</td>\n",
       "      <td>0.521848</td>\n",
       "      <td>0.414662</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.147905</td>\n",
       "      <td>0.529429</td>\n",
       "      <td>0.959006</td>\n",
       "      <td>0.182730</td>\n",
       "      <td>0.259130</td>\n",
       "      <td>0.120583</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.594602</td>\n",
       "      <td>0.926769</td>\n",
       "      <td>0.238015</td>\n",
       "      <td>0.472421</td>\n",
       "      <td>0.907615</td>\n",
       "      <td>0.708283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.528287</td>\n",
       "      <td>0.639307</td>\n",
       "      <td>0.730840</td>\n",
       "      <td>0.728478</td>\n",
       "      <td>0.484977</td>\n",
       "      <td>0.281476</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.206175</td>\n",
       "      <td>0.918614</td>\n",
       "      <td>0.058247</td>\n",
       "      <td>0.337073</td>\n",
       "      <td>0.690050</td>\n",
       "      <td>0.600069</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.433219</td>\n",
       "      <td>0.104233</td>\n",
       "      <td>0.139528</td>\n",
       "      <td>0.696523</td>\n",
       "      <td>0.483697</td>\n",
       "      <td>0.339551</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature__1  feature__2  feature__3  feature__4  feature__5  feature__6  \\\n",
       "0      0.548814    0.715189    0.602763    0.544883    0.423655    0.645894   \n",
       "1      0.437587    0.891773    0.963663    0.383442    0.791725    0.528895   \n",
       "2      0.568045    0.925597    0.071036    0.087129    0.020218    0.832620   \n",
       "3      0.778157    0.870012    0.978618    0.799159    0.461479    0.780529   \n",
       "4      0.118274    0.639921    0.143353    0.944669    0.521848    0.414662   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "995    0.147905    0.529429    0.959006    0.182730    0.259130    0.120583   \n",
       "996    0.594602    0.926769    0.238015    0.472421    0.907615    0.708283   \n",
       "997    0.528287    0.639307    0.730840    0.728478    0.484977    0.281476   \n",
       "998    0.206175    0.918614    0.058247    0.337073    0.690050    0.600069   \n",
       "999    0.433219    0.104233    0.139528    0.696523    0.483697    0.339551   \n",
       "\n",
       "     target  \n",
       "0         0  \n",
       "1         1  \n",
       "2         0  \n",
       "3         1  \n",
       "4         1  \n",
       "..      ...  \n",
       "995       0  \n",
       "996       1  \n",
       "997       0  \n",
       "998       1  \n",
       "999       0  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "n_samples = 1000\n",
    "n_features = 6\n",
    "X = np.random.rand(n_samples, n_features)\n",
    "y = np.random.randint(2, size=n_samples)\n",
    "\n",
    "# Add imbalance to the target variable\n",
    "y[np.random.choice(np.where(y == 0)[0], size=n_samples // 5, replace=False)] = 1\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {'feature__' + str(i+1): X[:, i] for i in range(n_features)}\n",
    "data['target'] = y\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define class weights for target variable\n",
    "class_weights = {0: 1, 1: 3}  # Example class weights (adjust as needed)\n",
    "\n",
    "# Define columns to exclude from class weights\n",
    "columns_to_exclude_from_weights = ['feature__4']\n",
    "\n",
    "# Custom transformer to apply target class weights to specific columns\n",
    "class ColumnWeightTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, class_weights, columns_to_exclude):\n",
    "        self.class_weights = class_weights\n",
    "        self.columns_to_exclude = columns_to_exclude\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Apply class weights to specified columns\n",
    "        X_copy = X.copy()\n",
    "        for col in X.columns:\n",
    "            if col not in self.columns_to_exclude:\n",
    "                X_copy[col] *= X[col].map(self.class_weights)  # Multiply by class weight or 1 if not specified\n",
    "                \n",
    "        return X_copy\n",
    "\n",
    "# Define a preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('apply_class_weights', ColumnWeightTransformer(class_weights, columns_to_exclude_from_weights), slice(None)), # With slice(None) all columns are passed, but internally only those not present in columns_to_exclude_from_weights will be treated\n",
    "        ('other_columns', 'passthrough', columns_to_exclude_from_weights)  # Treat these columns normally\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define RandomForestClassifier pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())  # Apply class weights here\n",
    "])\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns='target')\n",
    "y = df['target']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 150],\n",
    "    'classifier__max_depth': [None, 10, 20]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_pipeline, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from GridSearchCV\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_accuracy = best_rf_model.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8ce789c1-ee22-4b21-9517-b4d9b3c8eec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\adult\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\balance-scale\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\balloons-a\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\balloons-b\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\balloons-c\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\balloons-d\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\banknote-authentication\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\blood-transfusion-service-center\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\breast-cancer\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\breast-cancer-wisconsin-diagnostic\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\breast-cancer-wisconsin-original\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\breast-cancer-wisconsin-prognostic\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\car-evaluation\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\chess-king-rook-vs-king\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\chess-king-rook-vs-king-pawn\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\climate-model-simulation-crashes\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\cnae-9\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\congressional-voting-records\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\connectionist-bench\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\connectionist-bench-sonar\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\contraceptive-method-choice\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\credit-approval\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\dermatology\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\echocardiogram\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\electrical-grid\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\fertility\\config.ini' does not exist.\n",
      "The file 'C:\\Users\\fjlobo\\Desktop\\estudio_mod\\descarga_de_datasets_de_uci\\directorio3\\datafiles\\classification\\glass-identification\\config.ini' does not exist.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.6511 +- 0.0855\n",
      "type_aux_mode=linear; selection_method_case=None\n",
      "0.9073 +- 0.0471\n",
      "type_aux_mode=randomforest; selection_method_case=None\n",
      "0.8905 +- 0.0598\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.9221 +- 0.0512\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9158 +- 0.0454\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\n",
      "0.8821 +- 0.0419\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\n",
      "0.9365 +- 0.0365\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\n",
      "0.914 +- 0.0409\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\n",
      "0.9153 +- 0.0461\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 575\u001b[0m\n\u001b[0;32m    573\u001b[0m new_performance \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_stat_rep):         \n\u001b[1;32m--> 575\u001b[0m     new_performance\u001b[38;5;241m.\u001b[39mappend(\u001b[43maux_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_variable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_variable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtype_aux_mod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature_imp_normal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mfrac_feature_imp_normal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrac_feature_imp_normal_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    579\u001b[0m new_performance_stdev \u001b[38;5;241m=\u001b[39m statistics\u001b[38;5;241m.\u001b[39mstdev(new_performance)\n\u001b[0;32m    580\u001b[0m cases_new_performances_stdev\u001b[38;5;241m.\u001b[39mappend(new_performance_stdev)    \n",
      "Cell \u001b[1;32mIn[90], line 352\u001b[0m, in \u001b[0;36maux_func\u001b[1;34m(data, target_variable, only_normal, type_aux_mod, selection_method, frac_feature_imp_normal, new_cols_corr_thr, seed)\u001b[0m\n\u001b[0;32m    339\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mrf_pipeline, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, \n\u001b[0;32m    340\u001b[0m                            scoring\u001b[38;5;241m=\u001b[39mcustom_scorer, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;66;03m# Target imbalance is only handled for the columns that are not new, i.e those\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# of the original dataframe.\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# grid_search needs to receive the dataset with all columns (including target),\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# to the target, and you shall see that the performance is much better than the one\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# that is actually obtained here.\u001b[39;00m\n\u001b[1;32m--> 352\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# grid_search.fit(features, target)\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \n\u001b[0;32m    356\u001b[0m \u001b[38;5;66;03m# Get the best model and corresponding score\u001b[39;00m\n\u001b[0;32m    357\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32m~\\Desktop\\prueba_missings_IAReproII\\virtual_env\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\prueba_missings_IAReproII\\virtual_env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\Desktop\\prueba_missings_IAReproII\\virtual_env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\prueba_missings_IAReproII\\virtual_env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32m~\\Desktop\\prueba_missings_IAReproII\\virtual_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\prueba_missings_IAReproII\\virtual_env\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\prueba_missings_IAReproII\\virtual_env\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\prueba_missings_IAReproII\\virtual_env\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import statistics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def aux_func(data, target_variable, only_normal=False, type_aux_mod=\"linear\", selection_method=None, \n",
    "             frac_feature_imp_normal=0.1, new_cols_corr_thr=0.2, seed=123):\n",
    "\n",
    "    # Custom transformer to apply target class weights to specific columns\n",
    "    class ColumnWeightTransformer(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, class_weights, columns_to_exclude_from_weights):\n",
    "            self.class_weights = class_weights\n",
    "            self.columns_to_exclude_from_weights = columns_to_exclude_from_weights        \n",
    "            # print(columns_to_exclude_from_weights)\n",
    "        \n",
    "        def fit(self, result_df, target=None):\n",
    "            return self\n",
    "        \n",
    "        def transform(self, result_df):\n",
    "            # Apply class weights to specified columns        \n",
    "            result_df_copy = result_df.copy()\n",
    "\n",
    "            for col in result_df.columns:\n",
    "                if col not in self.columns_to_exclude_from_weights:\n",
    "                    if (col != target_variable):\n",
    "                        result_df_copy[col] *= result_df[target_variable].map(self.class_weights)\n",
    "                    \n",
    "            result_df_copy = result_df_copy.drop(columns=[target_variable])        \n",
    "            return result_df_copy\n",
    "        \n",
    "    #################################\n",
    "    # PREPROCESSING\n",
    "    data = data.drop_duplicates()\n",
    "                            \n",
    "    # Handling missing values (drop rows with missing values for simplicity)\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    # Encoding categorical variables using one-hot encoding (OHE).\n",
    "    # First convert the target into categorical. This facilitates later\n",
    "    # calculations\n",
    "    data[target_variable] = data[target_variable].astype(\"category\")\n",
    "    # Now apply OHE to categorical columns (including the target)\n",
    "    data = pd.get_dummies(data)\n",
    "\n",
    "    # After OHE there will be two columns created for the target (since it was binary).\n",
    "    # Choose one of those columns as target and discard the other one. It is not important\n",
    "    # which of the two is selected as target, since the F1-score performance is measured\n",
    "    # using 'macro' average option\n",
    "    aux_names_target = [str(n) for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "    target_variable = aux_names_target[0]\n",
    "    data = data.drop(columns=[aux_names_target[1]])\n",
    "                            \n",
    "    # Normalizing numerical variables. Since previously OHE\n",
    "    # was performed, all variables with more than 2 different\n",
    "    # values will be numerical\n",
    "    numerical_columns = []\n",
    "    non_numerical_columns = []\n",
    "    for aux_num in data.columns:\n",
    "        unique_values_count = data[aux_num].nunique()\n",
    "        if (unique_values_count > 2):\n",
    "            numerical_columns.append(aux_num)\n",
    "        else:\n",
    "            non_numerical_columns.append(aux_num)\n",
    "    # Normalization\n",
    "    if (len(numerical_columns) != 0):\n",
    "        scaler = StandardScaler()\n",
    "        aux_data = scaler.fit_transform(data[numerical_columns])\n",
    "        aux_data = pd.DataFrame(aux_data, columns=numerical_columns)\n",
    "        aux_data = aux_data.reset_index(drop=True)\n",
    "        data_non_numerical = data[non_numerical_columns].reset_index(drop=True)\n",
    "        data = pd.concat([aux_data, data_non_numerical], axis=1)\n",
    "                                                                                \n",
    "\n",
    "    # Shuffle the DataFrame to randomize the rows\n",
    "    data = data.sample(frac=1, random_state=seed)  \n",
    "                            \n",
    "    # Save some registers for testing performance:\n",
    "    data_test = data.sample(frac=0.15, random_state=42)\n",
    "    data = data.drop(data_test.index)\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_counts = data[target_variable].value_counts()\n",
    "    total_samples = class_counts.sum()\n",
    "    class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "\n",
    "    # Define custom scorer\n",
    "    average_metric_type = 'macro'\n",
    "    custom_scorer = make_scorer(f1_score, average=average_metric_type)\n",
    "\n",
    "    #################################\n",
    "    # NORMAL MODEL\n",
    "\n",
    "    # If the function only returns the normal performance or the selection\n",
    "    # method of new variables is feature_imp_normal\n",
    "    if ((only_normal) | (selection_method is not None)):\n",
    "\n",
    "        X_normal = data.drop(columns=[target_variable])\n",
    "        y_normal = data[target_variable] \n",
    "\n",
    "        # Define RandomForestClassifier\n",
    "        rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        \n",
    "        # Define the parameter grid for grid search\n",
    "        min_samples_split_grid_search = [5, 10, 15]\n",
    "        min_samples_leaf_grid_search = [5, 10, 15]\n",
    "        param_grid = {\n",
    "            'min_samples_split': min_samples_split_grid_search,\n",
    "            'min_samples_leaf' : min_samples_leaf_grid_search\n",
    "        }\n",
    "\n",
    "        \n",
    "        # Perform grid search        \n",
    "        grid_search = GridSearchCV(estimator=rf_normal, param_grid=param_grid,\n",
    "                                   scoring=custom_scorer, cv=5, n_jobs=-1)\n",
    "        # The normal model takes imbalance into account to generate \n",
    "        # maximum normal performance\n",
    "        grid_search.fit(X_normal, y_normal, sample_weight=y_normal.map(class_weights))\n",
    "        \n",
    "        # Get the best normal model\n",
    "        rf_normal = grid_search.best_estimator_\n",
    "\n",
    "        # If the call to the function was to just calculate the\n",
    "        # normal performance\n",
    "        if (only_normal):\n",
    "            features_test_normal = data_test.drop(target_variable, axis=1)\n",
    "            target_test_normal = data_test[target_variable]\n",
    "            predictions_normal = rf_normal.predict(features_test_normal)\n",
    "            normal_score = f1_score(target_test_normal, predictions_normal, average=average_metric_type)\n",
    "                           \n",
    "            return (normal_score)\n",
    "    \n",
    "    \n",
    "    #######################################\n",
    "        \n",
    "                                                \n",
    "    #################################\n",
    "    # GENERATION OF AUXILIARY MODELS\n",
    "        \n",
    "    # List of dictionaries\n",
    "    list_of_dictionaries = []\n",
    "                            \n",
    "    # For each value of the target\n",
    "    for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                \n",
    "        # Generate auxiliary dataset\n",
    "        dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "        # Discard target in auxiliary dataset\n",
    "        dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "        # Generate dictionary of ficticious targets and the models that predict them:\n",
    "        dictionary_aux = {}\n",
    "                                \n",
    "        for fict_target in dataset_aux.columns.tolist():\n",
    "        \n",
    "            # Train auxiliary model and save it\n",
    "            X = dataset_aux.drop(columns=[fict_target])\n",
    "            y = dataset_aux[fict_target]                                 \n",
    "\n",
    "            if (type_aux_mod == \"linear\"):\n",
    "                aux_model = LinearRegression(n_jobs=-1)\n",
    "            else:\n",
    "                if (type_aux_mod == \"randomforest\"):\n",
    "                    aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5, n_jobs=-1)\n",
    "                else:\n",
    "                    print(\"Error: current allowed type for auxiliary models is 'linear' or 'randomforest'\")\n",
    "                    sys.exit()\n",
    "            \n",
    "            aux_model.fit(X, y)\n",
    "                                        \n",
    "            dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                    \n",
    "        list_of_dictionaries.append(dictionary_aux)    \n",
    "            \n",
    "\n",
    "    #################################\n",
    "    # GENERATION OF NEW COLUMNS BASED ON\n",
    "    # AUXILIARY MODELS\n",
    "        \n",
    "    list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "    list_of_rows_dataframe_new = []\n",
    "                          \n",
    "    new_columns = []\n",
    "\n",
    "    # For each value of the target\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "                              \n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "                                    \n",
    "        for fict_target in dictionary_case:\n",
    "                                    \n",
    "            X = data.drop(columns=[target_variable, fict_target])\n",
    "            y_predicted = dictionary_case[fict_target].predict(X)\n",
    "            y_real = data[fict_target]  \n",
    "                            \n",
    "            mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "            rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                          \n",
    "            # Add column to list of new columns\n",
    "            new_columns.append(rmse)\n",
    "\n",
    "        \n",
    "    dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "    names_cols_dataframe_new = []\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "        for u in dictionary_case.keys():\n",
    "            names_cols_dataframe_new.append(u + \"_\" + str(case))\n",
    "    dataframe_new.columns = names_cols_dataframe_new\n",
    "\n",
    "    cols_a = data.columns.to_list()\n",
    "    cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "    data = data.reset_index(drop=True)\n",
    "    dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                           \n",
    "    # Concatenate horizontally\n",
    "    result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "    result_df.columns = cols_b + cols_a    \n",
    "                            \n",
    "    #################################\n",
    "    # SELECTION OF NEW COLUMNS CREATED TO CREATE\n",
    "    # AUGMENTED DF.\n",
    "\n",
    "    if (selection_method == \"feature_imp_normal\"):\n",
    "        # Calculations for later selection of new columns\n",
    "        feature_importances = rf_normal.feature_importances_\n",
    "        # Create a list of (feature_name, importance) tuples\n",
    "        feature_importance_tuples = zip(X_normal.columns, feature_importances)\n",
    "        # Sort the tuples based on importance\n",
    "        sorted_feature_importance_tuples = sorted(feature_importance_tuples, key=lambda x: x[1], reverse=True)\n",
    "        # Extract sorted feature names\n",
    "        sorted_feature_names = [feature_name for feature_name, _ in sorted_feature_importance_tuples]\n",
    "        # Select some features\n",
    "        base_selected_columns = sorted_feature_names[0: int(len(sorted_feature_names) * frac_feature_imp_normal)]\n",
    "        \n",
    "        selected_columns = [] \n",
    "        for aux_base_cols in base_selected_columns:\n",
    "            selected_columns.append(aux_base_cols + \"_0\")\n",
    "            selected_columns.append(aux_base_cols + \"_1\")\n",
    "        \n",
    "    else:          \n",
    "        if (selection_method == \"correlation_target\"):\n",
    "            # Obtain new variables that have at least certain absolute correlation with the target\n",
    "            correlation = result_df.corr()[target_variable].abs()\n",
    "            # Select columns that have a at least certain absolute correlation with the target\n",
    "            selected_columns = correlation[correlation >= new_cols_corr_thr].index\n",
    "            selected_columns = [w for w in selected_columns if (w in dataframe_new.columns)]\n",
    "            \n",
    "        else:\n",
    "            if (selection_method is None):\n",
    "                # Select all created new columns\n",
    "                selected_columns = dataframe_new.columns.tolist()            \n",
    "                \n",
    "            else:\n",
    "                print(\"Error. The parameter selection_method must be 'feature_imp_normal', 'correlation_target' or None\")\n",
    "                sys.exit()\n",
    "                    \n",
    "\n",
    "    # The augmented dataframe will contain the original columns plus the \n",
    "    # selected new columns\n",
    "    result_df = result_df.loc[:, data.columns.tolist() + selected_columns]\n",
    "\n",
    "                            \n",
    "\n",
    "    #################################\n",
    "    # TRAINING OF MODEL BASED ON AUGMENTED DF.\n",
    "\n",
    "    features = result_df.drop(target_variable, axis=1)\n",
    "    target = result_df[target_variable]\n",
    "\n",
    "    columns_to_exclude_from_weights = [s for s in features.columns if (s in dataframe_new.columns.tolist())] \n",
    "    # Define a preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        ('apply_class_weights', ColumnWeightTransformer(class_weights, columns_to_exclude_from_weights), slice(None)), # With slice(None) all columns are passed, but internally only those not present in columns_to_exclude_from_weights will be treated\n",
    "        ('other_columns', 'passthrough', columns_to_exclude_from_weights)  # Treat these columns normally\n",
    "        ]\n",
    "        )\n",
    "    # Define RandomForestClassifier pipeline\n",
    "    rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier()) \n",
    "    ])\n",
    "    # Implement in GridSearchCV\n",
    "    min_samples_split_grid_search = [5, 10, 15]\n",
    "    min_samples_leaf_grid_search = [5, 10, 15]\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [100],\n",
    "        'classifier__min_samples_split': min_samples_split_grid_search,\n",
    "        'classifier__min_samples_leaf': min_samples_leaf_grid_search\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=rf_pipeline, param_grid=param_grid, \n",
    "                               scoring=custom_scorer, cv=5, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    # Target imbalance is only handled for the columns that are not new, i.e those\n",
    "    # of the original dataframe.\n",
    "    # grid_search needs to receive the dataset with all columns (including target),\n",
    "    # ATTENION: but it receives the target only so that ColumnWeightTransformer can handle weights\n",
    "    # of the original columns, ColumnWeightTransformer after doing so drops\n",
    "    # the target variable so that it does not take into account in training.\n",
    "    # If you are not convinced, you can create an artificial attribute column that is equal\n",
    "    # to the target, and you shall see that the performance is much better than the one\n",
    "    # that is actually obtained here.\n",
    "    grid_search.fit(result_df, target)\n",
    "  \n",
    "    # grid_search.fit(features, target)\n",
    "    \n",
    "    # Get the best model and corresponding score\n",
    "    rf_model = grid_search.best_estimator_\n",
    "                            \n",
    "\n",
    "    #################################\n",
    "    # PROCESSING OF TEST DATASET SO THAT THE TRAINED MODEL\n",
    "    # CAN BE APPLIED TO IT, AND IN THIS WAY OBTAIN \n",
    "    # PERFORMANCE METRICS\n",
    "    \n",
    "    # For each value of the target\n",
    "    new_columns = []\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "        \n",
    "        for fict_target in dictionary_case:\n",
    "            if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                y_real = data_test[fict_target]  \n",
    "                               \n",
    "                mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                \n",
    "                # Add column to list of new columns\n",
    "                new_columns.append(rmse)\n",
    "                            \n",
    "    dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "    names_cols_dataframe_new2 = []\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "        for fict_target in dictionary_case:\n",
    "            if ((fict_target + '_' + str(case)) in selected_columns):    \n",
    "                names_cols_dataframe_new2.append(fict_target + \"_\" + str(case))\n",
    "    dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "    cols_a = data_test.columns.to_list()\n",
    "    cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "    data_test = data_test.reset_index(drop=True)\n",
    "    dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                                                    \n",
    "    # Concatenate horizontally\n",
    "    data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "    data_test_processed.columns = cols_b + cols_a\n",
    "    # Adapt order of columns to the order in which the model was\n",
    "    # trained\n",
    "    data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                         \n",
    "    # Now apply trained model on test dataset to gauge performance\n",
    "    features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "    target_test = data_test_processed[target_variable]\n",
    "    # predictions = rf_model.predict(features_test)\n",
    "    # rf_model needs to receive the dataset with all columns (including target),\n",
    "    # ATTENION: but it receives the target only so that ColumnWeightTransformer can handle weights\n",
    "    # of the original columns, ColumnWeightTransformer after doing so drops\n",
    "    # the target variable so that it does not take into account in training.\n",
    "    # If you are not convinced, you can create an artificial attribute column that is equal\n",
    "    # to the target, and you shall see that the performance is much better than the one\n",
    "    # that is actually obtained here.\n",
    "    predictions = rf_model.predict(data_test_processed)\n",
    "                           \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(target_test, predictions, average=average_metric_type)\n",
    "    return (f1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "\n",
    "# TEST ON DATAFRAMES FROM UCI, DOWNLOAD ADAPTED FROM Perales-González, Carlos, (2020). UCI download-process, v1.3, GitHub repository, https://github.com/cperales/uci-download-process\n",
    "\n",
    "# Number of statistical repetitions\n",
    "num_stat_rep = 5\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "\n",
    "for i in files_classification:\n",
    "    # try:\n",
    "    if \"haber\" in i:\n",
    "        \n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header_option = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]  \n",
    "                   data_url = data_url.strip()\n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       try:\n",
    "                           if (len(lines[x].split(\" = \")) == 1):\n",
    "                               pass\n",
    "                           # If the separator is specified    \n",
    "                           else:\n",
    "                               separator = lines[x].split(\" = \")[1]\n",
    "                               separator = separator.strip()\n",
    "                               if (\"comma\" in separator):\n",
    "                                   separator = \",\"\n",
    "                       except:\n",
    "                           pass\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           try:\n",
    "                               target_index = lines[x].split(\" = \")[1]\n",
    "                               target_index = int(target_index.strip())\n",
    "                           except:\n",
    "                               pass\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               try:\n",
    "                                   # If there is not header specified, assume\n",
    "                                   # default value\n",
    "                                   if (len(lines[x].split(\" = \")) == 1):\n",
    "                                        pass\n",
    "                                   else:\n",
    "                                       # If it is specified\n",
    "                                       header_option = lines[x].split(\" = \")[1]\n",
    "                                       header_option = int(header_option.strip())\n",
    "                                       # If it is 0 assign it to None\n",
    "                                       if (str(header_option).startswith(\"0\")):\n",
    "                                           header_option = None\n",
    "                               except:\n",
    "                                   pass\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python', header=header_option)\n",
    "                aux_names_columns = [str(aux_n_c) for aux_n_c in data.columns]\n",
    "                data.columns = aux_names_columns\n",
    "                    \n",
    "                # If there will be enough dimensionality after one hot encoding\n",
    "                data_check = pd.get_dummies(data)\n",
    "                if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                    # Name of the target of the dataset (target_index - 1 since \n",
    "                    # in python first position is 0)\n",
    "                    target_variable = str(data.columns.tolist()[int(target_index) - 1])\n",
    "        \n",
    "                    # If it is a binary classification\n",
    "                    unique_values_count = data[target_variable].nunique()\n",
    "                    if (unique_values_count == 2):\n",
    "                        \n",
    "                        normal_performance = []\n",
    "                        for c in range(0, num_stat_rep):\n",
    "                            normal_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                           only_normal=True, seed=c))\n",
    "\n",
    "                        normal_performance_stdev = statistics.stdev(normal_performance)\n",
    "                        normal_performance = statistics.mean(normal_performance)\n",
    "                        print(\"Normal performance: \" + str(round(normal_performance, 4)) + \" +- \" + str(round(normal_performance_stdev, 4)))\n",
    "                        \n",
    "                        cases_new = [\"type_aux_mode=linear; selection_method_case=None\"]\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=None\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\")\n",
    "                        cases_new.append(\"type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\")\n",
    "                        cases_new.append(\"type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\")\n",
    "                        cases_new.append(\"type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\")\n",
    "                        cases_new.append(\"type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\")\n",
    "\n",
    "                        cases_new_performances = []\n",
    "                        cases_new_performances_stdev = []\n",
    "                        \n",
    "                        \n",
    "                        for case in cases_new:\n",
    "                            \n",
    "                            if (\"linear\" in case):\n",
    "                                model_aux = \"linear\"\n",
    "                            else:\n",
    "                                model_aux = \"randomforest\"\n",
    "\n",
    "                            if (\"None\" in case):\n",
    "                                new_performance = []\n",
    "                                for c in range(0, num_stat_rep):                            \n",
    "                                    new_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                                   type_aux_mod=model_aux, selection_method=None,\n",
    "                                                                    seed=c))\n",
    "                                new_performance_stdev = statistics.stdev(new_performance)    \n",
    "                                cases_new_performances_stdev.append(new_performance_stdev)\n",
    "                                new_performance = statistics.mean(new_performance)        \n",
    "                                cases_new_performances.append(new_performance)\n",
    "                                \n",
    "                            else:     \n",
    "                                if (\"feature_imp_normal\" in case):\n",
    "                                    frac_feature_imp_normal_value = float(case.split(\"frac_feature_imp_normal=\")[1])\n",
    "                                    new_performance = []\n",
    "                                    for c in range(0, num_stat_rep):         \n",
    "                                        new_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                                      type_aux_mod=model_aux, selection_method=\"feature_imp_normal\", \n",
    "                                                                      frac_feature_imp_normal=frac_feature_imp_normal_value,\n",
    "                                                                      seed=c))\n",
    "                                    new_performance_stdev = statistics.stdev(new_performance)\n",
    "                                    cases_new_performances_stdev.append(new_performance_stdev)    \n",
    "                                    new_performance = statistics.mean(new_performance)        \n",
    "                                    cases_new_performances.append(new_performance)\n",
    "                                else:\n",
    "                                    new_cols_corr_thr_value = float(case.split(\"new_cols_corr_thr=\")[1])\n",
    "                                    new_performance = []\n",
    "                                    for c in range(0, num_stat_rep):         \n",
    "                                        new_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                                     type_aux_mod=model_aux, selection_method=\"correlation_target\", \n",
    "                                                                     new_cols_corr_thr=new_cols_corr_thr_value, \n",
    "                                                                     seed=c))\n",
    "                                        \n",
    "                                    new_performance_stdev = statistics.stdev(new_performance)\n",
    "                                    cases_new_performances_stdev.append(new_performance_stdev)    \n",
    "                                    new_performance = statistics.mean(new_performance)        \n",
    "                                    cases_new_performances.append(new_performance)\n",
    "                                    \n",
    "                            print(case)\n",
    "                            print(str(round(new_performance, 4)) + \" +- \" + str(round(new_performance_stdev, 4)))\n",
    "                            \n",
    "\n",
    "                        # Save result\n",
    "                        result = pd.DataFrame({\"Case\": cases_new, \"Mean F1-score\": cases_new_performances, \"Stdev F1-score\": cases_new_performances_stdev})\n",
    "                        result[\"Normal Mean F1-score\"] = normal_performance\n",
    "                        result[\"Normal stdev F1-score\"] = normal_performance_stdev\n",
    "                        result = result.sort_values(by=\"Mean F1-score\", ascending=False)\n",
    "                        data_name = data_url.split(\"/\")\n",
    "                        data_name = data_name[len(data_name) - 1]\n",
    "                        display(result)\n",
    "                        result.to_csv(\"metrics_\" + data_name + \".csv\", index=False)\n",
    "                        \n",
    "                        # Get maximum performance of the new cases\n",
    "                        if (max(cases_new_performances) > normal_performance):\n",
    "                            \n",
    "                            print(\"New performance: \" + str(round(max(cases_new_performances), 3)))\n",
    "                            # Print details\n",
    "                            max_index = cases_new_performances.index(max(cases_new_performances))    \n",
    "                            print(\"Optimum case new: \" + str(cases_new[max_index]))\n",
    "                        else:\n",
    "                            print(\"No improvement found\")                        \n",
    "                                                        \n",
    "            \n",
    "            # except:   \n",
    "            else:    \n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "                    \n",
    "            \n",
    "    # except FileNotFoundError:  \n",
    "    else:\n",
    "        print(f\"The file '{i}' does not exist.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
