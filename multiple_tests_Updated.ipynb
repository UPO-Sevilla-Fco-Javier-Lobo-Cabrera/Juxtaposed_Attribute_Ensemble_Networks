{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5765ea6-bc3a-4730-ad0f-6b1e87b3e1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(32560, 15)\n",
      "F1 Score normal: [0.67607004 0.9135514 ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.66268062 0.91267896]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(624, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(15, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1371, 5)\n",
      "F1 Score normal: [0.98689956 0.98285714]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99559471 0.99435028]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(748, 5)\n",
      "F1 Score normal: [0.87218045 0.37037037]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.88721805 0.44444444]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(285, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(568, 32)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(698, 11)\n",
      "F1 Score normal: [0.97222222 0.9375    ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.95833333 0.90625   ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(197, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1727, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(28055, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3195, 37)\n",
      "F1 Score normal: [0.98181818 0.98056156]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.96957404 0.96774194]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(540, 461)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1079, 857)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(434, 17)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(989, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(207, 61)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1472, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(689, 16)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(365, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(10000, 14)\n",
      "F1 Score normal: [0.99947562 0.99908509]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99947562 0.99908509]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(99, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(213, 11)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(305, 4)\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.75757576 0.2       ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(131, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(302, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(293, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(154, 20)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(299, 28)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(582, 11)\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.77372263 0.06060606]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(350, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(149, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19999, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(359, 91)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(31, 57)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19019, 11)\n",
      "F1 Score normal: [0.80639823 0.90904379]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.79329609 0.9046883 ]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(960, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(8123, 23)\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [1. 1.]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3822, 65)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2533, 74)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2535, 74)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(195, 24)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(7493, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(181, 13)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(25009, 11)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1054, 42)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(209, 8)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(245056, 4)\n",
      "F1 Score normal: [0.99515571 0.99810794]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99792292 0.99918926]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(46, 36)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4600, 58)\n",
      "F1 Score normal: [0.95844156 0.93495935]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.96083551 0.93951613]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 23)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 45)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(999, 21)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4434, 37)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(150, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3771, 22)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(214, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3162, 26)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(957, 10)\n",
      "F1 Score normal: [0.98412698 0.96969697]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99465241 0.99009901]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 3)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 25)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4025, 1)\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(177, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1483, 10)\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares lineales\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]   \n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       if (len(lines[x].split(\" = \")) == 1):\n",
    "                           pass\n",
    "                       # If the separator is specified    \n",
    "                       else:\n",
    "                           separator = lines[x].split(\" = \")[1]\n",
    "                           separator = separator.strip()\n",
    "                           if (\"comma\" in separator):\n",
    "                               separator = \",\"\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           target_index = lines[x].split(\" = \")[1]\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               # If there is not header specified, assume\n",
    "                               # default value\n",
    "                               if (len(lines[x].split(\" = \")) == 1):\n",
    "                                    pass\n",
    "                               else:\n",
    "                                   # If it is specified\n",
    "                                   header = lines[x].split(\" = \")[1]\n",
    "                                   # If it is 0 assign it to None\n",
    "                                   if (header.startswith(\"0\")):\n",
    "                                       header = None\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python')\n",
    "                    print(data.shape)\n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = data.columns.tolist()[int(target_index) - 1]\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [n for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "                                        # aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        aux_model = LinearRegression()\n",
    "                                        aux_model.fit(X_train, y_train)\n",
    "                                        dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared = 1\n",
    "                                        else:    \n",
    "                                            r_squared = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "                                        \n",
    "                                        dictionary_aux_r_squared[fict_target] = r_squared\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "                            \n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data_test[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "                            \n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to read the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2863a78b-a0d0-4ac0-b7c8-f3543112e7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(32560, 15)\n",
      "F1 Score normal: [0.67607004 0.9135514 ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.64616886 0.90597847]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(624, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(15, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1371, 5)\n",
      "F1 Score normal: [0.98689956 0.98285714]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [1. 1.]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(748, 5)\n",
      "F1 Score normal: [0.87218045 0.37037037]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.88888889 0.4       ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(285, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(568, 32)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(698, 11)\n",
      "F1 Score normal: [0.97222222 0.9375    ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.96551724 0.92063492]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(197, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1727, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(28055, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3195, 37)\n",
      "F1 Score normal: [0.98181818 0.98056156]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.98167006 0.98072805]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(540, 461)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1079, 857)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(434, 17)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(989, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(207, 61)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1472, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(689, 16)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(365, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(10000, 14)\n",
      "F1 Score normal: [0.99947562 0.99908509]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [1. 1.]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(99, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(213, 11)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(305, 4)\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.75757576 0.2       ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(131, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(302, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(293, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(154, 20)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(299, 28)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(582, 11)\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.79104478 0.22222222]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(350, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(149, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19999, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(359, 91)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(31, 57)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19019, 11)\n",
      "F1 Score normal: [0.80639823 0.90904379]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.8127572  0.90236052]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(960, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(8123, 23)\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [1. 1.]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3822, 65)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2533, 74)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2535, 74)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(195, 24)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(7493, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(181, 13)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(25009, 11)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip\n",
      "...\n",
      "An error occurred: <urlopen error [WinError 10060] Se produjo un error durante el intento de conexión ya que la parte conectada no respondió adecuadamente tras un periodo de tiempo, o bien se produjo un error en la conexión establecida ya que el host conectado no ha podido responder>\n",
      "(25009, 11)\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1054, 42)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(209, 8)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(245056, 4)\n",
      "F1 Score normal: [0.99515571 0.99810794]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99351251 0.99748156]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(46, 36)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4600, 58)\n",
      "F1 Score normal: [0.95844156 0.93495935]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.96732026 0.94969819]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 23)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 45)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(999, 21)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4434, 37)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(150, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3771, 22)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(214, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3162, 26)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(957, 10)\n",
      "F1 Score normal: [0.98412698 0.96969697]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.93939394 0.86666667]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 3)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 25)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4025, 1)\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(177, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1483, 10)\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares de RF\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]   \n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       if (len(lines[x].split(\" = \")) == 1):\n",
    "                           pass\n",
    "                       # If the separator is specified    \n",
    "                       else:\n",
    "                           separator = lines[x].split(\" = \")[1]\n",
    "                           separator = separator.strip()\n",
    "                           if (\"comma\" in separator):\n",
    "                               separator = \",\"\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           target_index = lines[x].split(\" = \")[1]\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               # If there is not header specified, assume\n",
    "                               # default value\n",
    "                               if (len(lines[x].split(\" = \")) == 1):\n",
    "                                    pass\n",
    "                               else:\n",
    "                                   # If it is specified\n",
    "                                   header = lines[x].split(\" = \")[1]\n",
    "                                   # If it is 0 assign it to None\n",
    "                                   if (header.startswith(\"0\")):\n",
    "                                       header = None\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python')\n",
    "                    print(data.shape)\n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = data.columns.tolist()[int(target_index) - 1]\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [n for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "                                        aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        # aux_model = LinearRegression()\n",
    "                                        aux_model.fit(X_train, y_train)\n",
    "                                        dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared = 1\n",
    "                                        else:    \n",
    "                                            r_squared = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "                                        \n",
    "                                        dictionary_aux_r_squared[fict_target] = r_squared\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "                            \n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data_test[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "                            \n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to read the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0f67f-b689-4f11-b24e-920b43d25ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se puede observar que dependiendo de si se usan modelos auxiliares lineales o de RF\n",
    "# se obtienen unos resultados u otros --> Para cada dataset habrá que ver cuál es la mejor\n",
    "# opción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b9a23-8e4e-4eb4-ad77-0d6cb21c8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebas añadiendo como nuevas columnas solo aquellas que estén\n",
    "# por encima de un cierto umbral en su correlación con la target.\n",
    "# Por lo menos estableciendo un threshold del 20%, en la mayoría\n",
    "# de los casos empeora, pero en algunos se obtiene una mejora\n",
    "# significativa --> Para cada dataset habrá que ver cuál es la mejor\n",
    "# opción, o quizá buscar otra forma de probar con distintas nuevas\n",
    "# columnas\n",
    "\n",
    "# Para todas estas pruebas además habrá que realizar varias repeticiones\n",
    "# estadísticas para obtener datos más fiables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9a73785-c57c-4440-adb7-2be09324b697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(32560, 15)\n",
      "F1 Score normal: [0.67607004 0.9135514 ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "[]\n",
      "0\n",
      "(27656, 325)\n",
      "F1 Score: [0.67607004 0.9135514 ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(624, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(15, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1371, 5)\n",
      "F1 Score normal: [0.98689956 0.98285714]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['3.6216_0', '3.6216_1', '8.6661_0', '-2.8073_0', '-2.8073_1', '-0.44699_0', '-0.44699_1']\n",
      "7\n",
      "(1145, 13)\n",
      "F1 Score: [0.03208556 0.16589862]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(748, 5)\n",
      "F1 Score normal: [0.87218045 0.37037037]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['Monetary (c.c. blood)_0']\n",
      "1\n",
      "(453, 13)\n",
      "F1 Score: [0.87301587 0.52941176]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(285, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(568, 32)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(698, 11)\n",
      "F1 Score normal: [0.97222222 0.9375    ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['1000025_1', '5_0', '5_1', '1_0', '1_1', '1.1_0', '1.1_1', '1.2_0', '1.2_1', '2_0', '2_1', '3_0', '3_1', '1.4_0', '1.5_1', '1.3_1_1', '1.3_10_1', '1.3_2_0', '1.3_3_1', '1.3_4_0', '1.3_4_1', '1.3_1']\n",
      "22\n",
      "(586, 61)\n",
      "F1 Score: [0.95833333 0.90625   ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(197, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1727, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(28055, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3195, 37)\n",
      "F1 Score normal: [0.98181818 0.98056156]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['t_t_1', 'f.15_f_0', 'f.15_f_1', 'f.15_t_0', 'f.15_t_1', 'f.16_f_0', 'f.16_f_1', 'f.16_t_0', 'f.16_t_1', 'f.17_f_0', 'f.17_f_1', 'f.17_t_0', 'f.17_t_1', 'f.18_f_0', 'f.18_f_1', 'f.18_t_0', 'f.18_t_1', 'f.19_f_0', 'f.19_f_1', 'f.19_t_0', 'f.19_t_1', 'f.20_f_0', 'f.20_f_1', 'f.20_t_0', 'f.20_t_1', 'f.21_f_0', 'f.21_f_1', 'f.21_t_0', 'f.21_t_1', 't.1_f_0', 't.1_f_1', 't.1_t_0', 't.1_t_1', 'f.22_f_0', 'f.22_f_1', 'f.22_t_0', 'f.22_t_1', 'f.23_f_0', 'f.23_f_1', 'f.23_t_0', 'f.23_t_1', 'f.24_f_0', 'f.24_f_1', 'f.24_t_0', 'f.24_t_1', 'f.25_f_0', 'f.25_f_1', 'f.25_t_0', 'f.25_t_1', 'f.26_f_0', 'f.26_f_1', 'f.26_t_0', 'f.26_t_1', 'f.27_f_0', 'f.27_f_1', 'f.27_t_0', 'f.27_t_1', 'f.28_f_0', 'f.28_f_1', 'f.28_t_0', 'f.28_t_1', 't.2_f_0', 't.2_f_1', 't.2_t_0', 't.2_t_1', 't.3_f_0', 't.3_f_1', 't.3_t_0', 't.3_t_1', 'n.1_n_0', 'n.1_n_1', 'n.1_t_0', 'n.1_t_1']\n",
      "73\n",
      "(2716, 220)\n",
      "F1 Score: [0.85       0.87258687]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(540, 461)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1079, 857)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(434, 17)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(989, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(207, 61)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1472, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(689, 16)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(365, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(10000, 14)\n",
      "F1 Score normal: [0.99947562 0.99908509]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['p3_0', 'stab_1']\n",
      "2\n",
      "(8500, 40)\n",
      "F1 Score: [0.99947562 0.99908509]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(99, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(213, 11)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(305, 4)\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['64_0']\n",
      "1\n",
      "(245, 10)\n",
      "F1 Score: [0.78125    0.36363636]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(131, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(302, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(293, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(154, 20)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(299, 28)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(582, 11)\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['3.3_0']\n",
      "1\n",
      "(480, 34)\n",
      "F1 Score: [0.80597015 0.27777778]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(350, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(149, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19999, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(359, 91)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(31, 57)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19019, 11)\n",
      "F1 Score normal: [0.80639823 0.90904379]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['0.3918_0', '0.3918_1', '0.1982_0', '27.7004_0', '27.7004_1', '22.011_0', '40.092_0', '40.092_1', '81.8828_0', '81.8828_1']\n",
      "10\n",
      "(16068, 31)\n",
      "F1 Score: [0.58853974 0.62487361]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(960, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(8123, 23)\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['x_b_0', 'x_b_1', 'x_c_0', 'x_c_1', 'x_f_1', 'x_k_0', 'x_k_1', 'x_s_0', 'x_s_1', 'x_x_0', 'x_x_1', 's_f_0', 's_f_1', 's_g_0', 's_g_1', 's_y_0', 's_y_1', 'n_b_0', 'n_b_1', 'n_c_0', 'n_c_1', 'n_e_0', 'n_e_1', 'n_g_0', 'n_g_1', 'n_n_0', 'n_n_1', 'n_p_0', 'n_p_1', 'n_r_0', 'n_r_1', 'n_u_0', 'n_u_1', 'n_w_0', 'n_w_1', 'n_y_1', 't_f_0', 't_f_1', 't_t_0', 'p.1_a_0', 'p.1_a_1', 'p.1_c_0', 'p.1_c_1', 'p.1_f_0', 'p.1_f_1', 'p.1_l_0', 'p.1_l_1', 'p.1_m_0', 'p.1_m_1', 'p.1_n_0', 'p.1_p_0', 'p.1_p_1', 'p.1_s_0', 'p.1_y_0', 'p.1_y_1', 'f_a_0', 'f_a_1', 'f_f_0', 'f_f_1', 'c_c_1', 'c_w_0', 'n.1_b_0', 'n.1_b_1', 'n.1_n_0', 'n.1_n_1', 'k_b_0', 'k_e_0', 'k_e_1', 'k_g_1', 'k_h_0', 'k_h_1', 'k_o_0', 'k_o_1', 'k_p_0', 'k_p_1', 'k_r_0', 'k_u_0', 'k_u_1', 'k_w_0', 'k_y_0', 'k_y_1', 'e_e_0', 'e_t_0', 'e.1_?_0', 'e.1_b_0', 'e.1_b_1', 'e.1_c_0', 'e.1_c_1', 'e.1_e_0', 'e.1_e_1', 'e.1_r_0', 'e.1_r_1', 's.1_f_0', 's.1_f_1', 's.1_k_0', 's.1_k_1', 's.1_s_1', 's.1_y_1', 's.2_f_0', 's.2_f_1', 's.2_k_0', 's.2_k_1', 's.2_s_1', 's.2_y_0', 's.2_y_1', 'w_b_0', 'w_b_1', 'w_c_0', 'w_c_1', 'w_e_0', 'w_e_1', 'w_g_0', 'w_g_1', 'w_n_0', 'w_n_1', 'w_o_0', 'w_o_1', 'w_p_1', 'w_w_0', 'w_y_0', 'w.1_b_0', 'w.1_b_1', 'w.1_c_0', 'w.1_c_1', 'w.1_e_0', 'w.1_e_1', 'w.1_g_0', 'w.1_g_1', 'w.1_n_0', 'w.1_n_1', 'w.1_o_0', 'w.1_o_1', 'w.1_p_0', 'w.1_p_1', 'w.1_w_0', 'w.1_w_1', 'w.1_y_1', 'p.2_p_0', 'p.2_p_1', 'w.2_n_0', 'w.2_n_1', 'w.2_o_0', 'w.2_o_1', 'w.2_w_0', 'w.2_w_1', 'w.2_y_0', 'w.2_y_1', 'o_n_0', 'o_n_1', 'o_o_0', 'o_o_1', 'o_t_0', 'o_t_1', 'p.3_e_0', 'p.3_e_1', 'p.3_f_1', 'p.3_l_0', 'p.3_l_1', 'p.3_n_0', 'p.3_n_1', 'p.3_p_0', 'k.1_b_0', 'k.1_h_0', 'k.1_h_1', 'k.1_k_0', 'k.1_k_1', 'k.1_n_0', 'k.1_n_1', 'k.1_r_0', 'k.1_r_1', 'k.1_u_0', 'k.1_w_1', 'k.1_y_0', 'k.1_y_1', 's.3_a_0', 's.3_a_1', 's.3_c_1', 's.3_n_0', 's.3_n_1', 's.3_s_0', 's.3_s_1', 's.3_v_0', 's.3_y_0', 's.3_y_1', 'u_d_0', 'u_d_1', 'u_g_0', 'u_g_1', 'u_l_0', 'u_l_1', 'u_m_0', 'u_m_1', 'u_p_0', 'u_p_1', 'u_u_0', 'u_u_1', 'u_w_0', 'u_w_1']\n",
      "198\n",
      "(6905, 352)\n",
      "F1 Score: [0.92779783 0.93975904]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3822, 65)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2533, 74)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2535, 74)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(195, 24)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(7493, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(181, 13)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(25009, 11)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1054, 42)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(209, 8)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(245056, 4)\n",
      "F1 Score normal: [0.99515571 0.99810794]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['74_0', '74_1', '85_0', '123_0', '123_1']\n",
      "5\n",
      "(43727, 10)\n",
      "F1 Score: [0.93531853 0.97662521]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(46, 36)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4600, 58)\n",
      "F1 Score normal: [0.95844156 0.93495935]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['0.1_0', '0.1_1', '0.32_0', '0.2_0', '0.4_1', '0.5_0', '0.6_1', '0.64.2_0', '0.18_0', '0.21_0', '0.21_1', '0.25_0', '0.25_1', '0.26_0', '0.26_1', '0.27_0', '0.27_1', '0.29_0', '0.29_1', '0.30_1', '0.31_0', '0.33_1', '0.34_0', '0.35_0', '0.36_0', '0.36_1', '0.39_0', '0.39_1', '0.40_1', '0.42_1', '0.43_1', '0.778_0', '0.45_0', '61_1', '278_0', '278_1']\n",
      "36\n",
      "(3578, 172)\n",
      "F1 Score: [0.88520055 0.84601113]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 23)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 45)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(999, 21)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4434, 37)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(150, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3771, 22)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(214, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3162, 26)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(957, 10)\n",
      "F1 Score normal: [0.98412698 0.96969697]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['x_b_0', 'x_b_1', 'x_o_0', 'x_o_1', 'x_x_0', 'x_x_1', 'x.1_b_0', 'x.1_b_1', 'x.1_o_0', 'x.1_o_1', 'x.1_x_0', 'x.1_x_1', 'x.2_b_0', 'x.2_b_1', 'x.2_o_0', 'x.2_o_1', 'x.2_x_0', 'x.2_x_1', 'x.3_b_0', 'x.3_b_1', 'x.3_o_0', 'x.3_o_1', 'x.3_x_0', 'x.3_x_1', 'o_b_0', 'o_b_1', 'o_o_0']\n",
      "27\n",
      "(813, 82)\n",
      "F1 Score: [0.78481013 0.        ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 3)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 25)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4025, 1)\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(177, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1483, 10)\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares lineales y threshold de 20%\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Threshold of correlation with target for new columns\n",
    "new_cols_threshold = 0.2\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]   \n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       if (len(lines[x].split(\" = \")) == 1):\n",
    "                           pass\n",
    "                       # If the separator is specified    \n",
    "                       else:\n",
    "                           separator = lines[x].split(\" = \")[1]\n",
    "                           separator = separator.strip()\n",
    "                           if (\"comma\" in separator):\n",
    "                               separator = \",\"\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           target_index = lines[x].split(\" = \")[1]\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               # If there is not header specified, assume\n",
    "                               # default value\n",
    "                               if (len(lines[x].split(\" = \")) == 1):\n",
    "                                    pass\n",
    "                               else:\n",
    "                                   # If it is specified\n",
    "                                   header = lines[x].split(\" = \")[1]\n",
    "                                   # If it is 0 assign it to None\n",
    "                                   if (header.startswith(\"0\")):\n",
    "                                       header = None\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python')\n",
    "                    print(data.shape)\n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = data.columns.tolist()[int(target_index) - 1]\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [n for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "                                        # aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        aux_model = LinearRegression()\n",
    "                                        aux_model.fit(X_train, y_train)\n",
    "                                        dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared = 1\n",
    "                                        else:    \n",
    "                                            r_squared = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "                                        \n",
    "                                        dictionary_aux_r_squared[fict_target] = r_squared\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "                            \n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "\n",
    "                            #####\n",
    "                            # Calculate correlation with the target variable\n",
    "                            correlation = result_df.corr()[target_variable].abs()\n",
    "                            # Select columns whose names end with '_0' or '_1' and have a at least certain absolute correlation with the target\n",
    "                            selected_columns = correlation[(correlation.index.str.endswith('_0') | correlation.index.str.endswith('_1')) & (correlation >= new_cols_threshold)].index\n",
    "                            selected_columns = selected_columns.tolist()\n",
    "                            print(selected_columns)\n",
    "                            print(len(selected_columns))\n",
    "                            print(result_df.shape)\n",
    "                            \n",
    "                            result_df = result_df[data.columns.tolist() + selected_columns]\n",
    "                            #####\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                                        X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                        y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                        y_real = data_test[fict_target]  \n",
    "                                \n",
    "                                        mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                                        rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                        weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                        # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "\n",
    "                                        rmse = [1e5 if (x > 1e5) else x for x in rmse]\n",
    "                                \n",
    "                                        # Add column to list of new columns\n",
    "                                        new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new2 = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                for u in dictionary_case.keys():\n",
    "                                    if ((u + '_' + str(case)) in selected_columns):    \n",
    "                                        names_cols_dataframe_new2.append(u + \"_\" + str(case))\n",
    "                            dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            # Adapt order of columns to the order in which the model was\n",
    "                            # trained\n",
    "                            data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to read the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fb27a59-f581-4aca-82db-90779eef8a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(32560, 15)\n",
      "F1 Score normal: [0.67607004 0.9135514 ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['39_0', ' 13_1', ' Bachelors_ 12th_1', ' Bachelors_ Assoc-voc_0', ' Bachelors_ Preschool_0', ' Not-in-family_ Other-relative_1']\n",
      "6\n",
      "(27656, 325)\n",
      "F1 Score: [0.6651439 0.9031832]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(624, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(15, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1371, 5)\n",
      "F1 Score normal: [0.98689956 0.98285714]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['3.6216_0', '3.6216_1', '8.6661_0', '8.6661_1', '-2.8073_0', '-2.8073_1', '-0.44699_0', '-0.44699_1']\n",
      "8\n",
      "(1145, 13)\n",
      "F1 Score: [0.11458333 0.19811321]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(748, 5)\n",
      "F1 Score normal: [0.87218045 0.37037037]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['Monetary (c.c. blood)_0']\n",
      "1\n",
      "(453, 13)\n",
      "F1 Score: [0.87692308 0.46666667]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(285, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(568, 32)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(698, 11)\n",
      "F1 Score normal: [0.97222222 0.9375    ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['1000025_1', '5_0', '5_1', '1_0', '1_1', '1.1_0', '1.1_1', '1.2_0', '1.2_1', '2_0', '2_1', '3_0', '3_1', '1.4_0', '1.5_1', '1.3_1_1', '1.3_10_1', '1.3_2_0', '1.3_2_1', '1.3_3_1', '1.3_4_0', '1.3_5_0', '1.3_5_1', '1.3_7_0', '1.3_8_0', '1.3_8_1', '1.3_9_0', '1.3_?_1', '1.3_1']\n",
      "29\n",
      "(586, 61)\n",
      "F1 Score: [0.90225564 0.82666667]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(197, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1727, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(28055, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3195, 37)\n",
      "F1 Score normal: [0.98181818 0.98056156]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['f.24_f_0', 'f.24_f_1']\n",
      "2\n",
      "(2716, 220)\n",
      "F1 Score: [0.87804878 0.89151874]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(540, 461)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1079, 857)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(434, 17)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(989, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(207, 61)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1472, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(689, 16)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(365, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(10000, 14)\n",
      "F1 Score normal: [0.99947562 0.99908509]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['tau1_0', 'tau1_1', 'tau2_0', 'tau2_1', 'tau3_0', 'tau3_1', 'tau4_0', 'tau4_1', 'p1_0', 'p1_1', 'p2_0', 'p2_1', 'p3_0', 'p3_1', 'p4_0', 'p4_1', 'g1_0', 'g2_0', 'g3_0', 'g3_1', 'g4_0', 'g4_1', 'stab_0', 'stab_1']\n",
      "24\n",
      "(8500, 40)\n",
      "F1 Score: [0.99947562 0.99908509]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(99, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(213, 11)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(305, 4)\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['64_0']\n",
      "1\n",
      "(245, 10)\n",
      "F1 Score: [0.7761194  0.21052632]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(131, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(302, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(293, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(154, 20)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(299, 28)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(582, 11)\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['65_0', '6.8_1', '3.3_0']\n",
      "3\n",
      "(480, 34)\n",
      "F1 Score: [0.8        0.06666667]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(350, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(149, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19999, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(359, 91)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(31, 57)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19019, 11)\n",
      "F1 Score normal: [0.80639823 0.90904379]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['16.0021_0', '0.1982_0', '27.7004_0', '27.7004_1', '22.011_0', '22.011_1', '-8.2027_0', '-8.2027_1', '40.092_0', '40.092_1', '81.8828_0', '81.8828_1']\n",
      "12\n",
      "(16068, 31)\n",
      "F1 Score: [0.51201281 0.05088266]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(960, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(8123, 23)\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['x_b_0', 'x_f_1', 'x_k_0', 'x_s_0', 'x_x_0', 's_f_0', 's_f_1', 's_g_0', 's_g_1', 's_y_0', 's_y_1', 'n_c_0', 'n_c_1', 'n_e_0', 'n_e_1', 'n_g_1', 'n_n_0', 'n_n_1', 'n_p_0', 'n_w_1', 'n_y_1', 't_f_1', 't_t_0', 'p.1_a_0', 'p.1_a_1', 'p.1_c_0', 'p.1_c_1', 'p.1_f_0', 'p.1_l_1', 'p.1_m_0', 'p.1_m_1', 'p.1_n_0', 'p.1_p_1', 'p.1_s_0', 'p.1_y_0', 'p.1_y_1', 'f_a_1', 'c_c_1', 'n.1_b_0', 'n.1_b_1', 'k_e_0', 'k_g_1', 'k_h_0', 'k_p_0', 'k_p_1', 'k_r_0', 'k_w_0', 'k_y_0', 'k_y_1', 'e_e_0', 'e_t_0', 'e.1_?_0', 'e.1_b_0', 'e.1_b_1', 'e.1_c_0', 'e.1_c_1', 'e.1_e_0', 'e.1_e_1', 'e.1_r_0', 'e.1_r_1', 's.1_f_0', 's.1_f_1', 's.1_k_1', 's.1_y_1', 's.2_s_1', 'w_b_0', 'w_b_1', 'w_c_0', 'w_e_0', 'w_e_1', 'w_g_0', 'w_g_1', 'w_n_0', 'w_o_1', 'w_p_1', 'w_w_0', 'w_y_0', 'w.1_b_0', 'w.1_b_1', 'w.1_n_0', 'w.1_o_0', 'w.1_o_1', 'w.1_p_1', 'w.1_w_1', 'w.1_y_1', 'p.2_p_0', 'w.2_o_0', 'w.2_o_1', 'w.2_w_0', 'w.2_w_1', 'w.2_y_1', 'o_n_0', 'o_o_0', 'o_o_1', 'o_t_0', 'p.3_e_0', 'p.3_e_1', 'p.3_l_1', 'p.3_n_1', 'p.3_p_0', 'k.1_b_0', 'k.1_k_0', 'k.1_n_0', 'k.1_n_1', 'k.1_y_1', 's.3_a_0', 's.3_a_1', 's.3_c_1', 's.3_n_1', 's.3_s_0', 's.3_s_1', 's.3_y_0', 's.3_y_1', 'u_g_1', 'u_l_0', 'u_l_1', 'u_m_0', 'u_m_1', 'u_p_0', 'u_p_1', 'u_u_1']\n",
      "121\n",
      "(6905, 352)\n",
      "F1 Score: [0.54899416 0.22346369]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3822, 65)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2533, 74)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2535, 74)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(195, 24)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(7493, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(181, 13)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(25009, 11)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1054, 42)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(209, 8)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(245056, 4)\n",
      "F1 Score normal: [0.99515571 0.99810794]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['74_0', '74_1', '85_0', '85_1', '123_0', '123_1']\n",
      "6\n",
      "(43727, 10)\n",
      "F1 Score: [0.35596708 0.84997603]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(46, 36)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4600, 58)\n",
      "F1 Score normal: [0.95844156 0.93495935]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "['0.64.1_0', '0.64.1_1', '0.1_0', '0.1_1', '0.32_0', '0.32_1', '0.2_0', '0.3_0', '0.4_1', '0.5_0', '0.5_1', '0.6_1', '0.7_0', '0.64.2_0', '0.12_1', '0.18_0', '0.20_0', '0.26_0', '0.28_0', '0.29_1', '0.33_1', '0.34_0', '0.35_0', '0.36_0', '0.39_0', '0.39_1', '0.778_0']\n",
      "27\n",
      "(3578, 172)\n",
      "F1 Score: [0.82568807 0.61025641]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 23)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 45)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(999, 21)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4434, 37)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(150, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3771, 22)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(214, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3162, 26)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(957, 10)\n",
      "F1 Score normal: [0.98412698 0.96969697]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "[]\n",
      "0\n",
      "(813, 82)\n",
      "F1 Score: [0.98412698 0.96969697]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 3)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 25)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4025, 1)\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(177, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1483, 10)\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares de RF y threshold de 20%\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Threshold of correlation with target for new columns\n",
    "new_cols_threshold = 0.2\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]   \n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       if (len(lines[x].split(\" = \")) == 1):\n",
    "                           pass\n",
    "                       # If the separator is specified    \n",
    "                       else:\n",
    "                           separator = lines[x].split(\" = \")[1]\n",
    "                           separator = separator.strip()\n",
    "                           if (\"comma\" in separator):\n",
    "                               separator = \",\"\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           target_index = lines[x].split(\" = \")[1]\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               # If there is not header specified, assume\n",
    "                               # default value\n",
    "                               if (len(lines[x].split(\" = \")) == 1):\n",
    "                                    pass\n",
    "                               else:\n",
    "                                   # If it is specified\n",
    "                                   header = lines[x].split(\" = \")[1]\n",
    "                                   # If it is 0 assign it to None\n",
    "                                   if (header.startswith(\"0\")):\n",
    "                                       header = None\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python')\n",
    "                    print(data.shape)\n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = data.columns.tolist()[int(target_index) - 1]\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [n for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "                                        aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        # aux_model = LinearRegression()\n",
    "                                        aux_model.fit(X_train, y_train)\n",
    "                                        dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared = 1\n",
    "                                        else:    \n",
    "                                            r_squared = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "                                        \n",
    "                                        dictionary_aux_r_squared[fict_target] = r_squared\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "                            \n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "\n",
    "                            #####\n",
    "                            # Calculate correlation with the target variable\n",
    "                            correlation = result_df.corr()[target_variable].abs()\n",
    "                            # Select columns whose names end with '_0' or '_1' and have a at least certain absolute correlation with the target\n",
    "                            selected_columns = correlation[(correlation.index.str.endswith('_0') | correlation.index.str.endswith('_1')) & (correlation >= new_cols_threshold)].index\n",
    "                            selected_columns = selected_columns.tolist()\n",
    "                            print(selected_columns)\n",
    "                            print(len(selected_columns))\n",
    "                            print(result_df.shape)\n",
    "                            \n",
    "                            result_df = result_df[data.columns.tolist() + selected_columns]\n",
    "                            #####\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                                        X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                        y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                        y_real = data_test[fict_target]  \n",
    "                                \n",
    "                                        mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                                        rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                        weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                        # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "\n",
    "                                        rmse = [1e5 if (x > 1e5) else x for x in rmse]\n",
    "                                \n",
    "                                        # Add column to list of new columns\n",
    "                                        new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new2 = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                for u in dictionary_case.keys():\n",
    "                                    if ((u + '_' + str(case)) in selected_columns):    \n",
    "                                        names_cols_dataframe_new2.append(u + \"_\" + str(case))\n",
    "                            dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            # Adapt order of columns to the order in which the model was\n",
    "                            # trained\n",
    "                            data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to read the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4effe9-6fc5-4658-bbf3-6415694126ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4ead5-ecc7-4fd8-8b05-ed510835bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebas realizando un feature importance de un modelo generado solo con las\n",
    "# variables nuevas, y extrayendo el 10% de las variables nuevas con mayor \n",
    "# feature importance para añadir al dataset. Esta estrategia por lo general\n",
    "# parece poco efectiva, a diferencia de la selección por correlación con la\n",
    "# target, donde a veces (en el caso de los modelos lineales) sí se apreciaba\n",
    "# una ventaja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6472f66a-7b11-4d0a-9d07-4256d1418a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(32560, 15)\n",
      "F1 Score normal: [0.67607004 0.9135514 ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.52893519 0.89865538]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(624, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(15, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1371, 5)\n",
      "F1 Score normal: [0.98689956 0.98285714]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.98689956 0.98285714]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(748, 5)\n",
      "F1 Score normal: [0.87218045 0.37037037]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.87218045 0.37037037]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(285, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(568, 32)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(698, 11)\n",
      "F1 Score normal: [0.97222222 0.9375    ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.96551724 0.92063492]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(197, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1727, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(28055, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3195, 37)\n",
      "F1 Score normal: [0.98181818 0.98056156]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.94780793 0.94780793]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(540, 461)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1079, 857)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(434, 17)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(989, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(207, 61)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1472, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(689, 16)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(365, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(10000, 14)\n",
      "F1 Score normal: [0.99947562 0.99908509]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99947562 0.99908509]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(99, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(213, 11)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(305, 4)\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.75757576 0.2       ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(131, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(302, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(293, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(154, 20)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(299, 28)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(582, 11)\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.79710145 0.125     ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(350, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(149, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19999, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(359, 91)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(31, 57)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19019, 11)\n",
      "F1 Score normal: [0.80639823 0.90904379]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.66424682 0.78662053]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(960, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(8123, 23)\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.55581948 0.00531915]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3822, 65)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2533, 74)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2535, 74)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(195, 24)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(7493, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(181, 13)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(25009, 11)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1054, 42)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(209, 8)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(245056, 4)\n",
      "F1 Score normal: [0.99515571 0.99810794]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99515571 0.99810794]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(46, 36)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4600, 58)\n",
      "F1 Score normal: [0.95844156 0.93495935]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.89242054 0.8018018 ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 23)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 45)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(999, 21)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4434, 37)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(150, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3771, 22)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(214, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3162, 26)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(957, 10)\n",
      "F1 Score normal: [0.98412698 0.96969697]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.78481013 0.        ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 3)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 25)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4025, 1)\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(177, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1483, 10)\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares lineales\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Threshold of correlation with target for new columns\n",
    "new_cols_threshold = 0.2\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]   \n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       if (len(lines[x].split(\" = \")) == 1):\n",
    "                           pass\n",
    "                       # If the separator is specified    \n",
    "                       else:\n",
    "                           separator = lines[x].split(\" = \")[1]\n",
    "                           separator = separator.strip()\n",
    "                           if (\"comma\" in separator):\n",
    "                               separator = \",\"\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           target_index = lines[x].split(\" = \")[1]\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               # If there is not header specified, assume\n",
    "                               # default value\n",
    "                               if (len(lines[x].split(\" = \")) == 1):\n",
    "                                    pass\n",
    "                               else:\n",
    "                                   # If it is specified\n",
    "                                   header = lines[x].split(\" = \")[1]\n",
    "                                   # If it is 0 assign it to None\n",
    "                                   if (header.startswith(\"0\")):\n",
    "                                       header = None\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python')\n",
    "                    print(data.shape)\n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = data.columns.tolist()[int(target_index) - 1]\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [n for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "                                        # aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        aux_model = LinearRegression()\n",
    "                                        aux_model.fit(X_train, y_train)\n",
    "                                        dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared = 1\n",
    "                                        else:    \n",
    "                                            r_squared = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "                                        \n",
    "                                        dictionary_aux_r_squared[fict_target] = r_squared\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "\n",
    "                            #####################\n",
    "                            ####################\n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            aux_df = pd.concat([dataframe_new, data[[target_variable]]], axis=1, ignore_index=True)\n",
    "                            aux_df.columns = cols_b + [target_variable]\n",
    "                            \n",
    "                            features = aux_df.drop(target_variable, axis=1)\n",
    "                            target = aux_df[target_variable]\n",
    "                            rf_model_aux = RandomForestClassifier(n_estimators=200, random_state=42, min_samples_split=10, min_samples_leaf=5)\n",
    "                            rf_model_aux.fit(features, target)\n",
    "                            \n",
    "                            feature_importances = rf_model_aux.feature_importances_\n",
    "                            \n",
    "                            # Create a list of (feature_name, importance) tuples\n",
    "                            feature_importance_tuples = zip(features.columns, feature_importances)\n",
    "                                                        \n",
    "                            # Sort the tuples based on importance\n",
    "                            sorted_feature_importance_tuples = sorted(feature_importance_tuples, key=lambda x: x[1], reverse=True)\n",
    "                            \n",
    "                            # Extract sorted feature names\n",
    "                            sorted_feature_names = [feature_name for feature_name, _ in sorted_feature_importance_tuples]\n",
    "\n",
    "                            # Select some features\n",
    "                            selected_columns = sorted_feature_names[0: int(len(sorted_feature_names)/10)]\n",
    "                            \n",
    "                            ####################\n",
    "                            ####################\n",
    "\n",
    "                            dataframe_new = dataframe_new.loc[:, selected_columns]\n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "\n",
    "                            #####\n",
    "                            # Calculate correlation with the target variable\n",
    "                            # correlation = result_df.corr()[target_variable].abs()\n",
    "                            # Select columns whose names end with '_0' or '_1' and have a at least certain absolute correlation with the target\n",
    "                            # selected_columns = correlation[(correlation.index.str.endswith('_0') | correlation.index.str.endswith('_1')) & (correlation >= new_cols_threshold)].index\n",
    "                            # selected_columns = selected_columns.tolist()\n",
    "                            # print(selected_columns)\n",
    "                            # print(len(selected_columns))\n",
    "                            # print(result_df.shape)\n",
    "                            \n",
    "                            # result_df = result_df[data.columns.tolist() + selected_columns]\n",
    "                            #####\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                                        X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                        y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                        y_real = data_test[fict_target]  \n",
    "                                \n",
    "                                        mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                                        rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                        weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                        # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "\n",
    "                                        rmse = [1e5 if (x > 1e5) else x for x in rmse]\n",
    "                                \n",
    "                                        # Add column to list of new columns\n",
    "                                        new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new2 = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                for u in dictionary_case.keys():\n",
    "                                    if ((u + '_' + str(case)) in selected_columns):    \n",
    "                                        names_cols_dataframe_new2.append(u + \"_\" + str(case))\n",
    "                            dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            # Adapt order of columns to the order in which the model was\n",
    "                            # trained\n",
    "                            data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to read the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c99e2664-2d11-4b88-a3e5-1b6d46c42791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(32560, 15)\n",
      "F1 Score normal: [0.67607004 0.9135514 ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.58933195 0.89870992]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(624, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(15, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1371, 5)\n",
      "F1 Score normal: [0.98689956 0.98285714]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.98689956 0.98285714]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(748, 5)\n",
      "F1 Score normal: [0.87218045 0.37037037]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.87218045 0.37037037]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(285, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(568, 32)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(698, 11)\n",
      "F1 Score normal: [0.97222222 0.9375    ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.89393939 0.81578947]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(197, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1727, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(28055, 7)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3195, 37)\n",
      "F1 Score normal: [0.98181818 0.98056156]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.96537678 0.96359743]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(540, 461)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1079, 857)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(434, 17)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(989, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(207, 61)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1472, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(689, 16)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(365, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(10000, 14)\n",
      "F1 Score normal: [0.99947562 0.99908509]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99947617 0.99908341]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(99, 10)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(213, 11)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(305, 4)\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.75757576 0.2       ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(131, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(302, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(293, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(154, 20)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(606, 101)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(299, 28)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(582, 11)\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.80851064 0.06896552]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(350, 35)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(149, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19999, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(359, 91)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(31, 57)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(19019, 11)\n",
      "F1 Score normal: [0.80639823 0.90904379]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.56926258 0.54873646]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(960, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(431, 8)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(8123, 23)\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.62805663 0.3446712 ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3822, 65)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2533, 74)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(2535, 74)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(195, 24)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(7493, 17)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(181, 13)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(25009, 11)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1054, 42)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(209, 8)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(245056, 4)\n",
      "F1 Score normal: [0.99515571 0.99810794]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99515571 0.99810794]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(46, 36)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4600, 58)\n",
      "F1 Score normal: [0.95844156 0.93495935]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.85175879 0.74678112]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 23)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(79, 45)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(999, 21)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4434, 37)\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(150, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3771, 22)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(214, 6)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(3162, 26)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(957, 10)\n",
      "F1 Score normal: [0.98412698 0.96969697]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.97894737 0.95918367]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 3)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 25)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(5455, 5)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(4025, 1)\n",
      "It was not possible to read the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(177, 14)\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data\n",
      "...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "(1483, 10)\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares de RF\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Threshold of correlation with target for new columns\n",
    "new_cols_threshold = 0.2\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]   \n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       if (len(lines[x].split(\" = \")) == 1):\n",
    "                           pass\n",
    "                       # If the separator is specified    \n",
    "                       else:\n",
    "                           separator = lines[x].split(\" = \")[1]\n",
    "                           separator = separator.strip()\n",
    "                           if (\"comma\" in separator):\n",
    "                               separator = \",\"\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           target_index = lines[x].split(\" = \")[1]\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               # If there is not header specified, assume\n",
    "                               # default value\n",
    "                               if (len(lines[x].split(\" = \")) == 1):\n",
    "                                    pass\n",
    "                               else:\n",
    "                                   # If it is specified\n",
    "                                   header = lines[x].split(\" = \")[1]\n",
    "                                   # If it is 0 assign it to None\n",
    "                                   if (header.startswith(\"0\")):\n",
    "                                       header = None\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python')\n",
    "                    print(data.shape)\n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = data.columns.tolist()[int(target_index) - 1]\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [n for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "                                        aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        # aux_model = LinearRegression()\n",
    "                                        aux_model.fit(X_train, y_train)\n",
    "                                        dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared = 1\n",
    "                                        else:    \n",
    "                                            r_squared = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "                                        \n",
    "                                        dictionary_aux_r_squared[fict_target] = r_squared\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "\n",
    "                            #####################\n",
    "                            ####################\n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            aux_df = pd.concat([dataframe_new, data[[target_variable]]], axis=1, ignore_index=True)\n",
    "                            aux_df.columns = cols_b + [target_variable]\n",
    "                            \n",
    "                            features = aux_df.drop(target_variable, axis=1)\n",
    "                            target = aux_df[target_variable]\n",
    "                            rf_model_aux = RandomForestClassifier(n_estimators=200, random_state=42, min_samples_split=10, min_samples_leaf=5)\n",
    "                            rf_model_aux.fit(features, target)\n",
    "                            \n",
    "                            feature_importances = rf_model_aux.feature_importances_\n",
    "                            \n",
    "                            # Create a list of (feature_name, importance) tuples\n",
    "                            feature_importance_tuples = zip(features.columns, feature_importances)\n",
    "                                                        \n",
    "                            # Sort the tuples based on importance\n",
    "                            sorted_feature_importance_tuples = sorted(feature_importance_tuples, key=lambda x: x[1], reverse=True)\n",
    "                            \n",
    "                            # Extract sorted feature names\n",
    "                            sorted_feature_names = [feature_name for feature_name, _ in sorted_feature_importance_tuples]\n",
    "\n",
    "                            # Select some features\n",
    "                            selected_columns = sorted_feature_names[0: int(len(sorted_feature_names)/10)]\n",
    "                            ####################\n",
    "                            ####################\n",
    "\n",
    "                            dataframe_new = dataframe_new.loc[:, selected_columns]\n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "\n",
    "                            #####\n",
    "                            # Calculate correlation with the target variable\n",
    "                            # correlation = result_df.corr()[target_variable].abs()\n",
    "                            # Select columns whose names end with '_0' or '_1' and have a at least certain absolute correlation with the target\n",
    "                            # selected_columns = correlation[(correlation.index.str.endswith('_0') | correlation.index.str.endswith('_1')) & (correlation >= new_cols_threshold)].index\n",
    "                            # selected_columns = selected_columns.tolist()\n",
    "                            # print(selected_columns)\n",
    "                            # print(len(selected_columns))\n",
    "                            # print(result_df.shape)\n",
    "                            \n",
    "                            # result_df = result_df[data.columns.tolist() + selected_columns]\n",
    "                            #####\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                                        X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                        y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                        y_real = data_test[fict_target]  \n",
    "                                \n",
    "                                        mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                                        rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                        weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                        # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "\n",
    "                                        rmse = [1e5 if (x > 1e5) else x for x in rmse]\n",
    "                                \n",
    "                                        # Add column to list of new columns\n",
    "                                        new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new2 = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                for u in dictionary_case.keys():\n",
    "                                    if ((u + '_' + str(case)) in selected_columns):    \n",
    "                                        names_cols_dataframe_new2.append(u + \"_\" + str(case))\n",
    "                            dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            # Adapt order of columns to the order in which the model was\n",
    "                            # trained\n",
    "                            data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to read the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3becf-d295-41ce-89b2-a3a5b1e463e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b57e3f-a162-48fe-a962-6e17f1efbdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de nuevas columnas en base a la importancia de las variables\n",
    "# del modelo normal. En concreto, se toman como base el 10% de las variables\n",
    "# con más importancia en el modelo normal y luego se seleccionan para cada\n",
    "# una de ellas la variable nueva terminada en \"_0\" y la terminada en \"_1\".\n",
    "\n",
    "# Al menos usando modelos auxiliares lineales, se observa algunos casos donde\n",
    "# mejora, como en el breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d4467a-85b9-49af-8dc8-71d983cbf786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.69234427 0.91552422]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.64977192 0.91128515]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.98591549 0.98429319]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.98591549 0.98429319]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.78688525 0.31578947]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.78688525 0.31578947]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.94890511 0.90140845]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.95652174 0.91428571]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.98231827 0.97995546]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.98624754 0.9844098 ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [1. 1.]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.75757576 0.2       ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.75968992 0.24390244]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.81151832 0.90430622]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.78669276 0.87982359]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.02520252 0.1838734 ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.99607843 0.99846833]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99607843 0.99846833]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.94626474 0.91816367]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.89815951 0.81514477]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.95145631 0.87804878]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.98989899 0.97777778]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares lineales\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header_option = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]  \n",
    "                   data_url = data_url.strip()\n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       try:\n",
    "                           if (len(lines[x].split(\" = \")) == 1):\n",
    "                               pass\n",
    "                           # If the separator is specified    \n",
    "                           else:\n",
    "                               separator = lines[x].split(\" = \")[1]\n",
    "                               separator = separator.strip()\n",
    "                               if (\"comma\" in separator):\n",
    "                                   separator = \",\"\n",
    "                       except:\n",
    "                           pass\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           try:\n",
    "                               target_index = lines[x].split(\" = \")[1]\n",
    "                               target_index = int(target_index.strip())\n",
    "                           except:\n",
    "                               pass\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               try:\n",
    "                                   # If there is not header specified, assume\n",
    "                                   # default value\n",
    "                                   if (len(lines[x].split(\" = \")) == 1):\n",
    "                                        pass\n",
    "                                   else:\n",
    "                                       # If it is specified\n",
    "                                       header_option = lines[x].split(\" = \")[1]\n",
    "                                       header_option = int(header_option.strip())\n",
    "                                       # If it is 0 assign it to None\n",
    "                                       if (str(header_option).startswith(\"0\")):\n",
    "                                           header_option = None\n",
    "                               except:\n",
    "                                   pass\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python', header=header_option)\n",
    "                    aux_names_columns = [str(aux_n_c) for aux_n_c in data.columns]\n",
    "                    data.columns = aux_names_columns\n",
    "                    \n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = str(data.columns.tolist()[int(target_index) - 1])\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [str(n) for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "\n",
    "                            #######################################\n",
    "                            # Calculations for later selection of new columns\n",
    "                            feature_importances = rf_normal.feature_importances_\n",
    "                            # Create a list of (feature_name, importance) tuples\n",
    "                            feature_importance_tuples = zip(X_train.columns, feature_importances)\n",
    "                            # Sort the tuples based on importance\n",
    "                            sorted_feature_importance_tuples = sorted(feature_importance_tuples, key=lambda x: x[1], reverse=True)\n",
    "                            # Extract sorted feature names\n",
    "                            sorted_feature_names = [feature_name for feature_name, _ in sorted_feature_importance_tuples]\n",
    "                            # Select some features\n",
    "                            base_selected_columns = sorted_feature_names[0: int(len(sorted_feature_names)/10)]\n",
    "                            #######################################\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "                                        # aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        aux_model = LinearRegression()\n",
    "                                        aux_model.fit(X_train, y_train)\n",
    "                                        dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared = 1\n",
    "                                        else:    \n",
    "                                            r_squared = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "                                        \n",
    "                                        dictionary_aux_r_squared[fict_target] = r_squared\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "\n",
    "                            #####################\n",
    "                            ####################\n",
    "                            \n",
    "                            # Select some features\n",
    "                            selected_columns = [] \n",
    "                            for aux_base_cols in base_selected_columns:\n",
    "                                selected_columns.append(aux_base_cols + \"_0\")\n",
    "                                selected_columns.append(aux_base_cols + \"_1\")\n",
    "                            \n",
    "                            ####################\n",
    "                            ####################\n",
    "\n",
    "                            dataframe_new = dataframe_new.loc[:, selected_columns]\n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "\n",
    "                            #####\n",
    "                            # Calculate correlation with the target variable\n",
    "                            # correlation = result_df.corr()[target_variable].abs()\n",
    "                            # Select columns whose names end with '_0' or '_1' and have a at least certain absolute correlation with the target\n",
    "                            # selected_columns = correlation[(correlation.index.str.endswith('_0') | correlation.index.str.endswith('_1')) & (correlation >= new_cols_threshold)].index\n",
    "                            # selected_columns = selected_columns.tolist()\n",
    "                            # print(selected_columns)\n",
    "                            # print(len(selected_columns))\n",
    "                            # print(result_df.shape)\n",
    "                            \n",
    "                            # result_df = result_df[data.columns.tolist() + selected_columns]\n",
    "                            #####\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                                        X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                        y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                        y_real = data_test[fict_target]  \n",
    "                                \n",
    "                                        mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                                        rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                        weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                        # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "\n",
    "                                        rmse = [1e5 if (x > 1e5) else x for x in rmse]\n",
    "                                \n",
    "                                        # Add column to list of new columns\n",
    "                                        new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new2 = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                for u in dictionary_case.keys():\n",
    "                                    if ((u + '_' + str(case)) in selected_columns):    \n",
    "                                        names_cols_dataframe_new2.append(u + \"_\" + str(case))\n",
    "                            dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            # Adapt order of columns to the order in which the model was\n",
    "                            # trained\n",
    "                            data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to process the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3377461e-bcd2-4878-b2ce-5d288a649836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.69234427 0.91552422]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.67910751 0.89159929]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.98591549 0.98429319]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.98591549 0.98429319]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.78688525 0.31578947]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.78688525 0.31578947]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.94890511 0.90140845]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.95652174 0.91428571]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.98231827 0.97995546]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.98245614 0.97977528]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [1. 1.]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.75757576 0.2       ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.76335878 0.20512821]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.81151832 0.90430622]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.68487085 0.7116813 ]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99831366 0.99840256]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.99607843 0.99846833]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.99607843 0.99846833]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.94626474 0.91816367]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.89901478 0.81858407]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.95145631 0.87804878]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.97029703 0.93023256]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares RF\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header_option = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]  \n",
    "                   data_url = data_url.strip()\n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       try:\n",
    "                           if (len(lines[x].split(\" = \")) == 1):\n",
    "                               pass\n",
    "                           # If the separator is specified    \n",
    "                           else:\n",
    "                               separator = lines[x].split(\" = \")[1]\n",
    "                               separator = separator.strip()\n",
    "                               if (\"comma\" in separator):\n",
    "                                   separator = \",\"\n",
    "                       except:\n",
    "                           pass\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           try:\n",
    "                               target_index = lines[x].split(\" = \")[1]\n",
    "                               target_index = int(target_index.strip())\n",
    "                           except:\n",
    "                               pass\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               try:\n",
    "                                   # If there is not header specified, assume\n",
    "                                   # default value\n",
    "                                   if (len(lines[x].split(\" = \")) == 1):\n",
    "                                        pass\n",
    "                                   else:\n",
    "                                       # If it is specified\n",
    "                                       header_option = lines[x].split(\" = \")[1]\n",
    "                                       header_option = int(header_option.strip())\n",
    "                                       # If it is 0 assign it to None\n",
    "                                       if (str(header_option).startswith(\"0\")):\n",
    "                                           header_option = None\n",
    "                               except:\n",
    "                                   pass\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python', header=header_option)\n",
    "                    aux_names_columns = [str(aux_n_c) for aux_n_c in data.columns]\n",
    "                    data.columns = aux_names_columns\n",
    "                    \n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = str(data.columns.tolist()[int(target_index) - 1])\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [str(n) for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "\n",
    "                            #######################################\n",
    "                            # Calculations for later selection of new columns\n",
    "                            feature_importances = rf_normal.feature_importances_\n",
    "                            # Create a list of (feature_name, importance) tuples\n",
    "                            feature_importance_tuples = zip(X_train.columns, feature_importances)\n",
    "                            # Sort the tuples based on importance\n",
    "                            sorted_feature_importance_tuples = sorted(feature_importance_tuples, key=lambda x: x[1], reverse=True)\n",
    "                            # Extract sorted feature names\n",
    "                            sorted_feature_names = [feature_name for feature_name, _ in sorted_feature_importance_tuples]\n",
    "                            # Select some features\n",
    "                            base_selected_columns = sorted_feature_names[0: int(len(sorted_feature_names)/10)]\n",
    "                            #######################################\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "                                        aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        # aux_model = LinearRegression()\n",
    "                                        aux_model.fit(X_train, y_train)\n",
    "                                        dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared = 1\n",
    "                                        else:    \n",
    "                                            r_squared = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "                                        \n",
    "                                        dictionary_aux_r_squared[fict_target] = r_squared\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "\n",
    "                            #####################\n",
    "                            ####################\n",
    "                            \n",
    "                            # Select some features\n",
    "                            selected_columns = [] \n",
    "                            for aux_base_cols in base_selected_columns:\n",
    "                                selected_columns.append(aux_base_cols + \"_0\")\n",
    "                                selected_columns.append(aux_base_cols + \"_1\")\n",
    "                            \n",
    "                            ####################\n",
    "                            ####################\n",
    "\n",
    "                            dataframe_new = dataframe_new.loc[:, selected_columns]\n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "\n",
    "                            #####\n",
    "                            # Calculate correlation with the target variable\n",
    "                            # correlation = result_df.corr()[target_variable].abs()\n",
    "                            # Select columns whose names end with '_0' or '_1' and have a at least certain absolute correlation with the target\n",
    "                            # selected_columns = correlation[(correlation.index.str.endswith('_0') | correlation.index.str.endswith('_1')) & (correlation >= new_cols_threshold)].index\n",
    "                            # selected_columns = selected_columns.tolist()\n",
    "                            # print(selected_columns)\n",
    "                            # print(len(selected_columns))\n",
    "                            # print(result_df.shape)\n",
    "                            \n",
    "                            # result_df = result_df[data.columns.tolist() + selected_columns]\n",
    "                            #####\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                                        X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                        y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                        y_real = data_test[fict_target]  \n",
    "                                \n",
    "                                        mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                                        rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                        weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                        # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "\n",
    "                                        rmse = [1e5 if (x > 1e5) else x for x in rmse]\n",
    "                                \n",
    "                                        # Add column to list of new columns\n",
    "                                        new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new2 = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                for u in dictionary_case.keys():\n",
    "                                    if ((u + '_' + str(case)) in selected_columns):    \n",
    "                                        names_cols_dataframe_new2.append(u + \"_\" + str(case))\n",
    "                            dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            # Adapt order of columns to the order in which the model was\n",
    "                            # trained\n",
    "                            data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to process the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c90061-f96a-4580-8672-502fcf0611e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original but choosing for each auxiliary model\n",
    "# the best performance (linear vs RF). It seems\n",
    "# that this giving in general poor results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d9beaec-ed8b-4945-91e4-99184bb6aaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.69234427 0.91552422]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.60213904 0.90572732]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.98591549 0.98429319]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.59482759 0.45348837]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.78688525 0.31578947]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.78688525 0.31578947]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.94890511 0.90140845]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.91729323 0.85333333]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.98231827 0.97995546]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.84513274 0.86166008]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00233/CNAE-9.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel-context.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [1. 1.]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00244/fertility_Diagnosis.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.75757576 0.2       ]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.71875    0.18181818]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hayes-roth/hayes-roth.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_with_noise_Training.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.76119403 0.11111111]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.79432624 0.        ]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.81151832 0.90430622]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.5330911  0.01099505]\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-3.test...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [1. 1.]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [1. 1.]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00230/plrx.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.99607843 0.99846833]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.41528993 0.85381064]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.94626474 0.91816367]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.80672269 0.62645012]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECTF.train...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.trn.Z...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allbp.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allhyper.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/allrep.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/ann-train.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/dis.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/new-thyroid.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "It was not possible to process the downloaded dataset\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/sick-euthyroid.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "F1 Score normal: [0.95145631 0.87804878]\n",
      "0\n",
      "#########################################\n",
      "1\n",
      "#########################################\n",
      "F1 Score: [0.81327801 0.04255319]\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_2.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_24.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00194/sensor_readings_4.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00273/Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n"
     ]
    }
   ],
   "source": [
    "# Usando modelos auxiliares RF\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header_option = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]  \n",
    "                   data_url = data_url.strip()\n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       try:\n",
    "                           if (len(lines[x].split(\" = \")) == 1):\n",
    "                               pass\n",
    "                           # If the separator is specified    \n",
    "                           else:\n",
    "                               separator = lines[x].split(\" = \")[1]\n",
    "                               separator = separator.strip()\n",
    "                               if (\"comma\" in separator):\n",
    "                                   separator = \",\"\n",
    "                       except:\n",
    "                           pass\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           try:\n",
    "                               target_index = lines[x].split(\" = \")[1]\n",
    "                               target_index = int(target_index.strip())\n",
    "                           except:\n",
    "                               pass\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               try:\n",
    "                                   # If there is not header specified, assume\n",
    "                                   # default value\n",
    "                                   if (len(lines[x].split(\" = \")) == 1):\n",
    "                                        pass\n",
    "                                   else:\n",
    "                                       # If it is specified\n",
    "                                       header_option = lines[x].split(\" = \")[1]\n",
    "                                       header_option = int(header_option.strip())\n",
    "                                       # If it is 0 assign it to None\n",
    "                                       if (str(header_option).startswith(\"0\")):\n",
    "                                           header_option = None\n",
    "                               except:\n",
    "                                   pass\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                try:\n",
    "                    data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python', header=header_option)\n",
    "                    aux_names_columns = [str(aux_n_c) for aux_n_c in data.columns]\n",
    "                    data.columns = aux_names_columns\n",
    "                    \n",
    "                    # If there will be enough dimensionality after one hot encoding\n",
    "                    data_check = pd.get_dummies(data)\n",
    "                    if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                        # Name of the target of the dataset (target_index - 1 since \n",
    "                        # in python first position is 0)\n",
    "                        target_variable = str(data.columns.tolist()[int(target_index) - 1])\n",
    "        \n",
    "                        # If it is a binary classification\n",
    "                        unique_values_count = data[target_variable].nunique()\n",
    "                        if (unique_values_count == 2):\n",
    "            \n",
    "                            # -------------------------------------------------------------------------------------\n",
    "                            data = data.drop_duplicates()\n",
    "                            \n",
    "                            # Handling missing values (drop rows with missing values for simplicity)\n",
    "                            data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "                            # Encoding categorical variables using one-hot encoding (OHE)\n",
    "                            data = pd.get_dummies(data)\n",
    "        \n",
    "                            # Obtain new name of the target (after OHE) and discard the other option\n",
    "                            aux_names_target = [str(n) for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "                            # If the target was subjected to OHE\n",
    "                            if (len(aux_names_target) != 0):\n",
    "                                # Obtain new name of the target variable\n",
    "                                target_variable = aux_names_target[0]\n",
    "                                # Discard other columns with other values of the target variable, since\n",
    "                                # including them would artificially yield high performance\n",
    "                                for g in range(1, len(aux_names_target)):    \n",
    "                                    data = data.drop(columns=[aux_names_target[g]])\n",
    "                            \n",
    "                            \n",
    "                            # Normalizing variables:\n",
    "                            data_columns = data.columns\n",
    "                            scaler = StandardScaler()\n",
    "                            data = scaler.fit_transform(data)\n",
    "                            data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "                            # Denormalize target values (these must be 0 or 1):\n",
    "                            def aux_denormalize_target(aux):\n",
    "                                threshold = min(list(data[target_variable].unique()))\n",
    "                                if aux > threshold:\n",
    "                                    return 1\n",
    "                                else:\n",
    "                                    return 0\n",
    "                            \n",
    "                            data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "                            \n",
    "                            # Shuffle the DataFrame to randomize the rows\n",
    "                            data = data.sample(frac=1, random_state=12)  \n",
    "                            \n",
    "                            \n",
    "                            # Save some registers for testing performance:\n",
    "                            data_test = data.sample(frac=0.15, random_state=42)\n",
    "                            data = data.drop(data_test.index)\n",
    "                            \n",
    "                            #############################\n",
    "                            # Normal performance\n",
    "                            X_train = data.drop(columns=[target_variable])\n",
    "                            y_train = data[target_variable] \n",
    "                            X_test = data_test.drop(columns=[target_variable])\n",
    "                            y_test = data_test[target_variable]\n",
    "                            \n",
    "                            rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_normal.fit(X_train, y_train)\n",
    "                            \n",
    "                            predictions_normal = rf_normal.predict(X_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1_normal = f1_score(y_test, predictions_normal, average=None)\n",
    "                            print(f'F1 Score normal: {f1_normal}')\n",
    "\n",
    "                            #######################################\n",
    "                            # Calculations for later selection of new columns\n",
    "                            feature_importances = rf_normal.feature_importances_\n",
    "                            # Create a list of (feature_name, importance) tuples\n",
    "                            feature_importance_tuples = zip(X_train.columns, feature_importances)\n",
    "                            # Sort the tuples based on importance\n",
    "                            sorted_feature_importance_tuples = sorted(feature_importance_tuples, key=lambda x: x[1], reverse=True)\n",
    "                            # Extract sorted feature names\n",
    "                            sorted_feature_names = [feature_name for feature_name, _ in sorted_feature_importance_tuples]\n",
    "                            # Select some features\n",
    "                            base_selected_columns = sorted_feature_names[0: int(len(sorted_feature_names)/10)]\n",
    "                            #######################################\n",
    "                            \n",
    "                            \n",
    "                            #############################\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # List of dictionaries\n",
    "                            list_of_dictionaries = []\n",
    "                            \n",
    "                            # List of dictionaries of R²\n",
    "                            list_of_dictionaries_r_squared = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                print(target_value)\n",
    "                                print(\"#########################################\")\n",
    "                            \n",
    "                                # Generate auxiliary dataset\n",
    "                                dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "                                # Discard target in auxiliary dataset\n",
    "                                dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "                                # Generate dictionary of ficticious targets and the models that predict them:\n",
    "                                dictionary_aux = {}\n",
    "                                # Correspondant dictionary of rmse for weighing \n",
    "                                dictionary_aux_r_squared = {}\n",
    "                                \n",
    "                                for fict_target in dataset_aux.columns.tolist():\n",
    "                                    # print(fict_target)\n",
    "                                    \n",
    "                                    # Train auxiliary model and save it\n",
    "                                    X = dataset_aux.drop(columns=[fict_target])\n",
    "                                    y = dataset_aux[fict_target] \n",
    "                                    \n",
    "                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                            \n",
    "                                    \n",
    "                                    # Fit the auxiliary model:\n",
    "                                    if True:\n",
    "\n",
    "                                        # First try RF\n",
    "                                        aux_model_RF = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                                        aux_model_RF.fit(X_train, y_train)\n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model_RF.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared_RF = 1\n",
    "                                        else:    \n",
    "                                            r_squared_RF = 1 - (rss / tss)    \n",
    "\n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "\n",
    "  \n",
    "                                        # Now try linear regression\n",
    "                                        aux_model_linear = LinearRegression()\n",
    "                                        aux_model_linear.fit(X_train, y_train)\n",
    "                                        \n",
    "                                        \n",
    "                                        #####\n",
    "                                        # Computation of R²* for weighing:\n",
    "                                        # (*)Actually, it is a variation of R² so that the values are\n",
    "                                        # in the range [0, 1] negative R² values will be converted to 0,\n",
    "                                        # so it is not really R²\n",
    "                                        predictions = aux_model_linear.predict(X_test)\n",
    "                                        y_mean = np.mean(y_test)\n",
    "                                        # Calculate the total sum of squares\n",
    "                                        tss = np.sum((y_test - y_mean) ** 2)\n",
    "                                        # Calculate the residual sum of squares\n",
    "                                        rss = np.sum((y_test - predictions) ** 2)\n",
    "                                        # Calculate R² score\n",
    "                                        # If tss == 0 then R² will be 1\n",
    "                                        if (tss < 0.00001) & (tss > -0.00001):\n",
    "                                            r_squared_linear  = 1\n",
    "                                        else:    \n",
    "                                            r_squared_linear = 1 - (rss / tss)    \n",
    "                                        # Apply modification\n",
    "                                        # if (r_squared < 0):\n",
    "                                        #    r_squared = 0\n",
    "                                        # print(r_squared)    \n",
    "\n",
    "                                        if (r_squared_RF > r_squared_linear):\n",
    "                                            dictionary_aux[fict_target] = aux_model_RF\n",
    "                                            dictionary_aux_r_squared[fict_target] = r_squared_RF\n",
    "                                        else:\n",
    "                                            dictionary_aux[fict_target] = aux_model_linear\n",
    "                                            dictionary_aux_r_squared[fict_target] = r_squared_linear\n",
    "                                    \n",
    "                                list_of_dictionaries.append(dictionary_aux)    \n",
    "                                list_of_dictionaries_r_squared.append(dictionary_aux_r_squared)    \n",
    "                            \n",
    "                            list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "                            \n",
    "                            list_of_rows_dataframe_new = []\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            new_columns = []\n",
    "                            \n",
    "                            # For each value of the target\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    X = data.drop(columns=[target_variable, fict_target])\n",
    "                                    y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                    y_real = data[fict_target]  \n",
    "                            \n",
    "                                    mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "                                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                    weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                    # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "                            \n",
    "                                    # Add column to list of new columns\n",
    "                                    new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new = []\n",
    "                            for u in dictionary_case.keys():\n",
    "                                    names_cols_dataframe_new.append(u + \"_0\")\n",
    "                                    names_cols_dataframe_new.append(u + \"_1\")        \n",
    "                            dataframe_new.columns = names_cols_dataframe_new\n",
    "\n",
    "                            #####################\n",
    "                            ####################\n",
    "                            \n",
    "                            # Select some features\n",
    "                            selected_columns = [] \n",
    "                            for aux_base_cols in base_selected_columns:\n",
    "                                selected_columns.append(aux_base_cols + \"_0\")\n",
    "                                selected_columns.append(aux_base_cols + \"_1\")\n",
    "                            \n",
    "                            ####################\n",
    "                            ####################\n",
    "\n",
    "                            # dataframe_new = dataframe_new.loc[:, selected_columns]\n",
    "                            \n",
    "                            cols_a = data.columns.to_list()\n",
    "                            cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "                            data = data.reset_index(drop=True)\n",
    "                            dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "                            result_df.columns = cols_b + cols_a\n",
    "\n",
    "                            #####\n",
    "                            # Calculate correlation with the target variable\n",
    "                            # correlation = result_df.corr()[target_variable].abs()\n",
    "                            # Select columns whose names end with '_0' or '_1' and have a at least certain absolute correlation with the target\n",
    "                            # selected_columns = correlation[(correlation.index.str.endswith('_0') | correlation.index.str.endswith('_1')) & (correlation >= new_cols_threshold)].index\n",
    "                            # selected_columns = selected_columns.tolist()\n",
    "                            # print(selected_columns)\n",
    "                            # print(len(selected_columns))\n",
    "                            # print(result_df.shape)\n",
    "                            \n",
    "                            # result_df = result_df[data.columns.tolist() + selected_columns]\n",
    "                            #####\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            # The process has generated additional columns in the dataframe (those ending with _0 or _1).\n",
    "                            # These additional columns could enhance potentially performance.\n",
    "                            # The whole cycle may be repeated again (sort of a new layer) generating more additional\n",
    "                            # variables (these will contain also those now ending with _0_0, _0_1, _1_0, and 1_1).\n",
    "                            \n",
    "                            \n",
    "                            # Train model\n",
    "                            features = result_df.drop(target_variable, axis=1)\n",
    "                            target = result_df[target_variable]\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5)\n",
    "                            rf_model.fit(features, target)\n",
    "                            \n",
    "                            \n",
    "                            #######################################################################\n",
    "                            # Now process the test dataset so that the model can be applied to it\n",
    "                            # For each value of the target\n",
    "                            new_columns = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "                                dictionary_case = list_of_dictionaries[case]\n",
    "                                dictionary_case_r_squared = list_of_dictionaries_r_squared[case]\n",
    "                                    \n",
    "                                for fict_target in dictionary_case:\n",
    "                                    \n",
    "                                    # if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    if True:\n",
    "                                    \n",
    "                                        X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                                        y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                                        y_real = data_test[fict_target]  \n",
    "                                \n",
    "                                        mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                                        rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                        weighing_value = dictionary_case_r_squared[fict_target]\n",
    "                                        # rmse = [float(rmse_value * (weighing_value**2)) for rmse_value in rmse]\n",
    "\n",
    "                                        rmse = [1e5 if (x > 1e5) else x for x in rmse]\n",
    "                                \n",
    "                                        # Add column to list of new columns\n",
    "                                        new_columns.append(rmse)\n",
    "                            \n",
    "                            dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "                            names_cols_dataframe_new2 = []\n",
    "                            for case in range(0, len(list_unique_values_target)):\n",
    "                                for u in dictionary_case.keys():\n",
    "                                    # if ((u + '_' + str(case)) in selected_columns):    \n",
    "                                    names_cols_dataframe_new2.append(u + \"_\" + str(case))\n",
    "                            dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "                            cols_a = data_test.columns.to_list()\n",
    "                            cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "                            data_test = data_test.reset_index(drop=True)\n",
    "                            dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                            \n",
    "                            \n",
    "                            # Concatenate horizontally\n",
    "                            data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "                            data_test_processed.columns = cols_b + cols_a\n",
    "                            # Adapt order of columns to the order in which the model was\n",
    "                            # trained\n",
    "                            data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                            #######################################################################\n",
    "                            \n",
    "                            # Now apply trained model on test dataset to gauge performance\n",
    "                            features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "                            target_test = data_test_processed[target_variable]\n",
    "                            predictions = rf_model.predict(features_test)\n",
    "                            \n",
    "                            # Calculate F1 score for each class\n",
    "                            f1 = f1_score(target_test, predictions, average=None)\n",
    "                            print(f'F1 Score: {f1}')\n",
    "                            # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "                except:\n",
    "                    print(\"It was not possible to process the downloaded dataset\")\n",
    "    \n",
    "\n",
    "            \n",
    "            # except:\n",
    "            else:\n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                      \n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c9bbcf-53c8-4b15-8d2b-1f21be5a58a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing multiple hyperparameters (type of auxiliary model, selection of new variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6983d055-bab8-4486-9829-da034ee21354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column with the fewest 1 values: D\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    'A': [0, 1, 0, 1, 0],\n",
    "    'B': [1, 1, 1, 1, 0],\n",
    "    'C': [1, 0, 0, 1, 0],\n",
    "    'D': [0, 0, 0, 0, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Count the number of 1 values for each column\n",
    "count_ones = df.sum()\n",
    "\n",
    "# Find the name of the column with the fewest 1 values\n",
    "column_with_fewest_ones = count_ones.idxmin()\n",
    "\n",
    "print(\"Column with the fewest 1 values:\", column_with_fewest_ones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e81a4-3ed4-4f32-8f32-748796883e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import statistics\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "def aux_func(data, target_variable, only_normal=False, type_aux_mod=\"linear\", selection_method=None, \n",
    "             frac_feature_imp_normal=0.1, new_cols_corr_thr=0.2, seed=123):\n",
    "    \n",
    "    #################################\n",
    "    # PREPROCESSING\n",
    "    data = data.drop_duplicates()\n",
    "                            \n",
    "    # Handling missing values (drop rows with missing values for simplicity)\n",
    "    data.dropna(inplace=True)\n",
    "              \n",
    "                            \n",
    "    # Encoding categorical variables using one-hot encoding (OHE)\n",
    "    data = pd.get_dummies(data)\n",
    "        \n",
    "    # Obtain new name of the target (after OHE) and discard the other option\n",
    "    aux_names_target = [str(n) for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "    # If the target was subjected to OHE\n",
    "    if (len(aux_names_target) != 0):\n",
    "        # Obtain new name of the target variable (it will be the one with fewer 1 values)\n",
    "        # Sum the 1 values for each column\n",
    "        count_ones = data.loc[:, aux_names_target].sum()\n",
    "        # Find the name of the column with the fewest 1 values\n",
    "        target_variable = count_ones.idxmin()\n",
    "        \n",
    "        # Discard other columns with other values of the target variable, since\n",
    "        # including them would artificially yield high performance\n",
    "        for g in aux_names_target:    \n",
    "            if (g != target_variable):\n",
    "                data = data.drop(columns=[g])\n",
    "                            \n",
    "    # Normalizing variables:\n",
    "    data_columns = data.columns\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(data, columns=data_columns)\n",
    "                            \n",
    "                            \n",
    "    # Denormalize target values (these must be 0 or 1):\n",
    "    def aux_denormalize_target(aux):\n",
    "        threshold = min(list(data[target_variable].unique()))\n",
    "        if aux > threshold:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "                            \n",
    "    data[target_variable] = data[target_variable].apply(aux_denormalize_target)\n",
    "                            \n",
    "    # Shuffle the DataFrame to randomize the rows\n",
    "    data = data.sample(frac=1, random_state=seed)  \n",
    "                            \n",
    "    # Save some registers for testing performance:\n",
    "    data_test = data.sample(frac=0.15, random_state=42)\n",
    "    data = data.drop(data_test.index)\n",
    "     \n",
    "    #################################\n",
    "    # NORMAL MODEL\n",
    "\n",
    "    # If the function only returns the normal performance or the selection\n",
    "    # method of new variables is feature_imp_normal\n",
    "    if ((only_normal) | (selection_method is not None)):\n",
    "\n",
    "        X_normal = data.drop(columns=[target_variable])\n",
    "        y_normal = data[target_variable] \n",
    "\n",
    "        # Define RandomForestClassifier\n",
    "        rf_normal = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "        \n",
    "        # Define the parameter grid for grid search\n",
    "        min_samples_split_grid_search = [5, 10, 15]\n",
    "        min_samples_leaf_grid_search = [5, 10, 15]\n",
    "        param_grid = {\n",
    "            'min_samples_split': min_samples_split_grid_search,\n",
    "            'min_samples_leaf' : min_samples_leaf_grid_search\n",
    "        }\n",
    "        \n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(estimator=rf_normal, param_grid=param_grid, scoring='f1', cv=5, n_jobs=-1)\n",
    "        grid_search.fit(X_normal, y_normal)\n",
    "        \n",
    "        # Get the best normal model and corresponding score\n",
    "        rf_normal = grid_search.best_estimator_\n",
    "\n",
    "        # If the call to the function was to just calculate the\n",
    "        # normal performance\n",
    "        if (only_normal):\n",
    "            features_test_normal = data_test.drop(target_variable, axis=1)\n",
    "            target_test_normal = data_test[target_variable]\n",
    "            predictions_normal = rf_normal.predict(features_test_normal)\n",
    "            normal_score = f1_score(target_test_normal, predictions_normal)\n",
    "                           \n",
    "            return (normal_score)\n",
    "    \n",
    "    \n",
    "    #######################################\n",
    "        \n",
    "                                                \n",
    "    #################################\n",
    "    # GENERATION OF AUXILIARY MODELS\n",
    "        \n",
    "    # List of dictionaries\n",
    "    list_of_dictionaries = []\n",
    "                            \n",
    "    # For each value of the target\n",
    "    for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                \n",
    "        # Generate auxiliary dataset\n",
    "        dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "        # Discard target in auxiliary dataset\n",
    "        dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "        # Generate dictionary of ficticious targets and the models that predict them:\n",
    "        dictionary_aux = {}\n",
    "                                \n",
    "        for fict_target in dataset_aux.columns.tolist():\n",
    "        \n",
    "            # Train auxiliary model and save it\n",
    "            X = dataset_aux.drop(columns=[fict_target])\n",
    "            y = dataset_aux[fict_target] \n",
    "                                \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "            if (type_aux_mod == \"linear\"):\n",
    "                aux_model = LinearRegression(n_jobs=-1)\n",
    "            else:\n",
    "                if (type_aux_mod == \"randomforest\"):\n",
    "                    aux_model = RandomForestRegressor(n_estimators=200, random_state=42, min_samples_split=5, min_samples_leaf=5, n_jobs=-1)\n",
    "                else:\n",
    "                    print(\"Error: current allowed type for auxiliary models is 'linear' or 'randomforest'\")\n",
    "                    sys.exit()\n",
    "            \n",
    "            aux_model.fit(X_train, y_train)\n",
    "                                        \n",
    "            dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                    \n",
    "        list_of_dictionaries.append(dictionary_aux)    \n",
    "            \n",
    "\n",
    "    #################################\n",
    "    # GENERATION OF NEW COLUMNS BASED ON\n",
    "    # AUXILIARY MODELS\n",
    "        \n",
    "    list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "    list_of_rows_dataframe_new = []\n",
    "                          \n",
    "    new_columns = []\n",
    "\n",
    "    # For each value of the target\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "                              \n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "                                    \n",
    "        for fict_target in dictionary_case:\n",
    "                                    \n",
    "            X = data.drop(columns=[target_variable, fict_target])\n",
    "            y_predicted = dictionary_case[fict_target].predict(X)\n",
    "            y_real = data[fict_target]  \n",
    "                            \n",
    "            mse = (y_real - y_predicted) ** 2\n",
    "                            \n",
    "            rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                          \n",
    "            # Add column to list of new columns\n",
    "            new_columns.append(rmse)\n",
    "\n",
    "        \n",
    "    dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "    names_cols_dataframe_new = []\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "        for u in dictionary_case.keys():\n",
    "            names_cols_dataframe_new.append(u + \"_\" + str(case))\n",
    "    dataframe_new.columns = names_cols_dataframe_new\n",
    "\n",
    "    cols_a = data.columns.to_list()\n",
    "    cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "    data = data.reset_index(drop=True)\n",
    "    dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                           \n",
    "    # Concatenate horizontally\n",
    "    result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "    result_df.columns = cols_b + cols_a    \n",
    "                            \n",
    "    #################################\n",
    "    # SELECTION OF NEW COLUMNS CREATED TO CREATE\n",
    "    # AUGMENTED DF.\n",
    "\n",
    "    if (selection_method == \"feature_imp_normal\"):\n",
    "        # Calculations for later selection of new columns\n",
    "        feature_importances = rf_normal.feature_importances_\n",
    "        # Create a list of (feature_name, importance) tuples\n",
    "        feature_importance_tuples = zip(X_normal.columns, feature_importances)\n",
    "        # Sort the tuples based on importance\n",
    "        sorted_feature_importance_tuples = sorted(feature_importance_tuples, key=lambda x: x[1], reverse=True)\n",
    "        # Extract sorted feature names\n",
    "        sorted_feature_names = [feature_name for feature_name, _ in sorted_feature_importance_tuples]\n",
    "        # Select some features\n",
    "        base_selected_columns = sorted_feature_names[0: int(len(sorted_feature_names) * frac_feature_imp_normal)]\n",
    "        \n",
    "        selected_columns = [] \n",
    "        for aux_base_cols in base_selected_columns:\n",
    "            selected_columns.append(aux_base_cols + \"_0\")\n",
    "            selected_columns.append(aux_base_cols + \"_1\")\n",
    "        \n",
    "    else:          \n",
    "        if (selection_method == \"correlation_target\"):\n",
    "            # Obtain new variables that have at least certain absolute correlation with the target\n",
    "            correlation = result_df.corr()[target_variable].abs()\n",
    "            # Select columns that have a at least certain absolute correlation with the target\n",
    "            selected_columns = correlation[correlation >= new_cols_corr_thr].index\n",
    "            selected_columns = [w for w in selected_columns if (w in dataframe_new.columns)]\n",
    "            \n",
    "        else:\n",
    "            if (selection_method is None):\n",
    "                # Select all created new columns\n",
    "                selected_columns = dataframe_new.columns.tolist()            \n",
    "                \n",
    "            else:\n",
    "                print(\"Error. The parameter selection_method must be 'feature_imp_normal', 'correlation_target' or None\")\n",
    "                sys.exit()\n",
    "                    \n",
    "\n",
    "    # The augmented dataframe will contain the original columns plus the \n",
    "    # selected new columns\n",
    "    result_df = result_df.loc[:, data.columns.tolist() + selected_columns]\n",
    "\n",
    "                            \n",
    "\n",
    "    #################################\n",
    "    # TRAINING OF MODEL BASED ON AUGMENTED DF.\n",
    "\n",
    "    features = result_df.drop(target_variable, axis=1)\n",
    "    target = result_df[target_variable]\n",
    "    \n",
    "    rf_model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "        \n",
    "    # Define the parameter grid for grid search\n",
    "    min_samples_split_grid_search = [5, 10, 15]\n",
    "    min_samples_leaf_grid_search = [5, 10, 15]\n",
    "    param_grid = {\n",
    "        'min_samples_split': min_samples_split_grid_search,\n",
    "        'min_samples_leaf' : min_samples_leaf_grid_search\n",
    "    }\n",
    "        \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, scoring='f1', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(features, target)\n",
    "        \n",
    "    # Get the best model and corresponding score\n",
    "    rf_model = grid_search.best_estimator_\n",
    "                            \n",
    "\n",
    "    #################################\n",
    "    # PROCESSING OF TEST DATASET SO THAT THE TRAINED MODEL\n",
    "    # CAN BE APPLIED TO IT, AND IN THIS WAY OBTAIN \n",
    "    # PERFORMANCE METRICS\n",
    "    \n",
    "    # For each value of the target\n",
    "    new_columns = []\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "        \n",
    "        for fict_target in dictionary_case:\n",
    "            if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                y_real = data_test[fict_target]  \n",
    "                               \n",
    "                mse = (y_real - y_predicted) ** 2\n",
    "                                \n",
    "                rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                                \n",
    "                # Add column to list of new columns\n",
    "                new_columns.append(rmse)\n",
    "                            \n",
    "    dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "    names_cols_dataframe_new2 = []\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "        for u in dictionary_case.keys():\n",
    "            if ((u + '_' + str(case)) in selected_columns):    \n",
    "                names_cols_dataframe_new2.append(u + \"_\" + str(case))\n",
    "    dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "    cols_a = data_test.columns.to_list()\n",
    "    cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "    data_test = data_test.reset_index(drop=True)\n",
    "    dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                                                    \n",
    "    # Concatenate horizontally\n",
    "    data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "    data_test_processed.columns = cols_b + cols_a\n",
    "    # Adapt order of columns to the order in which the model was\n",
    "    # trained\n",
    "    data_test_processed = data_test_processed[result_df.columns.to_list()]\n",
    "                         \n",
    "    # Now apply trained model on test dataset to gauge performance\n",
    "    features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "    target_test = data_test_processed[target_variable]\n",
    "    predictions = rf_model.predict(features_test)\n",
    "                           \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(target_test, predictions)\n",
    "    return (f1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "\n",
    "# TEST ON DATAFRAMES FROM UCI, DOWNLOAD ADAPTED FROM Perales-González, Carlos, (2020). UCI download-process, v1.3, GitHub repository, https://github.com/cperales/uci-download-process\n",
    "\n",
    "# Number of statistical repetitions\n",
    "num_stat_rep = 10\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "\n",
    "for i in files_classification:\n",
    "    try:\n",
    "\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header_option = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]  \n",
    "                   data_url = data_url.strip()\n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       try:\n",
    "                           if (len(lines[x].split(\" = \")) == 1):\n",
    "                               pass\n",
    "                           # If the separator is specified    \n",
    "                           else:\n",
    "                               separator = lines[x].split(\" = \")[1]\n",
    "                               separator = separator.strip()\n",
    "                               if (\"comma\" in separator):\n",
    "                                   separator = \",\"\n",
    "                       except:\n",
    "                           pass\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           try:\n",
    "                               target_index = lines[x].split(\" = \")[1]\n",
    "                               target_index = int(target_index.strip())\n",
    "                           except:\n",
    "                               pass\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               try:\n",
    "                                   # If there is not header specified, assume\n",
    "                                   # default value\n",
    "                                   if (len(lines[x].split(\" = \")) == 1):\n",
    "                                        pass\n",
    "                                   else:\n",
    "                                       # If it is specified\n",
    "                                       header_option = lines[x].split(\" = \")[1]\n",
    "                                       header_option = int(header_option.strip())\n",
    "                                       # If it is 0 assign it to None\n",
    "                                       if (str(header_option).startswith(\"0\")):\n",
    "                                           header_option = None\n",
    "                               except:\n",
    "                                   pass\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            # try:\n",
    "            if True:\n",
    "                data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python', header=header_option)\n",
    "                aux_names_columns = [str(aux_n_c) for aux_n_c in data.columns]\n",
    "                data.columns = aux_names_columns\n",
    "                    \n",
    "                # If there will be enough dimensionality after one hot encoding\n",
    "                data_check = pd.get_dummies(data)\n",
    "                if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                    # Name of the target of the dataset (target_index - 1 since \n",
    "                    # in python first position is 0)\n",
    "                    target_variable = str(data.columns.tolist()[int(target_index) - 1])\n",
    "        \n",
    "                    # If it is a binary classification\n",
    "                    unique_values_count = data[target_variable].nunique()\n",
    "                    if (unique_values_count == 2):\n",
    "                        \n",
    "                        normal_performance = []\n",
    "                        for c in range(0, num_stat_rep):\n",
    "                            normal_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                           only_normal=True, seed=c))\n",
    "                        \n",
    "                        normal_performance = statistics.mean(normal_performance)\n",
    "                        print(\"Normal performance: \" + str(round(normal_performance, 3)))\n",
    "                        \n",
    "                        cases_new = [\"type_aux_mode=linear; selection_method_case=None\"]\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=None\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7\")\n",
    "                        cases_new.append(\"type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1\")\n",
    "                        cases_new.append(\"type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2\")\n",
    "                        cases_new.append(\"type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1\")\n",
    "                        cases_new.append(\"type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2\")\n",
    "\n",
    "                        cases_new_performances = []\n",
    "                        cases_new_performances_stdev = []\n",
    "                        \n",
    "                        \n",
    "                        for case in cases_new:\n",
    "                            \n",
    "                            if (\"linear\" in case):\n",
    "                                model_aux = \"linear\"\n",
    "                            else:\n",
    "                                model_aux = \"randomforest\"\n",
    "\n",
    "                            if (\"None\" in case):\n",
    "                                new_performance = []\n",
    "                                for c in range(0, num_stat_rep):                            \n",
    "                                    new_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                                   type_aux_mod=model_aux, selection_method=None, seed=c))\n",
    "                                cases_new_performances_stdev.append(statistics.stdev(new_performance))\n",
    "                                new_performance = statistics.mean(new_performance)        \n",
    "                                cases_new_performances.append(new_performance)\n",
    "                                \n",
    "                            else:     \n",
    "                                if (\"feature_imp_normal\" in case):\n",
    "                                    frac_feature_imp_normal_value = float(case.split(\"frac_feature_imp_normal=\")[1])\n",
    "                                    new_performance = []\n",
    "                                    for c in range(0, num_stat_rep):         \n",
    "                                        new_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                                      type_aux_mod=model_aux, selection_method=\"feature_imp_normal\", \n",
    "                                                                      frac_feature_imp_normal=frac_feature_imp_normal_value, seed=c))\n",
    "                                    cases_new_performances_stdev.append(statistics.stdev(new_performance))    \n",
    "                                    new_performance = statistics.mean(new_performance)        \n",
    "                                    cases_new_performances.append(new_performance)\n",
    "                                else:\n",
    "                                    new_cols_corr_thr_value = float(case.split(\"new_cols_corr_thr=\")[1])\n",
    "                                    new_performance = []\n",
    "                                    for c in range(0, num_stat_rep):         \n",
    "                                        new_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                                     type_aux_mod=model_aux, selection_method=\"correlation_target\", \n",
    "                                                                  new_cols_corr_thr=new_cols_corr_thr_value, seed=c))\n",
    "                                    cases_new_performances_stdev.append(statistics.stdev(new_performance))    \n",
    "                                    new_performance = statistics.mean(new_performance)        \n",
    "                                    cases_new_performances.append(new_performance)\n",
    "                            \n",
    "                            print(case)\n",
    "                            print(new_performance)\n",
    "\n",
    "                        # Save result\n",
    "                        result = pd.DataFrame({\"Case\": cases_new, \"Mean F1-score\": cases_new_performances, \"Stdev F1-score\": cases_new_performances_stdev})\n",
    "                        result = result.sort_values(by=\"Mean F1-score\", ascending=False)\n",
    "                        data_name = data_url.split(\"/\")\n",
    "                        data_name = data_name[len(data_name) - 1]\n",
    "                        display(result)\n",
    "                        result.to_csv(\"metrics_\" + data_name + \".csv\", index=False)\n",
    "                        \n",
    "                        # Get maximum performance of the new cases\n",
    "                        if (max(cases_new_performances) > normal_performance):\n",
    "                            \n",
    "                            print(\"New performance: \" + str(round(max(cases_new_performances), 3)))\n",
    "                            # Print details\n",
    "                            max_index = cases_new_performances.index(max(cases_new_performances))    \n",
    "                            print(\"Optimum case new: \" + str(cases_new[max_index]))\n",
    "                        else:\n",
    "                            print(\"No improvement found\")                        \n",
    "                                \n",
    "                        \n",
    "            \n",
    "            # except:\n",
    "            else:    \n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{i}' does not exist.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e16a99-a2ea-4fa7-b1a4-18905a1580bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
