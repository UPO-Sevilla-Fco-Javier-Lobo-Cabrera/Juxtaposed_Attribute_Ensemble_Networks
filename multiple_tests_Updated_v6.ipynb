{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a4b725ff-57d2-49a4-a284-912298d6117c",
   "metadata": {},
   "source": [
    "# Testing multiple hyperparameters (type of auxiliary model, selection of new variables).\n",
    "\n",
    "Implementation Example\n",
    "\n",
    "Let's walk through an example where you want to exclude certain features from the imbalance handling.\n",
    "Step-by-Step Implementation\n",
    "\n",
    "    Prepare Data: Have your features and target variables ready.\n",
    "    Split Features: Separate the features to be influenced by class weights and those not to be influenced.\n",
    "    Define Submodels: Create two submodels.\n",
    "    Combine and Compile: Combine the outputs of the submodels and compile the model with a custom loss function.\n",
    "\n",
    "Code Example\n",
    "\n",
    "python\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "\n",
    "# Example data preparation\n",
    "import numpy as np\n",
    "X_train = np.random.rand(1000, 10)  # 1000 samples, 10 features\n",
    "y_train = np.random.randint(0, 2, 1000)  # Binary target\n",
    "\n",
    "# Assume feature_x is the 0th feature and feature_y is the 1st feature\n",
    "feature_x = X_train[:, 0]\n",
    "feature_y = X_train[:, 1]\n",
    "# Splitting the features\n",
    "features_to_weight = X_train[:, :5]  # First 5 features are subject to class weighting\n",
    "features_not_to_weight = X_train[:, 5:]  # Last 5 features are not subject to class weighting\n",
    "\n",
    "# Define the model inputs\n",
    "input_weighted = Input(shape=(5,), name='weighted_input')\n",
    "input_not_weighted = Input(shape=(5,), name='not_weighted_input')\n",
    "\n",
    "# Define the submodel for weighted features\n",
    "x = Dense(64, activation='relu')(input_weighted)\n",
    "\n",
    "# Define the submodel for non-weighted features\n",
    "y = Dense(64, activation='relu')(input_not_weighted)\n",
    "\n",
    "# Concatenate the outputs of both submodels\n",
    "combined = Concatenate()([x, y])\n",
    "output = Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=[input_weighted, input_not_weighted], outputs=output)\n",
    "\n",
    "# Define the custom loss function with class weighting\n",
    "def custom_loss(y_true, y_pred):\n",
    "    class_weights = tf.where(tf.equal(y_true, 1), 2.0, 1.0)  # Example weights\n",
    "    base_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    weighted_loss = base_loss * class_weights\n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# Compile the model with the custom loss function\n",
    "model.compile(optimizer='adam', loss=custom_loss)\n",
    "\n",
    "# Fit the model\n",
    "model.fit({'weighted_input': features_to_weight, 'not_weighted_input': features_not_to_weight}, y_train, epochs=10)\n",
    "\n",
    "Explanation\n",
    "\n",
    "    Data Preparation:\n",
    "        X_train is split into features_to_weight and features_not_to_weight.\n",
    "\n",
    "    Model Definition:\n",
    "        Two input layers are defined, one for each set of features.\n",
    "        Two submodels are created, one for the features subject to class weighting and another for those that are not.\n",
    "        The outputs of the two submodels are concatenated and passed through a final output layer.\n",
    "\n",
    "    Custom Loss Function:\n",
    "        A custom loss function custom_loss applies class weights only to the overall prediction.\n",
    "        Class weights are defined using tf.where to give higher weight to the minority class.\n",
    "\n",
    "    Training:\n",
    "        The model is trained with the features appropriately split, and the custom loss function ensures class weighting is applied correctly.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "By separating the features into two submodels and using a custom loss function, you can effectively apply class weighting to handle target imbalance while excluding certain features from this influence. This approach provides the flexibility to fine-tune how class imbalance is managed within your neural network model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a50d3-f333-4040-aaa0-bc680475f801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small+adult-stretch.data...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Downloading file from http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt...\n",
      "File downloaded successfully to dataset_file_aux.txt.\n",
      "Normal performance: 0.992 +- 0.0077\n",
      "type_aux_mode=linear; selection_method_case=None; shap_treatment=False; sqrt_error=False\n",
      "0.997 +- 0.0035\n",
      "type_aux_mode=linear; selection_method_case=None; shap_treatment=True; sqrt_error=False\n",
      "0.9975 +- 0.0035\n",
      "type_aux_mode=randomforest; selection_method_case=None; shap_treatment=False; sqrt_error=False\n",
      "0.999 +- 0.0021\n",
      "type_aux_mode=randomforest; selection_method_case=None; shap_treatment=True; sqrt_error=False\n",
      "0.9995 +- 0.0016\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1; shap_treatment=False; sqrt_error=False\n",
      "0.991 +- 0.0071\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1; shap_treatment=True; sqrt_error=False\n",
      "0.996 +- 0.0057\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3; shap_treatment=False; sqrt_error=False\n",
      "0.9975 +- 0.0035\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3; shap_treatment=True; sqrt_error=False\n",
      "0.999 +- 0.0021\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5; shap_treatment=False; sqrt_error=False\n",
      "0.997 +- 0.0035\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5; shap_treatment=True; sqrt_error=False\n",
      "0.9975 +- 0.0035\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7; shap_treatment=False; sqrt_error=False\n",
      "0.997 +- 0.0035\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7; shap_treatment=True; sqrt_error=False\n",
      "0.9975 +- 0.0035\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1; shap_treatment=False; sqrt_error=False\n",
      "0.991 +- 0.0071\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1; shap_treatment=True; sqrt_error=False\n",
      "0.996 +- 0.0057\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3; shap_treatment=False; sqrt_error=False\n",
      "0.995 +- 0.0053\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3; shap_treatment=True; sqrt_error=False\n",
      "0.998 +- 0.0036\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5; shap_treatment=False; sqrt_error=False\n",
      "0.998 +- 0.0036\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5; shap_treatment=True; sqrt_error=False\n",
      "0.999 +- 0.0021\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7; shap_treatment=False; sqrt_error=False\n",
      "0.998 +- 0.0036\n",
      "type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7; shap_treatment=True; sqrt_error=False\n",
      "0.999 +- 0.0021\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1; shap_treatment=False; sqrt_error=False\n",
      "0.997 +- 0.0035\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1; shap_treatment=True; sqrt_error=False\n",
      "0.9975 +- 0.0035\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2; shap_treatment=False; sqrt_error=False\n",
      "0.997 +- 0.0035\n",
      "type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2; shap_treatment=True; sqrt_error=False\n",
      "0.997 +- 0.0035\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1; shap_treatment=False; sqrt_error=False\n",
      "0.999 +- 0.0021\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1; shap_treatment=True; sqrt_error=False\n",
      "0.9995 +- 0.0016\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2; shap_treatment=False; sqrt_error=False\n",
      "0.999 +- 0.0021\n",
      "type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2; shap_treatment=True; sqrt_error=False\n",
      "0.9995 +- 0.0016\n",
      "type_aux_mode=linear; selection_method_case=None; shap_treatment=True; sqrt_error=True\n",
      "0.9975 +- 0.0035\n",
      "type_aux_mode=randomforest; selection_method_case=None; shap_treatment=False; sqrt_error=True\n",
      "0.999 +- 0.0021\n",
      "type_aux_mode=randomforest; selection_method_case=None; shap_treatment=True; sqrt_error=True\n",
      "0.9995 +- 0.0016\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1; shap_treatment=False; sqrt_error=True\n",
      "0.991 +- 0.0071\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1; shap_treatment=True; sqrt_error=True\n",
      "0.996 +- 0.0057\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3; shap_treatment=False; sqrt_error=True\n",
      "0.9975 +- 0.0035\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3; shap_treatment=True; sqrt_error=True\n",
      "0.999 +- 0.0021\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5; shap_treatment=False; sqrt_error=True\n",
      "0.997 +- 0.0035\n",
      "type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5; shap_treatment=True; sqrt_error=True\n",
      "0.9975 +- 0.0035\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "import statistics\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def return_files(directory):\n",
    "    \"\"\"\n",
    "    Returns all files in the specified directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    return (files)\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"\n",
    "    Download a file from the specified URL to the specified destination.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        print(f\"File downloaded successfully to {destination}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def calculate_significant_features_mi(df, target, threshold=0.7):\n",
    "    significant_features = []\n",
    "\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    # Calculate mutual information using mutual_info_classif since the target is binary\n",
    "    mi = mutual_info_classif(X, y)\n",
    "\n",
    "    # Normalize MI values by the maximum MI value\n",
    "    max_mi = np.max(mi)\n",
    "    normalized_mi = mi / max_mi\n",
    "\n",
    "    # Select features with normalized MI above the threshold\n",
    "    for feature, norm_mi in zip(X.columns, normalized_mi):\n",
    "        if norm_mi >= threshold:\n",
    "            significant_features.append(feature)\n",
    "\n",
    "    return significant_features\n",
    "    \n",
    "\n",
    "def aux_func(data, target_variable, only_normal=False, type_aux_mod=\"linear\", selection_method=None, \n",
    "             frac_feature_imp_normal=0.1, new_cols_corr_thr=0.2,\n",
    "             shap_treatment=False, sqrt_error=False, seed=123):\n",
    "    \n",
    "    #################################\n",
    "    # PREPROCESSING\n",
    "    data = data.drop_duplicates()\n",
    "                            \n",
    "    # Handling missing values (drop rows with missing values for simplicity)\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    # Encoding categorical variables using one-hot encoding (OHE).\n",
    "    # First convert the target into categorical. This facilitates later\n",
    "    # calculations\n",
    "    data[target_variable] = data[target_variable].astype(\"category\")\n",
    "    # Now apply OHE to categorical columns (including the target)\n",
    "    data = pd.get_dummies(data)\n",
    "\n",
    "    # After OHE there will be two columns created for the target (since it was binary).\n",
    "    # Choose one of those columns as target and discard the other one. It is not important\n",
    "    # which of the two is selected as target, since the F1-score performance is measured\n",
    "    # using 'macro' average option\n",
    "    aux_names_target = [str(n) for n in data.columns.tolist() if (n.startswith(target_variable + \"_\"))]\n",
    "    target_variable = aux_names_target[0]\n",
    "    data = data.drop(columns=[aux_names_target[1]])\n",
    "                            \n",
    "    # Normalizing numerical variables. Since previously OHE\n",
    "    # was performed, all variables with more than 2 different\n",
    "    # values will be numerical\n",
    "    numerical_columns = []\n",
    "    non_numerical_columns = []\n",
    "    for aux_num in data.columns:\n",
    "        unique_values_count = data[aux_num].nunique()\n",
    "        if (unique_values_count > 2):\n",
    "            numerical_columns.append(aux_num)\n",
    "        else:\n",
    "            non_numerical_columns.append(aux_num)\n",
    "    # Normalization\n",
    "    if (len(numerical_columns) != 0):\n",
    "        scaler = StandardScaler()\n",
    "        aux_data = scaler.fit_transform(data[numerical_columns])\n",
    "        aux_data = pd.DataFrame(aux_data, columns=numerical_columns)\n",
    "        aux_data = aux_data.reset_index(drop=True)\n",
    "        data_non_numerical = data[non_numerical_columns].reset_index(drop=True)\n",
    "        data = pd.concat([aux_data, data_non_numerical], axis=1)\n",
    "                                                                                \n",
    "\n",
    "    # Shuffle the DataFrame to randomize the rows\n",
    "    data = data.sample(frac=1, random_state=seed)  \n",
    "                            \n",
    "    # Save some registers for testing performance:\n",
    "    data_test = data.sample(frac=0.15, random_state=42)\n",
    "    data = data.drop(data_test.index)\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_counts = data[target_variable].value_counts()\n",
    "    total_samples = class_counts.sum()\n",
    "    class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "\n",
    "    # Define custom scorer\n",
    "    average_metric_type = 'macro'\n",
    "    custom_scorer = make_scorer(f1_score, average=average_metric_type)\n",
    "\n",
    "    #################################\n",
    "    # NORMAL MODEL\n",
    "\n",
    "    # If the function only returns the normal performance or the selection\n",
    "    # method of new variables is feature_imp_normal\n",
    "    if ((only_normal) | (selection_method is not None)):\n",
    "\n",
    "        X_normal = data.drop(columns=[target_variable])\n",
    "        y_normal = data[target_variable] \n",
    "\n",
    "        # Define RandomForestClassifier\n",
    "        rf_normal = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        \n",
    "        # Define the parameter grid for grid search\n",
    "        min_samples_split_grid_search = [5, 10, 15]\n",
    "        min_samples_leaf_grid_search = [5, 10]\n",
    "        param_grid = {\n",
    "            'min_samples_split': min_samples_split_grid_search,\n",
    "            'min_samples_leaf' : min_samples_leaf_grid_search\n",
    "        }\n",
    "        \n",
    "        # Perform grid search        \n",
    "        grid_search = GridSearchCV(estimator=rf_normal, param_grid=param_grid,\n",
    "                                   scoring=custom_scorer, cv=5, n_jobs=-1)\n",
    "        # The normal model takes imbalance into account to generate \n",
    "        # maximum normal performance\n",
    "        grid_search.fit(X_normal, y_normal, sample_weight=y_normal.map(class_weights))\n",
    "        \n",
    "        # Get the best normal model\n",
    "        rf_normal = grid_search.best_estimator_\n",
    "\n",
    "        # If the call to the function was to just calculate the\n",
    "        # normal performance\n",
    "        if (only_normal):\n",
    "            features_test_normal = data_test.drop(target_variable, axis=1)\n",
    "            target_test_normal = data_test[target_variable]\n",
    "            predictions_normal = rf_normal.predict(features_test_normal)\n",
    "            normal_score = f1_score(target_test_normal, predictions_normal, average=average_metric_type)\n",
    "                           \n",
    "            return (normal_score)\n",
    "    \n",
    "    \n",
    "    #######################################\n",
    "        \n",
    "                                                \n",
    "    #################################\n",
    "    # GENERATION OF AUXILIARY MODELS\n",
    "        \n",
    "    # List of dictionaries\n",
    "    list_of_dictionaries = []\n",
    "                            \n",
    "    # For each value of the target\n",
    "    for target_value in sorted(list(data[target_variable].unique())):\n",
    "                                \n",
    "        # Generate auxiliary dataset\n",
    "        dataset_aux = data[data[target_variable] == target_value]\n",
    "                                \n",
    "        # Discard target in auxiliary dataset\n",
    "        dataset_aux = dataset_aux.drop(columns=[target_variable])\n",
    "                                \n",
    "        # Generate dictionary of ficticious targets and the models that predict them:\n",
    "        dictionary_aux = {}\n",
    "                                \n",
    "        for fict_target in dataset_aux.columns.tolist():\n",
    "        \n",
    "            # Train auxiliary model and save it\n",
    "            X = dataset_aux.drop(columns=[fict_target])\n",
    "            y = dataset_aux[fict_target]                                 \n",
    "\n",
    "            if (type_aux_mod == \"linear\"):\n",
    "                aux_model = LinearRegression(n_jobs=-1)\n",
    "            else:\n",
    "                if (type_aux_mod == \"randomforest\"):\n",
    "                    aux_model = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=5, min_samples_leaf=5, n_jobs=-1)\n",
    "                else:\n",
    "                    print(\"Error: current allowed type for auxiliary models is 'linear' or 'randomforest'\")\n",
    "                    sys.exit()\n",
    "            \n",
    "            aux_model.fit(X, y)\n",
    "                                        \n",
    "            dictionary_aux[fict_target] = aux_model\n",
    "                                        \n",
    "                                    \n",
    "        list_of_dictionaries.append(dictionary_aux)    \n",
    "            \n",
    "\n",
    "    #################################\n",
    "    # GENERATION OF NEW COLUMNS BASED ON\n",
    "    # AUXILIARY MODELS\n",
    "        \n",
    "    list_unique_values_target = sorted(list(data[target_variable].unique()))\n",
    "                                \n",
    "    list_of_rows_dataframe_new = []\n",
    "                          \n",
    "    new_columns = []\n",
    "\n",
    "    # For each value of the target\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "                              \n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "                                    \n",
    "        for fict_target in dictionary_case:\n",
    "                                    \n",
    "            X = data.drop(columns=[target_variable, fict_target])\n",
    "            y_predicted = dictionary_case[fict_target].predict(X)\n",
    "            y_real = data[fict_target]  \n",
    "\n",
    "            if (sqrt_error):\n",
    "                mse = (y_real - y_predicted) ** 2                            \n",
    "                rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "            else:    \n",
    "                rmse = (y_real - y_predicted) \n",
    "                rmse = [round(row_mse_value, 4) for row_mse_value in rmse]\n",
    "                          \n",
    "            # Add column to list of new columns\n",
    "            new_columns.append(rmse)\n",
    "\n",
    "        \n",
    "    dataframe_new = pd.DataFrame(new_columns).transpose()\n",
    "    names_cols_dataframe_new = []\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "        for u in dictionary_case.keys():\n",
    "            names_cols_dataframe_new.append(u + \"_\" + str(case))\n",
    "    dataframe_new.columns = names_cols_dataframe_new\n",
    "\n",
    "    cols_a = data.columns.to_list()\n",
    "    cols_b = dataframe_new.columns.to_list()\n",
    "                            \n",
    "    data = data.reset_index(drop=True)\n",
    "    dataframe_new = dataframe_new.reset_index(drop=True)\n",
    "                           \n",
    "    # Concatenate horizontally\n",
    "    result_df = pd.concat([dataframe_new, data], axis=1, ignore_index=True)\n",
    "    result_df.columns = cols_b + cols_a    \n",
    "                            \n",
    "    #################################\n",
    "    # SELECTION OF NEW COLUMNS CREATED TO CREATE\n",
    "    # AUGMENTED DF.\n",
    "\n",
    "    if (selection_method == \"feature_imp_normal\"):\n",
    "        # Calculations for later selection of new columns\n",
    "        # Compute permutation importance using the training set\n",
    "        perm_importance = permutation_importance(rf_normal, X_normal, y_normal, n_repeats=10, random_state=42)\n",
    "        # Extract mean importances and store in a variable\n",
    "        feature_importances = perm_importance.importances_mean\n",
    "        \n",
    "        # Create a list of (feature_name, importance) tuples\n",
    "        feature_importance_tuples = zip(X_normal.columns, feature_importances)\n",
    "        # Sort the tuples based on importance\n",
    "        sorted_feature_importance_tuples = sorted(feature_importance_tuples, key=lambda x: x[1], reverse=True)\n",
    "        # Extract sorted feature names\n",
    "        sorted_feature_names = [feature_name for feature_name, _ in sorted_feature_importance_tuples]\n",
    "        # Select some features\n",
    "        base_selected_columns = sorted_feature_names[0: int(len(sorted_feature_names) * frac_feature_imp_normal)]\n",
    "        \n",
    "        selected_columns = [] \n",
    "        for aux_base_cols in base_selected_columns:\n",
    "            selected_columns.append(aux_base_cols + \"_0\")\n",
    "            selected_columns.append(aux_base_cols + \"_1\")\n",
    "        \n",
    "    else:          \n",
    "        if (selection_method == \"correlation_target\"):\n",
    "\n",
    "            # Obtain new variables that have at least certain absolute correlation with the target\n",
    "            selected_columns = calculate_significant_features_mi(result_df, target_variable, new_cols_corr_thr)\n",
    "            selected_columns = [w for w in selected_columns if (w in dataframe_new.columns)]\n",
    "            \n",
    "        else:\n",
    "            if (selection_method is None):\n",
    "                # Select all created new columns\n",
    "                selected_columns = dataframe_new.columns.tolist()            \n",
    "                \n",
    "            else:\n",
    "                print(\"Error. The parameter selection_method must be 'feature_imp_normal', 'correlation_target' or None\")\n",
    "                sys.exit()\n",
    "                    \n",
    "\n",
    "    # The augmented dataframe will contain the original columns plus the \n",
    "    # selected new columns\n",
    "    result_df = result_df.loc[:, data.columns.tolist() + selected_columns]\n",
    "\n",
    "                            \n",
    "\n",
    "    #################################\n",
    "    # TRAINING OF MODEL BASED ON AUGMENTED DF. \n",
    "\n",
    "    features = result_df.drop(target_variable, axis=1)\n",
    "    target = result_df[target_variable]\n",
    "\n",
    "    if (shap_treatment):\n",
    "        # Note: The selected new columns \n",
    "        # usually render lower performance if class_weight='balanced' is applied\n",
    "        # in the model. So, theoretically the best option would be to apply\n",
    "        # class_weight='balanced' to the original df columns, but the application\n",
    "        # of class_weight='balanced' only to specific columns is currently not\n",
    "        # supported in scikit-learn. As a workaround, an auxiliary model (aux_model_W)\n",
    "        # is trained only with the original df columns using class_weight='balanced' .\n",
    "        # Then, the SHAP contributions from this auxiliary model are extracted and\n",
    "        # added to selected_columns and from that dataset a model is fit without\n",
    "        # class_weight='balanced'. The goal is that the final model incorporates the\n",
    "        # class balance application to the original df columns (at least implicitely\n",
    "        # via their SHAP contributions) but not to the new columns\n",
    "        features_with_balance = features.drop(columns=selected_columns)\n",
    "        features_without_balance = features[selected_columns]\n",
    "            \n",
    "        # Train the auxiliary model for handling imbalance\n",
    "        aux_model_W = RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=42)\n",
    "        aux_model_W.fit(features_with_balance, target)    \n",
    "        \n",
    "        # Initialize SHAP explainer\n",
    "        explainer = shap.TreeExplainer(aux_model_W)\n",
    "        \n",
    "        # Calculate SHAP values for the entire dataset\n",
    "        shap_values = explainer.shap_values(features_with_balance)\n",
    "        \n",
    "        # Extract the SHAP values for class 1 (assuming binary classification)\n",
    "        shap_values = np.array(shap_values) \n",
    "        # Class 1 is the second element in the most inner array, and since in Python\n",
    "        # notation starts at 0 this would be 1\n",
    "        shap_values_class_1 = shap_values[:, :, 1]\n",
    "        \n",
    "        # Ensure the shape of SHAP values matches features_with_balance\n",
    "        if (shap_values_class_1.shape == features_with_balance.shape):\n",
    "            # Convert SHAP values to a DataFrame\n",
    "            shap_values_df = pd.DataFrame(shap_values_class_1, columns=features_with_balance.columns)\n",
    "        else:\n",
    "            raise ValueError(\"Shape mismatch between SHAP values and features_with_balance_scaled\")\n",
    "    \n",
    "        \n",
    "        # Extend the original features with SHAP values\n",
    "        features = pd.concat([features_without_balance, shap_values_df], axis=1)\n",
    "\n",
    "    # Train the model\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)    \n",
    "    # Define the parameter grid for grid search\n",
    "    param_grid = {\n",
    "        'min_samples_split': [5, 10, 15],\n",
    "        'min_samples_leaf': [5, 10]\n",
    "    }\n",
    "    # Perform grid search with F1 score as the metric\n",
    "    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                               scoring=make_scorer(f1_score, average='weighted'), cv=5, n_jobs=-1)\n",
    "    grid_search.fit(features, target)\n",
    "    # Get the best model\n",
    "    rf_model = grid_search.best_estimator_\n",
    "    \n",
    "    #################################\n",
    "    # PROCESSING OF TEST DATASET SO THAT THE TRAINED MODEL\n",
    "    # CAN BE APPLIED TO IT, AND IN THIS WAY OBTAIN \n",
    "    # PERFORMANCE METRICS\n",
    "    \n",
    "    # For each value of the target\n",
    "    new_columns = []\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "                                \n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "        \n",
    "        for fict_target in dictionary_case:\n",
    "            if ((fict_target + '_' + str(case)) in selected_columns):\n",
    "                                    \n",
    "                X = data_test.drop(columns=[target_variable, fict_target])\n",
    "                y_predicted = dictionary_case[fict_target].predict(X)\n",
    "                y_real = data_test[fict_target]  \n",
    "\n",
    "                if (sqrt_error):\n",
    "                    mse = (y_real - y_predicted) ** 2                            \n",
    "                    rmse = [round(np.sqrt(row_mse_value),4) for row_mse_value in mse]\n",
    "                else:    \n",
    "                    rmse = (y_real - y_predicted) \n",
    "                    rmse = [round(row_mse_value, 4) for row_mse_value in rmse]\n",
    "                                \n",
    "                # Add column to list of new columns\n",
    "                new_columns.append(rmse)\n",
    "                            \n",
    "    dataframe_new2 = pd.DataFrame(new_columns).transpose()\n",
    "    names_cols_dataframe_new2 = []\n",
    "    for case in range(0, len(list_unique_values_target)):\n",
    "        dictionary_case = list_of_dictionaries[case]\n",
    "        for fict_target in dictionary_case:\n",
    "            if ((fict_target + '_' + str(case)) in selected_columns):    \n",
    "                names_cols_dataframe_new2.append(fict_target + \"_\" + str(case))\n",
    "    dataframe_new2.columns = names_cols_dataframe_new2\n",
    "                            \n",
    "    cols_a = data_test.columns.to_list()\n",
    "    cols_b = dataframe_new2.columns.to_list()\n",
    "                            \n",
    "    data_test = data_test.reset_index(drop=True)\n",
    "    dataframe_new2 = dataframe_new2.reset_index(drop=True)\n",
    "                                                    \n",
    "    # Concatenate horizontally\n",
    "    data_test_processed = pd.concat([dataframe_new2, data_test], axis=1, ignore_index=True)\n",
    "    data_test_processed.columns = cols_b + cols_a\n",
    "\n",
    "    features_test = data_test_processed.drop(target_variable, axis=1)\n",
    "    features_test = features_test.loc[:, features.columns.tolist()] \n",
    "    \n",
    "    target_test = data_test_processed[target_variable]\n",
    "\n",
    "    if (shap_treatment):\n",
    "        # Obtain SHAP contributions trained on model with class_weight='balanced'\n",
    "        def transform_new_data(new_data, aux_model_W, explainer, selected_columns):\n",
    "            features_with_balance_new = new_data.drop(columns=selected_columns)\n",
    "            features_without_balance_new = new_data[selected_columns]\n",
    "        \n",
    "            # Get SHAP values for the entire dataset\n",
    "            shap_values = explainer.shap_values(features_with_balance_new)\n",
    "        \n",
    "            shap_explanations_new = []\n",
    "            for i in range(features_with_balance_new.shape[0]):\n",
    "                explanation_dict = dict(zip(features_with_balance_new.columns, shap_values[i]))\n",
    "                shap_explanations_new.append([explanation_dict.get(col, 0) for col in features_with_balance_new.columns])\n",
    "        \n",
    "            shap_contributions_new = pd.DataFrame(shap_explanations_new, columns=[f'shap_{col}' for col in features_with_balance_new.columns])        \n",
    "            shap_contributions_new.columns = features_with_balance_new.columns\n",
    "            # Extract the second element in each array (which is the shap\n",
    "            # value for class 1)\n",
    "            shap_contributions_new = shap_contributions_new.apply(lambda col: col.map(lambda x: x[1]))\n",
    "            # Concatenate columns in same order it was done for training the shap model\n",
    "            extended_features_new = pd.concat([features_without_balance_new, shap_contributions_new], axis=1)\n",
    "        \n",
    "            return extended_features_new\n",
    "        \n",
    "        # Initialize the TreeExplainer\n",
    "        explainer = shap.TreeExplainer(aux_model_W)\n",
    "        # Obtain features_test\n",
    "        features_test = transform_new_data(features_test, aux_model_W, explainer, selected_columns)\n",
    "                             \n",
    "    # Now apply trained model on test dataset to gauge performance\n",
    "    predictions = rf_model.predict(features_test)\n",
    "                           \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(target_test, predictions, average=average_metric_type)\n",
    "    return (f1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "\n",
    "# TEST ON DATAFRAMES FROM UCI, DOWNLOAD ADAPTED FROM Perales-González, Carlos, (2020). UCI download-process, v1.3, GitHub repository, https://github.com/cperales/uci-download-process\n",
    "\n",
    "# Number of statistical repetitions\n",
    "num_stat_rep = 10\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the directory you want to list files for\n",
    "directory_path_classification = os.path.join(current_directory, 'descarga_de_datasets_de_uci', 'directorio3', \n",
    "                              'datafiles', 'classification')\n",
    "\n",
    "# Call the function to return files in the directory\n",
    "files_classification = return_files(directory_path_classification)\n",
    "files_classification = [os.path.join(directory_path_classification, x, \"config.ini\") for x in files_classification]\n",
    "\n",
    "for i in files_classification:\n",
    "    try:\n",
    "        if (i == files_classification[0]): # Big dataset (slow for tests)\n",
    "            continue\n",
    "        with open(i, 'r') as file:\n",
    "            lines = file.readlines()            \n",
    "            len_lines = len(lines)\n",
    "            data_url = \"\"\n",
    "            separator = \"\\\\s+\"\n",
    "            target_index = \"\"\n",
    "            header_option = True\n",
    "            for x in range(0, len_lines):\n",
    "               if ('data_url' in lines[x]):\n",
    "                   data_url = lines[x].split(\" = \")[1]  \n",
    "                   data_url = data_url.strip()\n",
    "               else:\n",
    "                   if ('separator' in lines[x]):\n",
    "                       # If there is not separator specified, assume\n",
    "                       # default value (\"\\t\")\n",
    "                       try:\n",
    "                           if (len(lines[x].split(\" = \")) == 1):\n",
    "                               pass\n",
    "                           # If the separator is specified    \n",
    "                           else:\n",
    "                               separator = lines[x].split(\" = \")[1]\n",
    "                               separator = separator.strip()\n",
    "                               if (\"comma\" in separator):\n",
    "                                   separator = \",\"\n",
    "                       except:\n",
    "                           pass\n",
    "                               \n",
    "                   else:\n",
    "                       if ('target_index' in lines[x]):    \n",
    "                           try:\n",
    "                               target_index = lines[x].split(\" = \")[1]\n",
    "                               target_index = int(target_index.strip())\n",
    "                           except:\n",
    "                               pass\n",
    "                       else:\n",
    "                           if ('header' in lines[x]):    \n",
    "                               try:\n",
    "                                   # If there is not header specified, assume\n",
    "                                   # default value\n",
    "                                   if (len(lines[x].split(\" = \")) == 1):\n",
    "                                        pass\n",
    "                                   else:\n",
    "                                       # If it is specified\n",
    "                                       header_option = lines[x].split(\" = \")[1]\n",
    "                                       header_option = int(header_option.strip())\n",
    "                                       # If it is 0 assign it to None\n",
    "                                       if (str(header_option).startswith(\"0\")):\n",
    "                                           header_option = None\n",
    "                               except:\n",
    "                                   pass\n",
    "                   \n",
    "                \n",
    "            # Fetch plain text content from the URL\n",
    "            download_file(data_url, 'dataset_file_aux.txt')\n",
    "            # Read downloaded file\n",
    "            try:\n",
    "                data = pd.read_csv('dataset_file_aux.txt', sep=separator, engine='python', header=header_option)\n",
    "                aux_names_columns = [str(aux_n_c) for aux_n_c in data.columns]\n",
    "                data.columns = aux_names_columns\n",
    "                    \n",
    "                # If there will be enough dimensionality after one hot encoding\n",
    "                data_check = pd.get_dummies(data)\n",
    "                if (data_check.shape[0] >= (data_check.shape[1]*3*10)):\n",
    "                    \n",
    "                    # Name of the target of the dataset (target_index - 1 since \n",
    "                    # in python first position is 0)\n",
    "                    target_variable = str(data.columns.tolist()[int(target_index) - 1])\n",
    "        \n",
    "                    # If it is a binary classification\n",
    "                    unique_values_count = data[target_variable].nunique()\n",
    "                    if (unique_values_count == 2):\n",
    "                        \n",
    "                        normal_performance = []\n",
    "                        for c in range(0, num_stat_rep):\n",
    "                            normal_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                           only_normal=True, seed=c))\n",
    "\n",
    "                        normal_performance_stdev = statistics.stdev(normal_performance)\n",
    "                        normal_performance = statistics.mean(normal_performance)\n",
    "                        print(\"Normal performance: \" + str(round(normal_performance, 4)) + \" +- \" + str(round(normal_performance_stdev, 4)))\n",
    "                        \n",
    "                        cases_new = [\"type_aux_mode=linear; selection_method_case=None; shap_treatment=False; sqrt_error=False\"]\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=None; shap_treatment=True; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=None; shap_treatment=False; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=None; shap_treatment=True; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1; shap_treatment=False; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1; shap_treatment=True; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3; shap_treatment=False; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3; shap_treatment=True; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5; shap_treatment=False; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5; shap_treatment=True; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7; shap_treatment=False; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7; shap_treatment=True; sqrt_error=False\")                \n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1; shap_treatment=False; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1; shap_treatment=True; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3; shap_treatment=False; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3; shap_treatment=True; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5; shap_treatment=False; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5; shap_treatment=True; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7; shap_treatment=False; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7; shap_treatment=True; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1; shap_treatment=False; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1; shap_treatment=True; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2; shap_treatment=False; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2; shap_treatment=True; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1; shap_treatment=False; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1; shap_treatment=True; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2; shap_treatment=False; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2; shap_treatment=True; sqrt_error=False\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=None; shap_treatment=True; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=None; shap_treatment=False; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=None; shap_treatment=True; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1; shap_treatment=False; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1; shap_treatment=True; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3; shap_treatment=False; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3; shap_treatment=True; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5; shap_treatment=False; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5; shap_treatment=True; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7; shap_treatment=False; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=linear; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7; shap_treatment=True; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1; shap_treatment=False; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.1; shap_treatment=True; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3; shap_treatment=False; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.3; shap_treatment=True; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5; shap_treatment=False; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.5; shap_treatment=True; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7; shap_treatment=False; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode=randomforest; selection_method_case=feature_imp_normal; frac_feature_imp_normal=0.7; shap_treatment=True; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1; shap_treatment=False; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.1; shap_treatment=True; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2; shap_treatment=False; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode_case=linear; selection_method_case=correlation_target; new_cols_corr_thr=0.2; shap_treatment=True; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1; shap_treatment=False; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.1; shap_treatment=True; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2; shap_treatment=False; sqrt_error=True\")\n",
    "                        cases_new.append(\"type_aux_mode_case=randomforest; selection_method_case=correlation_target; new_cols_corr_thr=0.2; shap_treatment=True; sqrt_error=True\")\n",
    "\n",
    "                        cases_new_performances = []\n",
    "                        cases_new_performances_stdev = []\n",
    "                        \n",
    "                        \n",
    "                        for case in cases_new:\n",
    "\n",
    "                            if(\"shap_treatment=True\" in case):\n",
    "                                shap_treatm = True\n",
    "                            else:\n",
    "                                shap_treatm = False\n",
    "                            \n",
    "                            if (\"linear\" in case):\n",
    "                                model_aux = \"linear\"\n",
    "                            else:\n",
    "                                model_aux = \"randomforest\"\n",
    "\n",
    "                            if (\"None\" in case):\n",
    "                                new_performance = []\n",
    "                                for c in range(0, num_stat_rep):                            \n",
    "                                    new_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                                   type_aux_mod=model_aux, selection_method=None,\n",
    "                                                                   shap_treatment=shap_treatm, seed=c))\n",
    "                                new_performance_stdev = statistics.stdev(new_performance)    \n",
    "                                cases_new_performances_stdev.append(new_performance_stdev)\n",
    "                                new_performance = statistics.mean(new_performance)        \n",
    "                                cases_new_performances.append(new_performance)\n",
    "                                \n",
    "                            else:     \n",
    "                                if (\"feature_imp_normal\" in case):\n",
    "                                    frac_feature_imp_normal_value = float(case.split(\"frac_feature_imp_normal=\")[1].split(\";\")[0])\n",
    "                                    new_performance = []\n",
    "                                    for c in range(0, num_stat_rep):         \n",
    "                                        new_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                                      type_aux_mod=model_aux, selection_method=\"feature_imp_normal\", \n",
    "                                                                      frac_feature_imp_normal=frac_feature_imp_normal_value,\n",
    "                                                                      shap_treatment=shap_treatm, seed=c))\n",
    "                                    new_performance_stdev = statistics.stdev(new_performance)\n",
    "                                    cases_new_performances_stdev.append(new_performance_stdev)    \n",
    "                                    new_performance = statistics.mean(new_performance)        \n",
    "                                    cases_new_performances.append(new_performance)\n",
    "                                else:\n",
    "                                    new_cols_corr_thr_value = float(case.split(\"new_cols_corr_thr=\")[1].split(\";\")[0])\n",
    "                                    new_performance = []\n",
    "                                    for c in range(0, num_stat_rep):         \n",
    "                                        new_performance.append(aux_func(data=data.copy(), target_variable=target_variable,\n",
    "                                                                     type_aux_mod=model_aux, selection_method=\"correlation_target\", \n",
    "                                                                     new_cols_corr_thr=new_cols_corr_thr_value, \n",
    "                                                                     shap_treatment=shap_treatm, seed=c))\n",
    "                                        \n",
    "                                    new_performance_stdev = statistics.stdev(new_performance)\n",
    "                                    cases_new_performances_stdev.append(new_performance_stdev)    \n",
    "                                    new_performance = statistics.mean(new_performance)        \n",
    "                                    cases_new_performances.append(new_performance)\n",
    "                                    \n",
    "                            print(case)\n",
    "                            print(str(round(new_performance, 4)) + \" +- \" + str(round(new_performance_stdev, 4)))\n",
    "                            \n",
    "\n",
    "                        # Save result\n",
    "                        result = pd.DataFrame({\"Case\": cases_new, \"Mean F1-score\": cases_new_performances, \"Stdev F1-score\": cases_new_performances_stdev})\n",
    "                        result[\"Normal Mean F1-score\"] = normal_performance\n",
    "                        result[\"Normal stdev F1-score\"] = normal_performance_stdev\n",
    "                        result = result.sort_values(by=\"Mean F1-score\", ascending=False)\n",
    "                        data_name = data_url.split(\"/\")\n",
    "                        data_name = data_name[len(data_name) - 1]\n",
    "                        display(result)\n",
    "                        result.to_csv(\"metrics_\" + data_name + \".csv\", index=False)\n",
    "                        \n",
    "                        # Get maximum performance of the new cases\n",
    "                        if (max(cases_new_performances) > normal_performance):\n",
    "                            \n",
    "                            print(\"New performance: \" + str(round(max(cases_new_performances), 3)))\n",
    "                            # Print details\n",
    "                            max_index = cases_new_performances.index(max(cases_new_performances))    \n",
    "                            print(\"Optimum case new: \" + str(cases_new[max_index]))\n",
    "                        else:\n",
    "                            print(\"No improvement found\")                        \n",
    "                                                        \n",
    "            \n",
    "            except:      \n",
    "                print(\"Dataset \" + i + \" could not be processed.\")\n",
    "                    \n",
    "            \n",
    "    except FileNotFoundError:  \n",
    "        print(f\"The file '{i}' does not exist.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
